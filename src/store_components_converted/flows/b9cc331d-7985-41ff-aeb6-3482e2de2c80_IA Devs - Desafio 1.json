{
  "id": "b9cc331d-7985-41ff-aeb6-3482e2de2c80",
  "name": "IA Devs - Desafio 1",
  "description": "Uncover Business Opportunities with NLP. (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "jr3564",
    "first_name": "Rodolfo",
    "last_name": "Martins",
    "id": "f6d08889-7aff-4f6f-9651-a096910f88ab",
    "full_name": "Rodolfo Martins"
  },
  "store_url": "https://www.langflow.store/store/component/b9cc331d-7985-41ff-aeb6-3482e2de2c80",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-07-12T23:33:34.511Z",
    "updated": "2024-07-13T01:09:08.696Z",
    "downloaded": "2025-08-19T17:50:06.034Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.7",
    "private": false,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "id": "ParseData-mRcCW",
        "type": "genericNode",
        "position": {
          "x": -230.95905651722933,
          "y": -551.1053652590484
        },
        "data": {
          "type": "ParseData",
          "node": {
            "template": {
              "_type": "Component",
              "data": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "data",
                "display_name": "Data",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "The data to convert to text.",
                "title_case": false,
                "type": "other"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "sep": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "\n",
                "name": "sep",
                "display_name": "Separator",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "template": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "{text}",
                "name": "template",
                "display_name": "Template",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Convert Data into plain text following a specified template.",
            "icon": "braces",
            "base_classes": [
              "Message"
            ],
            "display_name": "Parse Data",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "parse_data",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false
              }
            ],
            "field_order": [
              "data",
              "template",
              "sep"
            ],
            "beta": false,
            "edited": false
          },
          "id": "ParseData-mRcCW",
          "description": "Convert Data into plain text following a specified template.",
          "display_name": "Parse Data"
        },
        "selected": false,
        "width": 384,
        "height": 385,
        "dragging": false,
        "positionAbsolute": {
          "x": -230.95905651722933,
          "y": -551.1053652590484
        }
      },
      {
        "id": "Prompt-KzjOn",
        "type": "genericNode",
        "position": {
          "x": 357.74686005723015,
          "y": -802.22142313877
        },
        "data": {
          "type": "Prompt",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "## Objective of the Prompting\nThe objective of the prompting is to segment a Markdown text into manageable parts, transforming the original text into a template containing only titles and subtitles, with necessary metadata to reconstruct it in chunks.\n\n## Criteria\n1. **Reconstructibility:** Each chunk should allow the complete reconstruction of the original text segment from the contained sentence and metadata.\n2. **Markdown Formatting:** The segmentation results should be presented in Markdown.\n3. **Output in Portuguese:** The output of this prompting should be in Portuguese.\n4. **Keep the examples:** The examples should be preserved in a reduced form, maintaining the original objective.\n\n### Input\n**Markdown Text:** Text formatted using Markdown syntax.\n\n### Context\nConvert the input text into a template containing only titles and subtitles using Markdown syntax. Segment the content into chunks, preserving the original structure of the text. Add necessary metadata for the faithful reconstruction of the original text.\n\n#### Example\nOriginal text:\n```markdown\n# Prompt Engineering Guide\n---\n# **Introduction**\nPrompt engineering is a new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help better understand the features and limitations of large language models (LLMs). Researchers use prompt engineering to improve the capabilities of LLMs in a wide range of tasks, such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompt techniques that interact with LLMs and other tools.\nThis guide covers the fundamentals of prompts to provide an approximate idea of how to use them to interact with and instruct large language models (LLMs).\nAll examples are tested with `text-davinci-003` (using the OpenAI playground), unless otherwise specified. It uses the default settings, i.e., `temperature=0.7` and `top-p=1`.\n```\n\nConverted text:\n```markdown\n# Prompt Engineering Guide\n# **Introduction**\n//chunk//\n**Main Topics:** Prompt engineering, interaction with LLMs (Large Language Models), NLP (Natural Language Processing) tasks.\n**Subtopics:** Introduction to prompt engineering, applications and research topics. Examples and settings.\n**Named Entities:** LMs, LLMs, text-davinci-003, OpenAI.\n**Specific Concepts:** Interaction and instruction of LLMs. Prompt optimization, model capabilities and limitations. Default settings, temperature, top-p.\n**Definitions and Examples:** Examples of tasks such as question answering and arithmetic reasoning. Fundamentals of using prompts with LLMs. Examples tested with `text-davinci-003`.\n**Important Quotes:** \"Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics.\" \"This guide covers the fundamentals of prompts to provide an approximate idea of how to use them to interact with and instruct large language models (LLMs).\"\n//\n```\n\n### Expected Output Format:\n```markdown\n# Title\n# Subtitle\n## Subtitle\n//chunk//\n**Main Topics:** \n**Subtopics:**\n**Named Entities:** \n**Specific Concepts:** \n**Definitions and Examples:** \n**Important Quotes:** \n//\n## Subtitle\n//chunk//\n```\n\n### Input:\n{content}\n\n### Output:\n",
                "name": "template",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "load_from_db": false
              },
              "content": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "content",
                "display_name": "content",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "content"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": false,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false
          },
          "id": "Prompt-KzjOn",
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt"
        },
        "selected": false,
        "width": 384,
        "height": 423,
        "dragging": false,
        "positionAbsolute": {
          "x": 357.74686005723015,
          "y": -802.22142313877
        }
      },
      {
        "id": "OpenAIModel-KhjwX",
        "type": "genericNode",
        "position": {
          "x": 1072.5048369395813,
          "y": -1239.2445194401093
        },
        "data": {
          "type": "OpenAIModel",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.base.models.openai_constants import MODEL_NAMES\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "input_value",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "json_mode": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "json_mode",
                "display_name": "JSON Mode",
                "advanced": true,
                "dynamic": false,
                "info": "If True, it will output JSON regardless of passing a schema.",
                "title_case": false,
                "type": "bool"
              },
              "max_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "max_tokens",
                "display_name": "Max Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "title_case": false,
                "type": "int"
              },
              "model_kwargs": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "model_kwargs",
                "display_name": "Model Kwargs",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "dict"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "gpt-4o",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-3.5-turbo-0125"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "gpt-4o",
                "name": "model_name",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "openai_api_base": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_base",
                "display_name": "OpenAI API Base",
                "advanced": true,
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "title_case": false,
                "type": "str"
              },
              "openai_api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_key",
                "display_name": "OpenAI API Key",
                "advanced": false,
                "input_types": [],
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "title_case": false,
                "password": true,
                "type": "str"
              },
              "output_schema": {
                "trace_as_input": true,
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "output_schema",
                "display_name": "Schema",
                "advanced": true,
                "dynamic": false,
                "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.",
                "title_case": false,
                "type": "dict"
              },
              "seed": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 1,
                "name": "seed",
                "display_name": "Seed",
                "advanced": true,
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "title_case": false,
                "type": "int"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "stream",
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool"
              },
              "system_message": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "system_message",
                "display_name": "System Message",
                "advanced": true,
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "0.5",
                "name": "temperature",
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float"
              }
            },
            "description": "Generates text using OpenAI LLMs.",
            "icon": "OpenAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "OpenAI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "output_schema",
              "model_name",
              "openai_api_base",
              "openai_api_key",
              "temperature",
              "stream",
              "system_message",
              "seed"
            ],
            "beta": false,
            "edited": false
          },
          "id": "OpenAIModel-KhjwX",
          "description": "Generates text using OpenAI LLMs.",
          "display_name": "OpenAI"
        },
        "selected": false,
        "width": 384,
        "height": 621,
        "dragging": false,
        "positionAbsolute": {
          "x": 1072.5048369395813,
          "y": -1239.2445194401093
        }
      },
      {
        "id": "ChatOutput-4Nm3e",
        "type": "genericNode",
        "position": {
          "x": 3005.5918965457686,
          "y": -439.5486037557737
        },
        "data": {
          "type": "ChatOutput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "data_template": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "{text}",
                "name": "data_template",
                "display_name": "Data Template",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "title_case": false,
                "type": "str"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "input_value",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as output.",
                "title_case": false,
                "type": "str"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Machine",
                "name": "sender",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Meu Resumo",
                "name": "sender_name",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "session_id",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Session ID for the message.",
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Display a chat message in the Playground.",
            "icon": "ChatOutput",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Output",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false
              }
            ],
            "field_order": [
              "input_value",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "beta": false,
            "edited": false
          },
          "id": "ChatOutput-4Nm3e",
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output"
        },
        "selected": false,
        "width": 384,
        "height": 309,
        "dragging": false,
        "positionAbsolute": {
          "x": 3005.5918965457686,
          "y": -439.5486037557737
        }
      },
      {
        "id": "File-gWYsn",
        "type": "genericNode",
        "position": {
          "x": -913.1015810068881,
          "y": -711.3327440313428
        },
        "data": {
          "type": "FileComponent",
          "node": {
            "template": {
              "_type": "Component",
              "path": {
                "trace_as_metadata": true,
                "file_path": "c76703e5-01ab-4967-a4ff-edb42eeb33f9/Prompt Engineering.md",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx"
                ],
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "path",
                "display_name": "Path",
                "advanced": false,
                "dynamic": false,
                "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx",
                "title_case": false,
                "type": "file"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from pathlib import Path\n\nfrom axiestudio.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, FileInput, Output\nfrom axiestudio.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data if data else \"No data\"\n        return data or Data()\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "silent_errors": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "silent_errors",
                "display_name": "Silent Errors",
                "advanced": true,
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "title_case": false,
                "type": "bool"
              }
            },
            "description": "A generic file loader.",
            "icon": "file-text",
            "base_classes": [
              "Data"
            ],
            "display_name": "File",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "data",
                "display_name": "Data",
                "method": "load_file",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false
              }
            ],
            "field_order": [
              "path",
              "silent_errors"
            ],
            "beta": false,
            "edited": true
          },
          "id": "File-gWYsn",
          "description": "A generic file loader.",
          "display_name": "File",
          "edited": false
        },
        "selected": false,
        "width": 384,
        "height": 301,
        "positionAbsolute": {
          "x": -913.1015810068881,
          "y": -711.3327440313428
        },
        "dragging": false
      },
      {
        "id": "ChatOutput-MnUY8",
        "type": "genericNode",
        "position": {
          "x": 4157.718815242517,
          "y": -271.8661846751068
        },
        "data": {
          "type": "ChatOutput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "data_template": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "{text}",
                "name": "data_template",
                "display_name": "Data Template",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "title_case": false,
                "type": "str"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "input_value",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as output.",
                "title_case": false,
                "type": "str"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Machine",
                "name": "sender",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Score",
                "name": "sender_name",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "session_id",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Session ID for the message.",
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Display a chat message in the Playground.",
            "icon": "ChatOutput",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Output",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "beta": false,
            "edited": false
          },
          "id": "ChatOutput-MnUY8",
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output"
        },
        "selected": true,
        "width": 384,
        "height": 309,
        "positionAbsolute": {
          "x": 4157.718815242517,
          "y": -271.8661846751068
        },
        "dragging": false
      },
      {
        "data": {
          "id": "GroupNode-Hr6e9",
          "type": "GroupNode",
          "node": {
            "display_name": "Avaliao",
            "documentation": "",
            "description": "Esse bloco carrega a parte de avaliao do flow.\n\n1. Score de similaridade do resumo com o artigo original\n2. Fator de reduo por nmero de caracteres do resumo\n\nScore Final = Similaridade * Reduo",
            "template": {
              "chunk_size_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 1000,
                "name": "chunk_size",
                "display_name": "Chunk Size",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "chunk_size"
                }
              },
              "client_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "client",
                "display_name": "Client",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "client"
                }
              },
              "code_OpenAIEmbeddings-B5dsW": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_openai.embeddings.base import OpenAIEmbeddings\n\nfrom axiestudio.base.embeddings.model import LCEmbeddingsModel\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        SecretStrInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            tiktoken_enabled=self.tiktoken_enable,\n            default_headers=self.default_headers,\n            default_query=self.default_query,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            deployment=self.deployment,\n            embedding_ctx_length=self.embedding_ctx_length,\n            max_retries=self.max_retries,\n            model=self.model,\n            model_kwargs=self.model_kwargs,\n            base_url=self.openai_api_base,\n            api_key=self.openai_api_key,\n            openai_api_type=self.openai_api_type,\n            api_version=self.openai_api_version,\n            organization=self.openai_organization,\n            openai_proxy=self.openai_proxy,\n            timeout=self.request_timeout or None,\n            show_progress_bar=self.show_progress_bar,\n            skip_empty=self.skip_empty,\n            tiktoken_model_name=self.tiktoken_model_name,\n        )\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "code"
                }
              },
              "default_headers_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "default_headers",
                "display_name": "Default Headers",
                "advanced": true,
                "dynamic": false,
                "info": "Default headers to use for the API request.",
                "title_case": false,
                "type": "dict",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "default_headers"
                }
              },
              "default_query_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "default_query",
                "display_name": "Default Query",
                "advanced": true,
                "dynamic": false,
                "info": "Default query parameters to use for the API request.",
                "title_case": false,
                "type": "dict",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "default_query"
                }
              },
              "deployment_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "deployment",
                "display_name": "Deployment",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "deployment"
                }
              },
              "embedding_ctx_length_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 1536,
                "name": "embedding_ctx_length",
                "display_name": "Embedding Context Length",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "embedding_ctx_length"
                }
              },
              "max_retries_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 3,
                "name": "max_retries",
                "display_name": "Max Retries",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "max_retries"
                }
              },
              "model_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "options": [
                  "text-embedding-3-small",
                  "text-embedding-3-large",
                  "text-embedding-ada-002"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "text-embedding-3-small",
                "name": "model",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "model"
                }
              },
              "model_kwargs_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "model_kwargs",
                "display_name": "Model Kwargs",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "dict",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "model_kwargs"
                }
              },
              "openai_api_base_OpenAIEmbeddings-B5dsW": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_base",
                "display_name": "OpenAI API Base",
                "advanced": true,
                "input_types": [],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "openai_api_base"
                }
              },
              "openai_api_key_OpenAIEmbeddings-B5dsW": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_key",
                "display_name": "OpenAI API Key",
                "advanced": false,
                "input_types": [],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "openai_api_key"
                }
              },
              "openai_api_type_OpenAIEmbeddings-B5dsW": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_type",
                "display_name": "OpenAI API Type",
                "advanced": true,
                "input_types": [],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "openai_api_type"
                }
              },
              "openai_api_version_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_version",
                "display_name": "OpenAI API Version",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "openai_api_version"
                }
              },
              "openai_organization_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_organization",
                "display_name": "OpenAI Organization",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "openai_organization"
                }
              },
              "openai_proxy_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_proxy",
                "display_name": "OpenAI Proxy",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "openai_proxy"
                }
              },
              "request_timeout_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "request_timeout",
                "display_name": "Request Timeout",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "request_timeout"
                }
              },
              "show_progress_bar_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "show_progress_bar",
                "display_name": "Show Progress Bar",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "bool",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "show_progress_bar"
                }
              },
              "skip_empty_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "skip_empty",
                "display_name": "Skip Empty",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "bool",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "skip_empty"
                }
              },
              "tiktoken_enable_OpenAIEmbeddings-B5dsW": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": true,
                "name": "tiktoken_enable",
                "display_name": "TikToken Enable",
                "advanced": true,
                "dynamic": false,
                "info": "If False, you must have transformers installed.",
                "title_case": false,
                "type": "bool",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "tiktoken_enable"
                }
              },
              "tiktoken_model_name_OpenAIEmbeddings-B5dsW": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "tiktoken_model_name",
                "display_name": "TikToken Model Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "OpenAIEmbeddings-k2BCE",
                  "field": "tiktoken_model_name"
                }
              },
              "code_CosineSimilarityComponent-kFO3J": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import MessageTextInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\nfrom typing import List\nimport numpy as np\n\nclass CosineSimilarityComponent(Component):\n    display_name = \"Cosine Similarity Component\"\n    description = \"Calculates cosine similarity between two texts.\"\n    icon = \"cosine\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"Text 1\",\n            info=\"First text input for similarity calculation.\",\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding Model\",\n            input_types=[\"Embeddings\"],\n            info=\"Model to generate embeddings for the texts.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cosine Similarity\", name=\"cosine_similarity\", method=\"calculate_cosine_similarity\"),\n    ]\n\n    def calculate_cosine_similarity(self) -> Data:\n        text1 = self.text1\n        \n        text2 = \"\"\"# Prompt Engineering Guide\n\n---\n\n# **Introduo**\n\nA engenharia de prompts  uma disciplina relativamente nova para desenvolver e otimizar prompts para usar eficientemente modelos de linguagem (LMs) para uma ampla variedade de aplicativos e tpicos de pesquisa. As habilidades imediatas de engenharia ajudam a entender melhor os recursos e as limitaes dos modelos de linguagem grandes (LLMs). Os pesquisadores usam a engenharia de prompt para melhorar a capacidade dos LLMs em uma ampla gama de tarefas comuns e complexas, como resposta a perguntas e raciocnio aritmtico. Os desenvolvedores usam engenharia de prompt para projetar tcnicas de prompt robustas e eficazes que fazem interface com LLMs e outras ferramentas.\n\nEste guia aborda os fundamentos dos prompts para fornecer uma ideia aproximada de como utiliza-los para interagir e instruir modelos de linguagem grandes (LLMs).\n\nTodos os exemplos so testados com`text-davinci-003`(usando o playground do OpenAI), a menos que especificado de outra forma. Ele usa as configuraes padro, ou seja,`temperatura=0.7`e`top-p=1`.\n\n# **Configuraes LLM**\n\nAo trabalhar com prompts, voc estar interagindo com o LLM diretamente ou por meio de uma API. Voc pode configurar alguns parmetros para obter resultados diferentes para seus prompts.\n\n**Temperatura**- Resumindo, quanto menor a`temperatura`, mais determinsticos so os resultados, no sentido de que o prximo token provvel mais alto  sempre escolhido. O aumento da temperatura pode levar a mais aleatoriedade, incentivando sadas mais diversificadas ou criativas. Estamos essencialmente aumentando os pesos dos outros tokens possveis. Em termos de aplicao, podemos querer usar um valor de temperatura mais baixo para tarefas como controle de qualidade baseado em fatos encorajando respostas mais factuais e concisas. Para gerao de poemas ou outras tarefas criativas, pode ser benfico aumentar o valor da temperatura.\n\n**Top_p**- Da mesma forma, com o`top_p`, uma tcnica de amostragem com temperatura chamada amostragem de ncleo, voc pode controlar o grau de determinismo do modelo na gerao de uma resposta. Se voc est procurando respostas exatas e factuais, mantenha isso baixo. Se voc estiver procurando respostas mais diversificadas, aumente para um valor mais alto.\n\nA recomendao geral  alterar um, no ambos.\n\nAntes de comear com alguns exemplos bsicos, lembre-se de que seus resultados podem variar dependendo da verso do LLM que voc est usando.\n\n# **Noes Bsicas de Prompt**\n\n## **Prompts Bsicos**\n\nVoc pode conseguir muito com prompts simples, mas a qualidade dos resultados depende da quantidade de informaes que voc fornece e de quo bem elaboradas so. Um prompt pode conter informaes como*instruo*ou*pergunta*que voc est passando para o modelo e incluir outros detalhes como*contexto*,*entradas*ou*exemplos*. Voc pode usar esses elementos para instruir melhor o modelo e, como resultado, obter melhores resultados.\n\nVamos comear analisando um exemplo bsico de um prompt simples:\n\n*Prompt*\n\n```\nO cu \n```\n\n*Sada:*\n\n```\nazulO cu  azul em um dia claro. Em um dia nublado, o cu pode ser cinza ou branco.\n```\n\nComo voc pode ver, o modelo de linguagem gera uma continuao de strings que fazem sentido no contexto`\"O cu \"`. A sada pode ser inesperada ou distante da tarefa que queremos realizar.\n\nEste exemplo bsico tambm destaca a necessidade de fornecer mais contexto ou instrues sobre o que especificamente queremos alcanar.\n\nVamos tentar melhorar um pouco:\n\n*Prompt:*\n\n```\nComplete a sentena:O cu \n```\n\n*Sada:*\n\n```\nto lindo.\n```\n\nIsto  melhor? Bem, dissemos ao modelo para completar a frase para que o resultado fique muito melhor, pois segue exatamente o que dissemos para fazer (\"complete a frase\"). Essa abordagem de projetar prompts ideais para instruir o modelo a executar uma tarefa  chamada de**engenharia de prompt**.\n\nO exemplo acima  uma ilustrao bsica do que  possvel com LLMs hoje. Os LLMs de hoje so capazes de executar todos os tipos de tarefas avanadas que variam de resumo de texto a raciocnio matemtico e gerao de cdigo.\n\n## **Formatao de prompt**\n\nTentamos um prompt muito simples acima. Um prompt padro tem o seguinte formato:\n\n```\n<Pergunta>?\n```\n\nou\n\n```\n<Instruo>\n```\n\nIsso pode ser formatado em um formato de resposta a perguntas (QA), que  padro em muitos conjuntos de dados de QA, como segue:\n\n```\nQ: <Pergunta>?A:\n```\n\nAo solicitar como o acima, tambm chamado de*prompt de tiro zero*, ou seja, voc est solicitando diretamente ao modelo uma resposta sem nenhum exemplo ou demonstrao sobre a tarefa que deseja realizar. Alguns modelos de linguagem grandes tm a capacidade de executar prompts zero-shot, mas isso depende da complexidade e do conhecimento da tarefa em questo.\n\nDado o formato padro acima, uma tcnica popular e eficaz para solicitao  chamada de*prompt de poucos tiros*, onde fornecemos exemplos (ou seja, demonstraes). Os prompts de poucos tiros podem ser formatados da seguinte maneira:\n\n```\n<Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?\n```\n\nA verso do formato QA ficaria assim:\n\n```\nQ: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A:\n```\n\nLembre-se de que no  necessrio usar o formato QA. O formato do prompt depende da tarefa em mos. Por exemplo, voc pode executar uma tarefa de classificao simples e fornecer exemplares que demonstrem a tarefa da seguinte forma:\n\n*Prompt:*\n\n```\nIsso  incrvel! // PositivoIsto  mau! // NegativoUau, esse filme foi radical! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nOs prompts de poucos tiros permitem o aprendizado no contexto, que  a capacidade dos modelos de linguagem de aprender tarefas dadas algumas demonstraes.\n\n# **Elementos de um prompt**\n\n medida que abordamos mais e mais exemplos e aplicativos possveis com a engenharia de prompt, voc notar que existem certos elementos que compem um prompt.\n\nUm prompt pode conter qualquer um dos seguintes componentes:\n\n**Instruo**- uma tarefa ou instruo especfica que voc deseja que o modelo execute\n\n**Contexto**- pode envolver informaes externas ou contexto adicional que pode direcionar o modelo para melhores respostas\n\n**Dados de entrada**-  a entrada ou pergunta para a qual estamos interessados em encontrar uma resposta\n\n**Indicador de sada**- indica o tipo ou formato da sada.\n\nNem todos os componentes so necessrios para um prompt e o formato depende da tarefa em questo. Abordaremos exemplos mais concretos nos prximos guias.\n\n# **Dicas gerais para projetar prompts**\n\nAqui esto algumas dicas para manter em mente ao projetar seus prompts:\n\n### **Comece Simples**\n\nAo comear a criar prompts, voc deve ter em mente que  realmente um processo iterativo que requer muita experimentao para obter os melhores resultados. Usar um playground simples como OpenAI ou Cohere's  um bom ponto de partida.\n\nVoc pode comear com prompts simples e continuar adicionando mais elementos e contexto  medida que busca melhores resultados. O controle de verso do seu prompt ao longo do caminho  vital por esse motivo. Ao ler o guia, voc ver muitos exemplos em que a especificidade, a simplicidade e a conciso geralmente lhe daro melhores resultados.\n\nQuando voc tem uma grande tarefa que envolve muitas subtarefas diferentes, pode tentar dividir a tarefa em subtarefas mais simples e continuar aumentando conforme obtm melhores resultados. Isso evita adicionar muita complexidade ao processo de design do prompt no incio.\n\n### **A instruo**\n\nVoc pode criar prompts eficazes para vrias tarefas simples usando comandos para instruir o modelo sobre o que deseja alcanar, como \"Escrever\", \"Classificar\", \"Resumir\", \"Traduzir\", \"Ordenar\" etc.\n\nTenha em mente que voc tambm precisa experimentar muito para ver o que funciona melhor. Experimente instrues diferentes com palavras-chave, contextos e dados diferentes e veja o que funciona melhor para seu caso de uso e tarefa especficos. Normalmente, quanto mais especfico e relevante for o contexto para a tarefa que voc est tentando executar, melhor. Abordaremos a importncia da amostragem e da adio de mais contexto nos prximos guias.\n\nOutros recomendam que as instrues sejam colocadas no incio do prompt. Tambm  recomendado que algum separador claro como \"###\" seja usado para separar a instruo e o contexto.\n\nPor exemplo:\n\n*Prompt:*\n\n```\n### Instruo ###Traduza o texto abaixo para o espanhol:Texto: \"ol!\"\n```\n\n*Sada:*\n\n```\nHola!\n```\n\n### **Especificidade**\n\nSeja muito especfico sobre a instruo e a tarefa que deseja que o modelo execute. Quanto mais descritivo e detalhado for o prompt, melhores sero os resultados. Isso  particularmente importante quando voc tem um resultado desejado ou estilo de gerao que est buscando. No h tokens ou palavras-chave especficas que levem a melhores resultados.  mais importante ter um bom formato e um prompt descritivo. Na verdade, fornecer exemplos no prompt  muito eficaz para obter a sada desejada em formatos especficos.\n\nAo criar prompts, voc tambm deve ter em mente o tamanho do prompt, pois h limitaes em relao a quo grande ele pode ser. Pensar em quo especfico e detalhado voc deve ser  algo a se considerar. Incluir muitos detalhes desnecessrios no  necessariamente uma boa abordagem. Os detalhes devem ser relevantes e contribuir para a tarefa em mos. Isso  algo que voc precisar experimentar muito. Incentivamos muita experimentao e iterao para otimizar os prompts de seus aplicativos.\n\nComo exemplo, vamos tentar um prompt simples para extrair informaes especficas de um texto.\n\n*Prompt:*\n\n```\nExtraia o nome dos lugares no texto a seguir.Formato desejado:Local: <lista_de_nomes_de_empresa_separados_por_vrgula>Input: \"Embora estes desenvolvimentos sejam encorajadores para os investigadores, muito ainda  um mistrio. Muitas vezes temos uma caixa preta entre o crebro e o efeito que vemos na periferia, diz Henrique Veiga-Fernandes, neuroimunologista do Centro Champalimaud para o Desconhecido em Lisboa. Se queremos utiliz-lo no contexto teraputico, precisamos de facto de perceber o mecanismo.\"\n```\n\n*Sada:*\n\n```\nLocal: Centro Champalimaud para o Desconhecido, Lisboa\n```\n\nO texto de entrada  obtido[neste artigo da Nature](https://www.nature.com/articles/d41586-023-00509-z).\n\n### **Evite Imprecises**\n\nDadas as dicas acima sobre como ser detalhado e melhorar o formato,  fcil cair na armadilha de querer ser muito inteligente sobre os prompts e potencialmente criar descries imprecisas. Geralmente  melhor ser especfico e direto. A analogia aqui  muito semelhante  comunicao eficaz - quanto mais direta, mais eficaz a mensagem  transmitida.\n\nPor exemplo, voc pode estar interessado em aprender o conceito de engenharia de prompt. Voc pode tentar algo como:\n\n```\nExplique o conceito de engenharia de prompt. Mantenha a explicao curta, apenas algumas frases, e no seja muito descritivo.\n```\n\nNo est claro no prompt acima quantas frases usar e qual estilo. Voc ainda pode obter uma boa resposta com o prompt acima, mas o melhor prompt seria aquele que  muito especfico, conciso e direto ao ponto. Algo como:\n\n```\nUse 2 a 3 frases para explicar o conceito de engenharia de prompt a um aluno do ensino mdio.\n```\n\n### **Fazer ou no fazer?**\n\nOutra dica comum ao criar prompts  evitar dizer o que no fazer, mas dizer o que fazer. Isso incentiva mais especificidade e concentra-se nos detalhes que levam a boas respostas do modelo.\n\nAqui est um exemplo de um chatbot de recomendao de filme falhando exatamente no que eu no quero que ele faa por causa de como escrevi a instruo -- focando no que no fazer.\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. NO PEA INTERESSES. NO PEA INFORMAES PESSOAIS.Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nClaro, posso recomendar um filme baseado em seus interesses. Que tipo de filme voc gostaria de assistir? Voc prefere ao, comdia, romance ou outra coisa?\n```\n\nAqui est um prompt melhor:\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. O agente  responsvel por recomendar um filme dos principais filmes de tendncias globais. Deve abster-se de perguntar aos usurios sobre suas preferncias e evitar pedir informaes pessoais. Se o agente no tiver um filme para recomendar, ele deve responder \"Desculpe, no foi possvel encontrar um filme para recomendar hoje.\".Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nDesculpe, no tenho nenhuma informao sobre seus interesses. No entanto, aqui est uma lista dos principais filmes de tendncias globais no momento: [lista de filmes]. Espero que encontre algo que goste!\n```\n\nAlguns dos exemplos acima foram adotados do[artigo \"Melhores prticas para engenharia de prompt com OpenAI API\".](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n\n# **Exemplos de Prompts**\n\nNa seo anterior, apresentamos e fornecemos exemplos bsicos de como solicitar LLMs.\n\nNesta seo, forneceremos mais exemplos de como os prompts so usados para realizar diferentes tarefas e introduziremos os principais conceitos ao longo do caminho. Muitas vezes, a melhor maneira de aprender conceitos  atravs de exemplos. Abaixo, abordamos alguns exemplos de como prompts bem elaborados podem ser usados para executar diferentes tipos de tarefas.\n\nTpicos:\n\n- [Resumo de Texto](https://www.promptingguide.ai/pt/introduction/examples#resumo-de-texto)\n- [Extrao de informaes](https://www.promptingguide.ai/pt/introduction/examples#extra%C3%A7%C3%A3o-de-informa%C3%A7%C3%B5es)\n- [Resposta  pergunta](https://www.promptingguide.ai/pt/introduction/examples#resposta-a-pergunta)\n- [Classificao de texto](https://www.promptingguide.ai/pt/introduction/examples#classifica%C3%A7%C3%A3o-de-texto)\n- [Conversao](https://www.promptingguide.ai/pt/introduction/examples#conversa%C3%A7%C3%A3o)\n- [Gerao de cdigo](https://www.promptingguide.ai/pt/introduction/examples#gera%C3%A7%C3%A3o-de-c%C3%B3digo)\n- [Raciocnio](https://www.promptingguide.ai/pt/introduction/examples#racioc%C3%ADnio)\n\n---\n\n## **Resumo de texto**\n\nUma das tarefas padro na gerao de linguagem natural  o resumo de texto. A sumarizao de texto pode incluir preferncias e domnios diferentes. Na verdade, uma das aplicaes mais promissoras dos modelos de linguagem  a capacidade de resumir artigos e conceitos em resumos rpidos e fceis de ler. Vamos tentar uma tarefa bsica de resumo usando prompts.\n\nDigamos que estou interessado em aprender sobre antibiticos, poderia tentar um prompt como este:\n\n*Prompt:*\n\n```\nExplique os antibiticosA:\n```\n\n*Sada:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.\n```\n\nO \"A:\"  um formato de prompt explcito usado para responder perguntas. Eu usei isso aqui para dizer ao modelo que h um futuro esperado. Neste exemplo, no est claro como isso  til ou no, mas deixaremos isso para exemplos posteriores. Vamos apenas supor que isso  muita informao e queremos resumi-la ainda mais. Na verdade, podemos instruir o modelo a resumir em uma frase da seguinte forma:\n\n*Prompt:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.**Explique o que foi dito acima em uma frase**:\n```\n\n*Sada:*\n\n```\nOs antibiticos so medicamentos usados para tratar infeces bacterianas, matando as bactrias ou impedindo-as de se reproduzir, mas no so eficazes contra vrus e o uso excessivo pode levar  resistncia aos antibiticos.\n```\n\nSem prestar muita ateno  preciso da sada acima, algo que abordaremos em um guia posterior, o modelo tentou resumir o pargrafo em uma frase. Voc pode ficar esperto com as instrues, mas deixaremos isso para um captulo posterior. Sinta-se  vontade para fazer uma pausa aqui e experimentar para ver se obtm melhores resultados.\n\n---\n\n## **Extrao de Informaes**\n\nEmbora os modelos de linguagem sejam treinados para executar a gerao de linguagem natural e tarefas relacionadas, eles tambm so muito capazes de realizar classificao e uma srie de outras tarefas de processamento de linguagem natural (NLP).\n\nAqui est um exemplo de um prompt que extrai informaes de um determinado pargrafo.\n\n*Prompt:*\n\n```\nAs declaraes de contribuio do autor e os agradecimentos em trabalhos de pesquisa devem indicar clara e especificamente se, e em que medida, os autores usaram tecnologias de IA, como ChatGPT, na preparao de seus manuscritos e anlises. Eles tambm devem indicar quais LLMs foram usados. Isso alertar os editores e revisores para examinar os manuscritos com mais cuidado em busca de possveis vieses, imprecises e crditos de origem imprprios. Da mesma forma, os peridicos cientficos devem ser transparentes sobre o uso de LLMs, por exemplo, ao selecionar manuscritos enviados.**Mencione o produto baseado em modelo de linguagem grande mencionado no pargrafo acima**:\n```\n\n*Sada:*\n\n```\nO produto baseado em modelo de linguagem grande mencionado no pargrafo acima  o ChatGPT.\n```\n\nExistem muitas maneiras de melhorar os resultados acima, mas isso j  muito til.\n\nAt agora deve ser bvio que voc pode pedir ao modelo para executar diferentes tarefas simplesmente instruindo-o sobre o que fazer. Esse  um recurso poderoso que os desenvolvedores de produtos de IA j esto usando para criar produtos e experincias poderosos.\n\nFonte do pargrafo:[ChatGPT: cinco prioridades para pesquisa](https://www.nature.com/articles/d41586-023-00288-7)\n\n---\n\n## **Resposta a perguntas**\n\nUma das melhores maneiras de fazer com que o modelo responda a respostas especficas  melhorar o formato do prompt. Conforme abordado anteriormente, um prompt pode combinar instrues, contexto, entrada e indicadores de sada para obter melhores resultados.\n\nEmbora esses componentes no sejam necessrios, torna-se uma boa prtica, pois quanto mais especfico voc for com a instruo, melhores resultados obter. Abaixo est um exemplo de como isso ficaria seguindo um prompt mais estruturado.\n\n*Prompt:*\n\n```\nResponda a pergunta com base no contexto abaixo. Mantenha a resposta curta e concisa. Responda \"No tenho certeza sobre a resposta\" se no tiver certeza da resposta.Contexto: Teplizumab tem suas razes em uma empresa farmacutica de Nova Jersey chamada Ortho Pharmaceutical. L, os cientistas geraram uma verso inicial do anticorpo, apelidada de OKT3. Originalmente proveniente de camundongos, a molcula foi capaz de se ligar  superfcie das clulas T e limitar seu potencial de morte celular. Em 1986, foi aprovado para ajudar a prevenir a rejeio de rgos aps transplantes renais, tornando-se o primeiro anticorpo teraputico permitido para uso humano.Pergunta: De onde veio originalmente o OKT3?Responder:\n```\n\n*Sada:*\n\n```\nCamundongos.\n```\n\nContexto obtido da[Nature](https://www.nature.com/articles/d41586-023-00400-x).\n\n---\n\n## **Classificao de texto**\n\nAt agora, usamos instrues simples para executar uma tarefa. Como um engenheiro de prompt, voc precisar melhorar o fornecimento de melhores instrues. Mas isso no  tudo! Voc tambm descobrir que, para casos de uso mais difceis, apenas fornecer instrues no ser suficiente.  aqui que voc precisa pensar mais sobre o contexto e os diferentes elementos que pode usar em um prompt. Outros elementos que voc pode fornecer so`input data`ou`examples`.\n\nVamos tentar demonstrar isso fornecendo um exemplo de classificao de texto.\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nDemos a instruo para classificar o texto e o modelo respondeu com`'Neutro'`que est correto. No h nada de errado nisso, mas digamos que o que realmente precisamos  que o modelo d o rtulo no formato exato que queremos. Portanto, em vez de`Neutral`, queremos que retorne`neutral`. Como alcanamos isso? Existem diferentes maneiras de fazer isso. Ns nos preocupamos com a especificidade aqui, portanto, quanto mais informaes pudermos fornecer, melhores sero os resultados. Podemos tentar fornecer exemplos para especificar o comportamento correto. Vamos tentar de novo:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento: neutroTexto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nneutro\n```\n\nPerfeito! Desta vez, o modelo retornou`neutro`, que  o rtulo especfico que eu estava procurando. Parece que o exemplo fornecido no prompt ajudou o modelo a ser especfico em sua sada. Para destacar por que s vezes ser especfico  importante, confira este exemplo e identifique o problema:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nQual  o problema aqui?\n\n---\n\n## **Conversao**\n\nTalvez uma das coisas mais interessantes que voc pode conseguir com a engenharia imediata seja instruir o sistema LLM sobre como se comportar, sua inteno e sua identidade. Isso  particularmente til quando voc est construindo sistemas de conversao, como chatbots de atendimento ao cliente.\n\nPor exemplo, vamos criar um sistema de conversao capaz de gerar respostas mais tcnicas e cientficas s perguntas. Observe como estamos dizendo explicitamente como se comportar por meio da instruo. s vezes, isso  chamado de*solicitao de funo*.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. O tom assistente  tcnico e cientfico.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Buracos negros so regies do espao-tempo onde a fora gravitacional  to forte que nada, nem mesmo a luz, consegue escapar dela. Eles so criados quando uma estrela muito massiva morre e seu ncleo colapsa sobre si mesmo, formando uma singularidade de densidade infinita. A intensa gravidade da singularidade atrai toda a matria e radiao ao seu redor, criando o buraco negro.\n```\n\nNosso assistente de pesquisa de IA parece um pouco tcnico demais, certo? Ok, vamos mudar esse comportamento e instruir o sistema a dar respostas mais acessveis.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. As respostas do assistente devem ser fceis de entender mesmo por alunos do ensino fundamental.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Os buracos negros so criados quando uma estrela muito massiva fica sem combustvel e colapsa sobre si mesma. A gravidade da estrela  to forte que nada, nem mesmo a luz, pode escapar de sua atrao. Isso cria um ponto no espao-tempo com uma intensa fora gravitacional chamada buraco negro.\n```\n\nAcho que fizemos algum progresso. Voc pode continuar melhorando. Tenho certeza que se voc adicionar mais exemplos voc pode obter resultados ainda melhores.\n\n---\n\n## **Gerao de Cdigo**\n\nUma aplicao em que os LLMs so bastante eficazes  a gerao de cdigo. O Copilot  um timo exemplo disso. H um grande nmero de tarefas de gerao de cdigo que voc pode executar com prompts inteligentes. Vejamos alguns exemplos abaixo.\n\nPrimeiro, vamos tentar um programa simples que sada o usurio.\n\n*Prompt:*\n\n```\n/*Pergunte ao usurio o nome dele e diga \"Ol\"*/\n```\n\n*Sada:*\n\n```\nlet name = prompt(\"Qual  o seu nome?\");console.log(`Ol, ${name}!`);\n```\n\nVoc pode ver que nem precisamos especificar a linguagem a ser usada.\n\nVamos mudar um pouco os nveis. Quero mostrar a voc como os LLMs podem ser poderosos com um pouco mais de esforo no design dos prompts.\n\n*Prompt:*\n\n```\n\"Tabela departamentos, colunas = [DepartmentId, DepartmentName]Alunos da tabela, colunas = [DepartmentId, StudentId, StudentName]Crie uma consulta MySQL para todos os alunos do Departamento de Cincia da Computao\"\n```\n\n*Sada:*\n\n```\nSELECT AlunoId, AlunoNomeDE alunosWHERE DepartmentId IN (SELECT DepartmentId FROM departamentos WHERE DepartmentName = 'Cincia da Computao');\n```\n\nIsso  muito impressionante. Nesse caso, fornecemos dados sobre o esquema do banco de dados e solicitamos que ele gerasse uma consulta MySQL vlida.\n\n---\n\n## **Raciocnio**\n\nTalvez uma das tarefas mais difceis para um LLM hoje seja aquela que requer alguma forma de raciocnio. O raciocnio  uma das reas que mais me entusiasma devido aos tipos de aplicativos complexos que podem surgir dos LLMs.\n\nHouve algumas melhorias em tarefas envolvendo capacidades matemticas. Dito isso,  importante observar que os LLMs atuais lutam para executar tarefas de raciocnio, portanto, isso requer tcnicas de engenharia de prompt ainda mais avanadas. Abordaremos essas tcnicas avanadas no prximo guia. Por enquanto, abordaremos alguns exemplos bsicos para mostrar as capacidades aritmticas.\n\n*Prompt:*\n\n```\nQuanto  9.000 * 9.000?\n```\n\n*Sada:*\n\n```\n81.000.000\n```\n\nVamos tentar algo mais difcil.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada*\n\n```\nNo, os nmeros mpares neste grupo somam um nmero mpar: 119.\n```\n\nIsso  incorreto! Vamos tentar melhorar isso melhorando o prompt.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.Resolva dividindo o problema em etapas. Primeiro, identifique os nmeros mpares, some-os e indique se o resultado  par ou mpar.\n```\n\n*Sada:*\n\n```\nNmeros mpares: 15, 5, 13, 7, 1Total 4141  um nmero mpar.\n```\n\nMuito melhor, certo? A propsito, tentei isso algumas vezes e o sistema s vezes falha. Se voc fornecer instrues melhores combinadas com exemplos, isso pode ajudar a obter resultados mais precisos.\n\nContinuaremos a incluir mais exemplos de aplicativos comuns nesta seo do guia.\n\nNa prxima seo, abordaremos conceitos e tcnicas de engenharia de prompt ainda mais avanados para melhorar o desempenho em todas essas e em tarefas mais difceis.\n\n# **Zero-Shot Prompting**\n\nOs LLMs hoje treinados em grandes quantidades de dados e sintonizados para seguir instrues so capazes de executar tarefas de tiro zero. Tentamos alguns exemplos de tiro zero na seo anterior. Aqui est um dos exemplos que usamos:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nObserve que no prompt acima no fornecemos nenhum exemplo ao modelo -- esses so os recursos de tiro zero em ao.\n\nO ajuste de instruo demonstrou melhorar o aprendizado de tiro zero[Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). O ajuste de instruo  essencialmente o conceito de modelos de ajuste fino em conjuntos de dados descritos por meio de instrues. Alm disso,[RLHF](https://arxiv.org/abs/1706.03741)(aprendizado por reforo a partir de feedback humano) foi adotado para escalar o ajuste de instrues em que o modelo  alinhado para melhor atender s preferncias humanas. Este desenvolvimento recente alimenta modelos como o ChatGPT. Discutiremos todas essas abordagens e mtodos nas prximas sees.\n\nQuando o tiro zero no funciona,  recomendvel fornecer demonstraes ou exemplos no prompt que levam ao prompt de poucos tiros. Na prxima seo, demonstramos a solicitao de poucos disparos.\n\n# **Few-Shot Prompting**\n\nEmbora os modelos de linguagem grande demonstrem recursos notveis de disparo zero, eles ainda ficam aqum em tarefas mais complexas ao usar a configurao de disparo zero. O prompt de poucos disparos pode ser usado como uma tcnica para permitir o aprendizado no contexto, onde fornecemos demonstraes no prompt para direcionar o modelo para um melhor desempenho. As demonstraes servem de condicionamento para exemplos subsequentes onde gostaramos que o modelo gerasse uma resposta.\n\nDe acordo com[Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf)poucas propriedades de tiro apareceram pela primeira vez quando os modelos foram dimensionados para um tamanho suficiente[(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).\n\nVamos demonstrar a solicitao de poucos disparos por meio de um exemplo apresentado em[Brown et al. 2020](https://arxiv.org/abs/2005.14165). No exemplo, a tarefa  usar corretamente uma nova palavra em uma frase.\n\n*Prompt:*\n\n```\nUm \"whatpu\"  um pequeno animal peludo nativo da Tanznia. Exemplo de frase que usaa palavra whatpu :Estvamos viajando pela frica e vimos esses whatpus muito fofos.\"Farduddlear\" significa pular para cima e para baixo muito rpido. Exemplo de frase que usaa palavra farduddlear :\n```\n\n*Sada:*\n\n```\nQuando ganhamos o jogo, todos farduddleamos em festejo.\n```\n\nPodemos observar que o modelo aprendeu de alguma forma como executar a tarefa fornecendo apenas um exemplo (ou seja, 1-shot). Para tarefas mais difceis, podemos experimentar aumentar as demonstraes (por exemplo, 3 tiros, 5 tiros, 10 tiros, etc.).\n\nSeguindo as descobertas de[Min et al. (2022)](https://arxiv.org/abs/2202.12837), aqui esto mais algumas dicas sobre demonstraes/exemplares ao fazer poucos disparos:\n\n- \"o espao do rtulo e a distribuio do texto de entrada especificado pelas demonstraes so importantes (independentemente de os rtulos estarem corretos para entradas individuais)\"\n- o formato que voc usa tambm desempenha um papel fundamental no desempenho, mesmo que voc use apenas rtulos aleatrios, isso  muito melhor do que nenhum rtulo.\n- resultados adicionais mostram que selecionar rtulos aleatrios de uma distribuio verdadeira de rtulos (em vez de uma distribuio uniforme) tambm ajuda.\n\nVamos experimentar alguns exemplos. Vamos primeiro tentar um exemplo com rtulos aleatrios (o que significa que os rtulos Negativo e Positivo so atribudos aleatoriamente s entradas):\n\n*Prompt:*\n\n```\nIsso  incrvel! // NegativoIsto  mau! // PositivoUau, esse filme foi rad! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nAinda obtemos a resposta correta, mesmo que os rtulos tenham sido randomizados. Observe que tambm mantivemos o formato, o que tambm ajuda. Na verdade, com mais experimentao, parece que os modelos GPT mais recentes que estamos experimentando esto se tornando mais robustos at mesmo para formatos aleatrios. Exemplo:\n\n*Prompt:*\n\n```\nPositivo Isso  incrvel!Isto  mau! NegativoUau, esse filme foi rad!PositivoQue espetculo horrvel! --\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nNo h consistncia no formato acima, mas o modelo ainda previu o rtulo correto. Temos que realizar uma anlise mais completa para confirmar se isso vale para tarefas diferentes e mais complexas, incluindo diferentes variaes de prompts.\n\n### **Limitaes da solicitao de poucos disparos**\n\nO prompt padro de poucos disparos funciona bem para muitas tarefas, mas ainda no  uma tcnica perfeita, especialmente ao lidar com tarefas de raciocnio mais complexas. Vamos demonstrar por que esse  o caso. Voc se lembra do exemplo anterior onde fornecemos a seguinte tarefa:\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\nSe tentarmos isso novamente, o modelo produzir o seguinte:\n\n```\nSim, os nmeros mpares neste grupo somam 107, que  um nmero par.\n```\n\nEsta no  a resposta correta, o que no apenas destaca as limitaes desses sistemas, mas tambm a necessidade de uma engenharia imediata mais avanada.\n\nVamos tentar adicionar alguns exemplos para ver se a solicitao de poucos tiros melhora os resultados.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nA resposta  verdadeira.\n```\n\nIsso no funcionou. Parece que a solicitao de poucos disparos no  suficiente para obter respostas confiveis para esse tipo de problema de raciocnio. O exemplo acima fornece informaes bsicas sobre a tarefa. Se voc olhar mais de perto, o tipo de tarefa que introduzimos envolve mais algumas etapas de raciocnio. Em outras palavras, pode ajudar se dividirmos o problema em etapas e demonstrarmos isso ao modelo. Mais recentemente,[insero de cadeia de pensamento (CoT)](https://arxiv.org/abs/2201.11903)foi popularizada para abordar mais aritmtica complexa, senso comum e tarefas de raciocnio simblico.\n\nNo geral, parece que fornecer exemplos  til para resolver algumas tarefas. Quando a solicitao de disparo zero e a solicitao de poucos disparos no so suficientes, isso pode significar que tudo o que foi aprendido pelo modelo no  suficiente para se sair bem na tarefa. A partir daqui,  recomendvel comear a pensar em ajustar seus modelos ou experimentar tcnicas de solicitao mais avanadas. A seguir, falaremos sobre uma das tcnicas populares de sugesto, chamada de sugesto em cadeia de pensamento, que ganhou muita popularidade.\n\n# **Cadeia-de-Pensamento Prompt**\n\n## **Cadeia-de-Pensamento (CoT) Prompting**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n\nFonte da imagem:[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n\nIntroduzido em[Wei et al. (2022)](https://arxiv.org/abs/2201.11903), a solicitao de cadeia de pensamento (CoT) permite recursos de raciocnio complexos por meio de etapas intermedirias de raciocnio. Voc pode combin-lo com prompts de poucos tiros para obter melhores resultados em tarefas mais complexas que exigem raciocnio antes de responder.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.R: Somando todos os nmeros mpares (17, 19) d 36. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.R: Somando todos os nmeros mpares (11, 13) d 24. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.R: Somando todos os nmeros mpares (17, 9, 13) d 39. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nUau! Podemos ver um resultado perfeito quando fornecemos a etapa de raciocnio. Na verdade, podemos resolver essa tarefa fornecendo ainda menos exemplos, ou seja, apenas um exemplo parece suficiente:\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nTenha em mente que os autores afirmam que esta  uma habilidade emergente que surge com modelos de linguagem suficientemente grandes.\n\n## **Prompt COT Zero-Shot**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)\n\nFonte da imagem:[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n\nUma ideia recente que surgiu mais recentemente  a ideia de[zero-shot CoT](https://arxiv.org/abs/2205.11916)(Kojima et al. 2022) que envolve essencialmente adicionar \"Vamos pensar passo a passo\" para a solicitao original. Vamos tentar um problema simples e ver como o modelo funciona:\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?\n```\n\n*Sada:*\n\n```\n11 mas\n```\n\nA resposta est incorreta! Agora vamos tentar com o prompt especial.\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?Vamos pensar passo a passo.\n```\n\n*Sada:*\n\n```\nPrimeiro, voc comeou com 10 mas.Voc deu 2 mas para o vizinho e 2 para o reparador, ento voc tinha 6 mas sobrando.Ento voc comprou mais 5 mas, ento agora voc tinha 11 mas.Finalmente, voc comeu 1 ma, ento ficaria com 10 mas.\n```\n\n impressionante que esse prompt simples seja eficaz nessa tarefa. Isso  particularmente til onde voc no tem muitos exemplos para usar no prompt.\n\"\"\"\n        \n        \n        embedding_model = self.embedding\n\n        # Generate embeddings for the input texts\n        embedding1 = np.array(embedding_model.embed_query(text1))\n        embedding2 = np.array(embedding_model.embed_query(text2))\n\n        # Calculate cosine similarity manually\n        dot_product = np.dot(embedding1, embedding2)\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n        similarity = dot_product / (norm1 * norm2)\n        \n        result = Data(data={\"cosine_similarity\": similarity})\n\n\n        self.status = result\n        return result\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "proxy": {
                  "id": "CosineSimilarityComponent-gJ3Vw",
                  "field": "code"
                },
                "display_name": "Code"
              },
              "code_ScoreCalculatorComponent-yUVde": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import DataInput, MessageTextInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\n\nclass ScoreCalculatorComponent(Component):\n    display_name = \"Score Calculator Component\"\n    description = \"Calculates a score based on the initial LLM score and the length of the response.\"\n    icon = \"calculator\"\n\n    inputs = [\n        DataInput(\n            name=\"llm_score\",\n            display_name=\"LLM Score\",\n            info=\"Initial LLM score.\",\n        ),\n        MessageTextInput(\n            name=\"resposta\",\n            display_name=\"Resposta\",\n            info=\"Response text for the score calculation.\",\n        ),\n        \n    ]\n\n    outputs = [\n        Output(display_name=\"Final Score\", name=\"final_score\", method=\"calculate_score\"),\n    ]\n\n    def calculate_score(self) -> Data:\n        llm_score = self.llm_score.cosine_similarity\n        resposta = self.resposta\n\n        max_chars = 10000  # Limite mximo de caracteres\n        min_score = 0.0  # Score mnimo\n        max_score = 1.0  # Score mximo\n\n        tamanho_resposta = len(resposta)\n\n        if tamanho_resposta >= max_chars:\n            score_final = min_score\n        else:\n            fator_reducao = (max_chars - tamanho_resposta) / max_chars\n            score_final = llm_score * fator_reducao\n            score_final = max(min_score, min(max_score, score_final))\n\n        result = Data(data={\"score_final\": score_final, \"tamanho_resumo\": tamanho_resposta, \"similaridade\": llm_score, \"fator_reducao\": fator_reducao})\n        self.status = result\n        return result",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "ScoreCalculatorComponent-8nQC0",
                  "field": "code"
                }
              },
              "code_CustomComponent-63VyB": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.io import MessageInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema.message import Message\nfrom typing import List\nimport numpy as np\n\n\nclass MessagePassThroughComponent(Component):\n    display_name = \"Message Pass-Through Component\"\n    description = \"Passes a message through without any modifications.\"\n    icon = \"message\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to pass through.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        input_message = self.input_message\n        \n        result = Message(text=input_message)\n\n        self.status = result\n        return result\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "CustomComponent-gD9lk",
                  "field": "code"
                }
              },
              "input_message_CustomComponent-63VyB": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "input_message",
                "display_name": "Input Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The message to pass through.",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "CustomComponent-gD9lk",
                  "field": "input_message"
                }
              },
              "code_ParseData-M2bQq": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "ParseData-Ua8zO",
                  "field": "code"
                }
              },
              "sep_ParseData-M2bQq": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "\n",
                "name": "sep",
                "display_name": "Separator",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "ParseData-Ua8zO",
                  "field": "sep"
                }
              },
              "template_ParseData-M2bQq": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Score Inicial: {similaridade}\nFator de Reduo: {fator_reducao} (1.0 = sem reduo)\nScore Final: {score_final}\n",
                "name": "template",
                "display_name": "Template",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "ParseData-Ua8zO",
                  "field": "template"
                }
              }
            },
            "flow": {
              "data": {
                "nodes": [
                  {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "type": "genericNode",
                    "position": {
                      "x": 3930.7495819264814,
                      "y": 1579.261560325239
                    },
                    "data": {
                      "type": "OpenAIEmbeddings",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "chunk_size": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": 1000,
                            "name": "chunk_size",
                            "display_name": "Chunk Size",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "int"
                          },
                          "client": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "client",
                            "display_name": "Client",
                            "advanced": true,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          },
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from langchain_openai.embeddings.base import OpenAIEmbeddings\n\nfrom axiestudio.base.embeddings.model import LCEmbeddingsModel\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        SecretStrInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            tiktoken_enabled=self.tiktoken_enable,\n            default_headers=self.default_headers,\n            default_query=self.default_query,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            deployment=self.deployment,\n            embedding_ctx_length=self.embedding_ctx_length,\n            max_retries=self.max_retries,\n            model=self.model,\n            model_kwargs=self.model_kwargs,\n            base_url=self.openai_api_base,\n            api_key=self.openai_api_key,\n            openai_api_type=self.openai_api_type,\n            api_version=self.openai_api_version,\n            organization=self.openai_organization,\n            openai_proxy=self.openai_proxy,\n            timeout=self.request_timeout or None,\n            show_progress_bar=self.show_progress_bar,\n            skip_empty=self.skip_empty,\n            tiktoken_model_name=self.tiktoken_model_name,\n        )\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "default_headers": {
                            "trace_as_input": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": {},
                            "name": "default_headers",
                            "display_name": "Default Headers",
                            "advanced": true,
                            "dynamic": false,
                            "info": "Default headers to use for the API request.",
                            "title_case": false,
                            "type": "dict"
                          },
                          "default_query": {
                            "trace_as_input": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": {},
                            "name": "default_query",
                            "display_name": "Default Query",
                            "advanced": true,
                            "dynamic": false,
                            "info": "Default query parameters to use for the API request.",
                            "title_case": false,
                            "type": "dict"
                          },
                          "deployment": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "deployment",
                            "display_name": "Deployment",
                            "advanced": true,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          },
                          "embedding_ctx_length": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": 1536,
                            "name": "embedding_ctx_length",
                            "display_name": "Embedding Context Length",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "int"
                          },
                          "max_retries": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": 3,
                            "name": "max_retries",
                            "display_name": "Max Retries",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "int"
                          },
                          "model": {
                            "trace_as_metadata": true,
                            "options": [
                              "text-embedding-3-small",
                              "text-embedding-3-large",
                              "text-embedding-ada-002"
                            ],
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "text-embedding-3-small",
                            "name": "model",
                            "display_name": "Model",
                            "advanced": false,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          },
                          "model_kwargs": {
                            "trace_as_input": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": {},
                            "name": "model_kwargs",
                            "display_name": "Model Kwargs",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "dict"
                          },
                          "openai_api_base": {
                            "load_from_db": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "openai_api_base",
                            "display_name": "OpenAI API Base",
                            "advanced": true,
                            "input_types": [],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "password": true,
                            "type": "str"
                          },
                          "openai_api_key": {
                            "load_from_db": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "openai_api_key",
                            "display_name": "OpenAI API Key",
                            "advanced": false,
                            "input_types": [],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "password": true,
                            "type": "str"
                          },
                          "openai_api_type": {
                            "load_from_db": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "openai_api_type",
                            "display_name": "OpenAI API Type",
                            "advanced": true,
                            "input_types": [],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "password": true,
                            "type": "str"
                          },
                          "openai_api_version": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "openai_api_version",
                            "display_name": "OpenAI API Version",
                            "advanced": true,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          },
                          "openai_organization": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "openai_organization",
                            "display_name": "OpenAI Organization",
                            "advanced": true,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          },
                          "openai_proxy": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "openai_proxy",
                            "display_name": "OpenAI Proxy",
                            "advanced": true,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          },
                          "request_timeout": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "request_timeout",
                            "display_name": "Request Timeout",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "float"
                          },
                          "show_progress_bar": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": false,
                            "name": "show_progress_bar",
                            "display_name": "Show Progress Bar",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "bool"
                          },
                          "skip_empty": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": false,
                            "name": "skip_empty",
                            "display_name": "Skip Empty",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "bool"
                          },
                          "tiktoken_enable": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": true,
                            "name": "tiktoken_enable",
                            "display_name": "TikToken Enable",
                            "advanced": true,
                            "dynamic": false,
                            "info": "If False, you must have transformers installed.",
                            "title_case": false,
                            "type": "bool"
                          },
                          "tiktoken_model_name": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "tiktoken_model_name",
                            "display_name": "TikToken Model Name",
                            "advanced": true,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          }
                        },
                        "description": "Generate embeddings using OpenAI models.",
                        "icon": "OpenAI",
                        "base_classes": [
                          "Embeddings"
                        ],
                        "display_name": "OpenAI Embeddings",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Embeddings"
                            ],
                            "selected": "Embeddings",
                            "name": "embeddings",
                            "display_name": "Embeddings",
                            "method": "build_embeddings",
                            "value": "__UNDEFINED__",
                            "cache": true,
                            "hidden": false
                          }
                        ],
                        "field_order": [
                          "default_headers",
                          "default_query",
                          "chunk_size",
                          "client",
                          "deployment",
                          "embedding_ctx_length",
                          "max_retries",
                          "model",
                          "model_kwargs",
                          "openai_api_base",
                          "openai_api_key",
                          "openai_api_type",
                          "openai_api_version",
                          "openai_organization",
                          "openai_proxy",
                          "request_timeout",
                          "show_progress_bar",
                          "skip_empty",
                          "tiktoken_model_name",
                          "tiktoken_enable"
                        ],
                        "beta": false,
                        "edited": false
                      },
                      "id": "OpenAIEmbeddings-k2BCE"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 395,
                    "positionAbsolute": {
                      "x": 3930.7495819264814,
                      "y": 1579.261560325239
                    },
                    "dragging": false
                  },
                  {
                    "id": "CosineSimilarityComponent-gJ3Vw",
                    "type": "genericNode",
                    "position": {
                      "x": 4449.337686675583,
                      "y": 1356.3189405398239
                    },
                    "data": {
                      "type": "CosineSimilarityComponent",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "embedding": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "embedding",
                            "display_name": "Embedding Model",
                            "advanced": false,
                            "input_types": [
                              "Embeddings"
                            ],
                            "dynamic": false,
                            "info": "Model to generate embeddings for the texts.",
                            "title_case": false,
                            "type": "other"
                          },
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import MessageTextInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\nfrom typing import List\nimport numpy as np\n\nclass CosineSimilarityComponent(Component):\n    display_name = \"Cosine Similarity Component\"\n    description = \"Calculates cosine similarity between two texts.\"\n    icon = \"cosine\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"Text 1\",\n            info=\"First text input for similarity calculation.\",\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding Model\",\n            input_types=[\"Embeddings\"],\n            info=\"Model to generate embeddings for the texts.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cosine Similarity\", name=\"cosine_similarity\", method=\"calculate_cosine_similarity\"),\n    ]\n\n    def calculate_cosine_similarity(self) -> Data:\n        text1 = self.text1\n        \n        text2 = \"\"\"# Prompt Engineering Guide\n\n---\n\n# **Introduo**\n\nA engenharia de prompts  uma disciplina relativamente nova para desenvolver e otimizar prompts para usar eficientemente modelos de linguagem (LMs) para uma ampla variedade de aplicativos e tpicos de pesquisa. As habilidades imediatas de engenharia ajudam a entender melhor os recursos e as limitaes dos modelos de linguagem grandes (LLMs). Os pesquisadores usam a engenharia de prompt para melhorar a capacidade dos LLMs em uma ampla gama de tarefas comuns e complexas, como resposta a perguntas e raciocnio aritmtico. Os desenvolvedores usam engenharia de prompt para projetar tcnicas de prompt robustas e eficazes que fazem interface com LLMs e outras ferramentas.\n\nEste guia aborda os fundamentos dos prompts para fornecer uma ideia aproximada de como utiliza-los para interagir e instruir modelos de linguagem grandes (LLMs).\n\nTodos os exemplos so testados com`text-davinci-003`(usando o playground do OpenAI), a menos que especificado de outra forma. Ele usa as configuraes padro, ou seja,`temperatura=0.7`e`top-p=1`.\n\n# **Configuraes LLM**\n\nAo trabalhar com prompts, voc estar interagindo com o LLM diretamente ou por meio de uma API. Voc pode configurar alguns parmetros para obter resultados diferentes para seus prompts.\n\n**Temperatura**- Resumindo, quanto menor a`temperatura`, mais determinsticos so os resultados, no sentido de que o prximo token provvel mais alto  sempre escolhido. O aumento da temperatura pode levar a mais aleatoriedade, incentivando sadas mais diversificadas ou criativas. Estamos essencialmente aumentando os pesos dos outros tokens possveis. Em termos de aplicao, podemos querer usar um valor de temperatura mais baixo para tarefas como controle de qualidade baseado em fatos encorajando respostas mais factuais e concisas. Para gerao de poemas ou outras tarefas criativas, pode ser benfico aumentar o valor da temperatura.\n\n**Top_p**- Da mesma forma, com o`top_p`, uma tcnica de amostragem com temperatura chamada amostragem de ncleo, voc pode controlar o grau de determinismo do modelo na gerao de uma resposta. Se voc est procurando respostas exatas e factuais, mantenha isso baixo. Se voc estiver procurando respostas mais diversificadas, aumente para um valor mais alto.\n\nA recomendao geral  alterar um, no ambos.\n\nAntes de comear com alguns exemplos bsicos, lembre-se de que seus resultados podem variar dependendo da verso do LLM que voc est usando.\n\n# **Noes Bsicas de Prompt**\n\n## **Prompts Bsicos**\n\nVoc pode conseguir muito com prompts simples, mas a qualidade dos resultados depende da quantidade de informaes que voc fornece e de quo bem elaboradas so. Um prompt pode conter informaes como*instruo*ou*pergunta*que voc est passando para o modelo e incluir outros detalhes como*contexto*,*entradas*ou*exemplos*. Voc pode usar esses elementos para instruir melhor o modelo e, como resultado, obter melhores resultados.\n\nVamos comear analisando um exemplo bsico de um prompt simples:\n\n*Prompt*\n\n```\nO cu \n```\n\n*Sada:*\n\n```\nazulO cu  azul em um dia claro. Em um dia nublado, o cu pode ser cinza ou branco.\n```\n\nComo voc pode ver, o modelo de linguagem gera uma continuao de strings que fazem sentido no contexto`\"O cu \"`. A sada pode ser inesperada ou distante da tarefa que queremos realizar.\n\nEste exemplo bsico tambm destaca a necessidade de fornecer mais contexto ou instrues sobre o que especificamente queremos alcanar.\n\nVamos tentar melhorar um pouco:\n\n*Prompt:*\n\n```\nComplete a sentena:O cu \n```\n\n*Sada:*\n\n```\nto lindo.\n```\n\nIsto  melhor? Bem, dissemos ao modelo para completar a frase para que o resultado fique muito melhor, pois segue exatamente o que dissemos para fazer (\"complete a frase\"). Essa abordagem de projetar prompts ideais para instruir o modelo a executar uma tarefa  chamada de**engenharia de prompt**.\n\nO exemplo acima  uma ilustrao bsica do que  possvel com LLMs hoje. Os LLMs de hoje so capazes de executar todos os tipos de tarefas avanadas que variam de resumo de texto a raciocnio matemtico e gerao de cdigo.\n\n## **Formatao de prompt**\n\nTentamos um prompt muito simples acima. Um prompt padro tem o seguinte formato:\n\n```\n<Pergunta>?\n```\n\nou\n\n```\n<Instruo>\n```\n\nIsso pode ser formatado em um formato de resposta a perguntas (QA), que  padro em muitos conjuntos de dados de QA, como segue:\n\n```\nQ: <Pergunta>?A:\n```\n\nAo solicitar como o acima, tambm chamado de*prompt de tiro zero*, ou seja, voc est solicitando diretamente ao modelo uma resposta sem nenhum exemplo ou demonstrao sobre a tarefa que deseja realizar. Alguns modelos de linguagem grandes tm a capacidade de executar prompts zero-shot, mas isso depende da complexidade e do conhecimento da tarefa em questo.\n\nDado o formato padro acima, uma tcnica popular e eficaz para solicitao  chamada de*prompt de poucos tiros*, onde fornecemos exemplos (ou seja, demonstraes). Os prompts de poucos tiros podem ser formatados da seguinte maneira:\n\n```\n<Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?\n```\n\nA verso do formato QA ficaria assim:\n\n```\nQ: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A:\n```\n\nLembre-se de que no  necessrio usar o formato QA. O formato do prompt depende da tarefa em mos. Por exemplo, voc pode executar uma tarefa de classificao simples e fornecer exemplares que demonstrem a tarefa da seguinte forma:\n\n*Prompt:*\n\n```\nIsso  incrvel! // PositivoIsto  mau! // NegativoUau, esse filme foi radical! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nOs prompts de poucos tiros permitem o aprendizado no contexto, que  a capacidade dos modelos de linguagem de aprender tarefas dadas algumas demonstraes.\n\n# **Elementos de um prompt**\n\n medida que abordamos mais e mais exemplos e aplicativos possveis com a engenharia de prompt, voc notar que existem certos elementos que compem um prompt.\n\nUm prompt pode conter qualquer um dos seguintes componentes:\n\n**Instruo**- uma tarefa ou instruo especfica que voc deseja que o modelo execute\n\n**Contexto**- pode envolver informaes externas ou contexto adicional que pode direcionar o modelo para melhores respostas\n\n**Dados de entrada**-  a entrada ou pergunta para a qual estamos interessados em encontrar uma resposta\n\n**Indicador de sada**- indica o tipo ou formato da sada.\n\nNem todos os componentes so necessrios para um prompt e o formato depende da tarefa em questo. Abordaremos exemplos mais concretos nos prximos guias.\n\n# **Dicas gerais para projetar prompts**\n\nAqui esto algumas dicas para manter em mente ao projetar seus prompts:\n\n### **Comece Simples**\n\nAo comear a criar prompts, voc deve ter em mente que  realmente um processo iterativo que requer muita experimentao para obter os melhores resultados. Usar um playground simples como OpenAI ou Cohere's  um bom ponto de partida.\n\nVoc pode comear com prompts simples e continuar adicionando mais elementos e contexto  medida que busca melhores resultados. O controle de verso do seu prompt ao longo do caminho  vital por esse motivo. Ao ler o guia, voc ver muitos exemplos em que a especificidade, a simplicidade e a conciso geralmente lhe daro melhores resultados.\n\nQuando voc tem uma grande tarefa que envolve muitas subtarefas diferentes, pode tentar dividir a tarefa em subtarefas mais simples e continuar aumentando conforme obtm melhores resultados. Isso evita adicionar muita complexidade ao processo de design do prompt no incio.\n\n### **A instruo**\n\nVoc pode criar prompts eficazes para vrias tarefas simples usando comandos para instruir o modelo sobre o que deseja alcanar, como \"Escrever\", \"Classificar\", \"Resumir\", \"Traduzir\", \"Ordenar\" etc.\n\nTenha em mente que voc tambm precisa experimentar muito para ver o que funciona melhor. Experimente instrues diferentes com palavras-chave, contextos e dados diferentes e veja o que funciona melhor para seu caso de uso e tarefa especficos. Normalmente, quanto mais especfico e relevante for o contexto para a tarefa que voc est tentando executar, melhor. Abordaremos a importncia da amostragem e da adio de mais contexto nos prximos guias.\n\nOutros recomendam que as instrues sejam colocadas no incio do prompt. Tambm  recomendado que algum separador claro como \"###\" seja usado para separar a instruo e o contexto.\n\nPor exemplo:\n\n*Prompt:*\n\n```\n### Instruo ###Traduza o texto abaixo para o espanhol:Texto: \"ol!\"\n```\n\n*Sada:*\n\n```\nHola!\n```\n\n### **Especificidade**\n\nSeja muito especfico sobre a instruo e a tarefa que deseja que o modelo execute. Quanto mais descritivo e detalhado for o prompt, melhores sero os resultados. Isso  particularmente importante quando voc tem um resultado desejado ou estilo de gerao que est buscando. No h tokens ou palavras-chave especficas que levem a melhores resultados.  mais importante ter um bom formato e um prompt descritivo. Na verdade, fornecer exemplos no prompt  muito eficaz para obter a sada desejada em formatos especficos.\n\nAo criar prompts, voc tambm deve ter em mente o tamanho do prompt, pois h limitaes em relao a quo grande ele pode ser. Pensar em quo especfico e detalhado voc deve ser  algo a se considerar. Incluir muitos detalhes desnecessrios no  necessariamente uma boa abordagem. Os detalhes devem ser relevantes e contribuir para a tarefa em mos. Isso  algo que voc precisar experimentar muito. Incentivamos muita experimentao e iterao para otimizar os prompts de seus aplicativos.\n\nComo exemplo, vamos tentar um prompt simples para extrair informaes especficas de um texto.\n\n*Prompt:*\n\n```\nExtraia o nome dos lugares no texto a seguir.Formato desejado:Local: <lista_de_nomes_de_empresa_separados_por_vrgula>Input: \"Embora estes desenvolvimentos sejam encorajadores para os investigadores, muito ainda  um mistrio. Muitas vezes temos uma caixa preta entre o crebro e o efeito que vemos na periferia, diz Henrique Veiga-Fernandes, neuroimunologista do Centro Champalimaud para o Desconhecido em Lisboa. Se queremos utiliz-lo no contexto teraputico, precisamos de facto de perceber o mecanismo.\"\n```\n\n*Sada:*\n\n```\nLocal: Centro Champalimaud para o Desconhecido, Lisboa\n```\n\nO texto de entrada  obtido[neste artigo da Nature](https://www.nature.com/articles/d41586-023-00509-z).\n\n### **Evite Imprecises**\n\nDadas as dicas acima sobre como ser detalhado e melhorar o formato,  fcil cair na armadilha de querer ser muito inteligente sobre os prompts e potencialmente criar descries imprecisas. Geralmente  melhor ser especfico e direto. A analogia aqui  muito semelhante  comunicao eficaz - quanto mais direta, mais eficaz a mensagem  transmitida.\n\nPor exemplo, voc pode estar interessado em aprender o conceito de engenharia de prompt. Voc pode tentar algo como:\n\n```\nExplique o conceito de engenharia de prompt. Mantenha a explicao curta, apenas algumas frases, e no seja muito descritivo.\n```\n\nNo est claro no prompt acima quantas frases usar e qual estilo. Voc ainda pode obter uma boa resposta com o prompt acima, mas o melhor prompt seria aquele que  muito especfico, conciso e direto ao ponto. Algo como:\n\n```\nUse 2 a 3 frases para explicar o conceito de engenharia de prompt a um aluno do ensino mdio.\n```\n\n### **Fazer ou no fazer?**\n\nOutra dica comum ao criar prompts  evitar dizer o que no fazer, mas dizer o que fazer. Isso incentiva mais especificidade e concentra-se nos detalhes que levam a boas respostas do modelo.\n\nAqui est um exemplo de um chatbot de recomendao de filme falhando exatamente no que eu no quero que ele faa por causa de como escrevi a instruo -- focando no que no fazer.\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. NO PEA INTERESSES. NO PEA INFORMAES PESSOAIS.Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nClaro, posso recomendar um filme baseado em seus interesses. Que tipo de filme voc gostaria de assistir? Voc prefere ao, comdia, romance ou outra coisa?\n```\n\nAqui est um prompt melhor:\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. O agente  responsvel por recomendar um filme dos principais filmes de tendncias globais. Deve abster-se de perguntar aos usurios sobre suas preferncias e evitar pedir informaes pessoais. Se o agente no tiver um filme para recomendar, ele deve responder \"Desculpe, no foi possvel encontrar um filme para recomendar hoje.\".Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nDesculpe, no tenho nenhuma informao sobre seus interesses. No entanto, aqui est uma lista dos principais filmes de tendncias globais no momento: [lista de filmes]. Espero que encontre algo que goste!\n```\n\nAlguns dos exemplos acima foram adotados do[artigo \"Melhores prticas para engenharia de prompt com OpenAI API\".](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n\n# **Exemplos de Prompts**\n\nNa seo anterior, apresentamos e fornecemos exemplos bsicos de como solicitar LLMs.\n\nNesta seo, forneceremos mais exemplos de como os prompts so usados para realizar diferentes tarefas e introduziremos os principais conceitos ao longo do caminho. Muitas vezes, a melhor maneira de aprender conceitos  atravs de exemplos. Abaixo, abordamos alguns exemplos de como prompts bem elaborados podem ser usados para executar diferentes tipos de tarefas.\n\nTpicos:\n\n- [Resumo de Texto](https://www.promptingguide.ai/pt/introduction/examples#resumo-de-texto)\n- [Extrao de informaes](https://www.promptingguide.ai/pt/introduction/examples#extra%C3%A7%C3%A3o-de-informa%C3%A7%C3%B5es)\n- [Resposta  pergunta](https://www.promptingguide.ai/pt/introduction/examples#resposta-a-pergunta)\n- [Classificao de texto](https://www.promptingguide.ai/pt/introduction/examples#classifica%C3%A7%C3%A3o-de-texto)\n- [Conversao](https://www.promptingguide.ai/pt/introduction/examples#conversa%C3%A7%C3%A3o)\n- [Gerao de cdigo](https://www.promptingguide.ai/pt/introduction/examples#gera%C3%A7%C3%A3o-de-c%C3%B3digo)\n- [Raciocnio](https://www.promptingguide.ai/pt/introduction/examples#racioc%C3%ADnio)\n\n---\n\n## **Resumo de texto**\n\nUma das tarefas padro na gerao de linguagem natural  o resumo de texto. A sumarizao de texto pode incluir preferncias e domnios diferentes. Na verdade, uma das aplicaes mais promissoras dos modelos de linguagem  a capacidade de resumir artigos e conceitos em resumos rpidos e fceis de ler. Vamos tentar uma tarefa bsica de resumo usando prompts.\n\nDigamos que estou interessado em aprender sobre antibiticos, poderia tentar um prompt como este:\n\n*Prompt:*\n\n```\nExplique os antibiticosA:\n```\n\n*Sada:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.\n```\n\nO \"A:\"  um formato de prompt explcito usado para responder perguntas. Eu usei isso aqui para dizer ao modelo que h um futuro esperado. Neste exemplo, no est claro como isso  til ou no, mas deixaremos isso para exemplos posteriores. Vamos apenas supor que isso  muita informao e queremos resumi-la ainda mais. Na verdade, podemos instruir o modelo a resumir em uma frase da seguinte forma:\n\n*Prompt:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.**Explique o que foi dito acima em uma frase**:\n```\n\n*Sada:*\n\n```\nOs antibiticos so medicamentos usados para tratar infeces bacterianas, matando as bactrias ou impedindo-as de se reproduzir, mas no so eficazes contra vrus e o uso excessivo pode levar  resistncia aos antibiticos.\n```\n\nSem prestar muita ateno  preciso da sada acima, algo que abordaremos em um guia posterior, o modelo tentou resumir o pargrafo em uma frase. Voc pode ficar esperto com as instrues, mas deixaremos isso para um captulo posterior. Sinta-se  vontade para fazer uma pausa aqui e experimentar para ver se obtm melhores resultados.\n\n---\n\n## **Extrao de Informaes**\n\nEmbora os modelos de linguagem sejam treinados para executar a gerao de linguagem natural e tarefas relacionadas, eles tambm so muito capazes de realizar classificao e uma srie de outras tarefas de processamento de linguagem natural (NLP).\n\nAqui est um exemplo de um prompt que extrai informaes de um determinado pargrafo.\n\n*Prompt:*\n\n```\nAs declaraes de contribuio do autor e os agradecimentos em trabalhos de pesquisa devem indicar clara e especificamente se, e em que medida, os autores usaram tecnologias de IA, como ChatGPT, na preparao de seus manuscritos e anlises. Eles tambm devem indicar quais LLMs foram usados. Isso alertar os editores e revisores para examinar os manuscritos com mais cuidado em busca de possveis vieses, imprecises e crditos de origem imprprios. Da mesma forma, os peridicos cientficos devem ser transparentes sobre o uso de LLMs, por exemplo, ao selecionar manuscritos enviados.**Mencione o produto baseado em modelo de linguagem grande mencionado no pargrafo acima**:\n```\n\n*Sada:*\n\n```\nO produto baseado em modelo de linguagem grande mencionado no pargrafo acima  o ChatGPT.\n```\n\nExistem muitas maneiras de melhorar os resultados acima, mas isso j  muito til.\n\nAt agora deve ser bvio que voc pode pedir ao modelo para executar diferentes tarefas simplesmente instruindo-o sobre o que fazer. Esse  um recurso poderoso que os desenvolvedores de produtos de IA j esto usando para criar produtos e experincias poderosos.\n\nFonte do pargrafo:[ChatGPT: cinco prioridades para pesquisa](https://www.nature.com/articles/d41586-023-00288-7)\n\n---\n\n## **Resposta a perguntas**\n\nUma das melhores maneiras de fazer com que o modelo responda a respostas especficas  melhorar o formato do prompt. Conforme abordado anteriormente, um prompt pode combinar instrues, contexto, entrada e indicadores de sada para obter melhores resultados.\n\nEmbora esses componentes no sejam necessrios, torna-se uma boa prtica, pois quanto mais especfico voc for com a instruo, melhores resultados obter. Abaixo est um exemplo de como isso ficaria seguindo um prompt mais estruturado.\n\n*Prompt:*\n\n```\nResponda a pergunta com base no contexto abaixo. Mantenha a resposta curta e concisa. Responda \"No tenho certeza sobre a resposta\" se no tiver certeza da resposta.Contexto: Teplizumab tem suas razes em uma empresa farmacutica de Nova Jersey chamada Ortho Pharmaceutical. L, os cientistas geraram uma verso inicial do anticorpo, apelidada de OKT3. Originalmente proveniente de camundongos, a molcula foi capaz de se ligar  superfcie das clulas T e limitar seu potencial de morte celular. Em 1986, foi aprovado para ajudar a prevenir a rejeio de rgos aps transplantes renais, tornando-se o primeiro anticorpo teraputico permitido para uso humano.Pergunta: De onde veio originalmente o OKT3?Responder:\n```\n\n*Sada:*\n\n```\nCamundongos.\n```\n\nContexto obtido da[Nature](https://www.nature.com/articles/d41586-023-00400-x).\n\n---\n\n## **Classificao de texto**\n\nAt agora, usamos instrues simples para executar uma tarefa. Como um engenheiro de prompt, voc precisar melhorar o fornecimento de melhores instrues. Mas isso no  tudo! Voc tambm descobrir que, para casos de uso mais difceis, apenas fornecer instrues no ser suficiente.  aqui que voc precisa pensar mais sobre o contexto e os diferentes elementos que pode usar em um prompt. Outros elementos que voc pode fornecer so`input data`ou`examples`.\n\nVamos tentar demonstrar isso fornecendo um exemplo de classificao de texto.\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nDemos a instruo para classificar o texto e o modelo respondeu com`'Neutro'`que est correto. No h nada de errado nisso, mas digamos que o que realmente precisamos  que o modelo d o rtulo no formato exato que queremos. Portanto, em vez de`Neutral`, queremos que retorne`neutral`. Como alcanamos isso? Existem diferentes maneiras de fazer isso. Ns nos preocupamos com a especificidade aqui, portanto, quanto mais informaes pudermos fornecer, melhores sero os resultados. Podemos tentar fornecer exemplos para especificar o comportamento correto. Vamos tentar de novo:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento: neutroTexto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nneutro\n```\n\nPerfeito! Desta vez, o modelo retornou`neutro`, que  o rtulo especfico que eu estava procurando. Parece que o exemplo fornecido no prompt ajudou o modelo a ser especfico em sua sada. Para destacar por que s vezes ser especfico  importante, confira este exemplo e identifique o problema:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nQual  o problema aqui?\n\n---\n\n## **Conversao**\n\nTalvez uma das coisas mais interessantes que voc pode conseguir com a engenharia imediata seja instruir o sistema LLM sobre como se comportar, sua inteno e sua identidade. Isso  particularmente til quando voc est construindo sistemas de conversao, como chatbots de atendimento ao cliente.\n\nPor exemplo, vamos criar um sistema de conversao capaz de gerar respostas mais tcnicas e cientficas s perguntas. Observe como estamos dizendo explicitamente como se comportar por meio da instruo. s vezes, isso  chamado de*solicitao de funo*.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. O tom assistente  tcnico e cientfico.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Buracos negros so regies do espao-tempo onde a fora gravitacional  to forte que nada, nem mesmo a luz, consegue escapar dela. Eles so criados quando uma estrela muito massiva morre e seu ncleo colapsa sobre si mesmo, formando uma singularidade de densidade infinita. A intensa gravidade da singularidade atrai toda a matria e radiao ao seu redor, criando o buraco negro.\n```\n\nNosso assistente de pesquisa de IA parece um pouco tcnico demais, certo? Ok, vamos mudar esse comportamento e instruir o sistema a dar respostas mais acessveis.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. As respostas do assistente devem ser fceis de entender mesmo por alunos do ensino fundamental.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Os buracos negros so criados quando uma estrela muito massiva fica sem combustvel e colapsa sobre si mesma. A gravidade da estrela  to forte que nada, nem mesmo a luz, pode escapar de sua atrao. Isso cria um ponto no espao-tempo com uma intensa fora gravitacional chamada buraco negro.\n```\n\nAcho que fizemos algum progresso. Voc pode continuar melhorando. Tenho certeza que se voc adicionar mais exemplos voc pode obter resultados ainda melhores.\n\n---\n\n## **Gerao de Cdigo**\n\nUma aplicao em que os LLMs so bastante eficazes  a gerao de cdigo. O Copilot  um timo exemplo disso. H um grande nmero de tarefas de gerao de cdigo que voc pode executar com prompts inteligentes. Vejamos alguns exemplos abaixo.\n\nPrimeiro, vamos tentar um programa simples que sada o usurio.\n\n*Prompt:*\n\n```\n/*Pergunte ao usurio o nome dele e diga \"Ol\"*/\n```\n\n*Sada:*\n\n```\nlet name = prompt(\"Qual  o seu nome?\");console.log(`Ol, ${name}!`);\n```\n\nVoc pode ver que nem precisamos especificar a linguagem a ser usada.\n\nVamos mudar um pouco os nveis. Quero mostrar a voc como os LLMs podem ser poderosos com um pouco mais de esforo no design dos prompts.\n\n*Prompt:*\n\n```\n\"Tabela departamentos, colunas = [DepartmentId, DepartmentName]Alunos da tabela, colunas = [DepartmentId, StudentId, StudentName]Crie uma consulta MySQL para todos os alunos do Departamento de Cincia da Computao\"\n```\n\n*Sada:*\n\n```\nSELECT AlunoId, AlunoNomeDE alunosWHERE DepartmentId IN (SELECT DepartmentId FROM departamentos WHERE DepartmentName = 'Cincia da Computao');\n```\n\nIsso  muito impressionante. Nesse caso, fornecemos dados sobre o esquema do banco de dados e solicitamos que ele gerasse uma consulta MySQL vlida.\n\n---\n\n## **Raciocnio**\n\nTalvez uma das tarefas mais difceis para um LLM hoje seja aquela que requer alguma forma de raciocnio. O raciocnio  uma das reas que mais me entusiasma devido aos tipos de aplicativos complexos que podem surgir dos LLMs.\n\nHouve algumas melhorias em tarefas envolvendo capacidades matemticas. Dito isso,  importante observar que os LLMs atuais lutam para executar tarefas de raciocnio, portanto, isso requer tcnicas de engenharia de prompt ainda mais avanadas. Abordaremos essas tcnicas avanadas no prximo guia. Por enquanto, abordaremos alguns exemplos bsicos para mostrar as capacidades aritmticas.\n\n*Prompt:*\n\n```\nQuanto  9.000 * 9.000?\n```\n\n*Sada:*\n\n```\n81.000.000\n```\n\nVamos tentar algo mais difcil.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada*\n\n```\nNo, os nmeros mpares neste grupo somam um nmero mpar: 119.\n```\n\nIsso  incorreto! Vamos tentar melhorar isso melhorando o prompt.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.Resolva dividindo o problema em etapas. Primeiro, identifique os nmeros mpares, some-os e indique se o resultado  par ou mpar.\n```\n\n*Sada:*\n\n```\nNmeros mpares: 15, 5, 13, 7, 1Total 4141  um nmero mpar.\n```\n\nMuito melhor, certo? A propsito, tentei isso algumas vezes e o sistema s vezes falha. Se voc fornecer instrues melhores combinadas com exemplos, isso pode ajudar a obter resultados mais precisos.\n\nContinuaremos a incluir mais exemplos de aplicativos comuns nesta seo do guia.\n\nNa prxima seo, abordaremos conceitos e tcnicas de engenharia de prompt ainda mais avanados para melhorar o desempenho em todas essas e em tarefas mais difceis.\n\n# **Zero-Shot Prompting**\n\nOs LLMs hoje treinados em grandes quantidades de dados e sintonizados para seguir instrues so capazes de executar tarefas de tiro zero. Tentamos alguns exemplos de tiro zero na seo anterior. Aqui est um dos exemplos que usamos:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nObserve que no prompt acima no fornecemos nenhum exemplo ao modelo -- esses so os recursos de tiro zero em ao.\n\nO ajuste de instruo demonstrou melhorar o aprendizado de tiro zero[Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). O ajuste de instruo  essencialmente o conceito de modelos de ajuste fino em conjuntos de dados descritos por meio de instrues. Alm disso,[RLHF](https://arxiv.org/abs/1706.03741)(aprendizado por reforo a partir de feedback humano) foi adotado para escalar o ajuste de instrues em que o modelo  alinhado para melhor atender s preferncias humanas. Este desenvolvimento recente alimenta modelos como o ChatGPT. Discutiremos todas essas abordagens e mtodos nas prximas sees.\n\nQuando o tiro zero no funciona,  recomendvel fornecer demonstraes ou exemplos no prompt que levam ao prompt de poucos tiros. Na prxima seo, demonstramos a solicitao de poucos disparos.\n\n# **Few-Shot Prompting**\n\nEmbora os modelos de linguagem grande demonstrem recursos notveis de disparo zero, eles ainda ficam aqum em tarefas mais complexas ao usar a configurao de disparo zero. O prompt de poucos disparos pode ser usado como uma tcnica para permitir o aprendizado no contexto, onde fornecemos demonstraes no prompt para direcionar o modelo para um melhor desempenho. As demonstraes servem de condicionamento para exemplos subsequentes onde gostaramos que o modelo gerasse uma resposta.\n\nDe acordo com[Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf)poucas propriedades de tiro apareceram pela primeira vez quando os modelos foram dimensionados para um tamanho suficiente[(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).\n\nVamos demonstrar a solicitao de poucos disparos por meio de um exemplo apresentado em[Brown et al. 2020](https://arxiv.org/abs/2005.14165). No exemplo, a tarefa  usar corretamente uma nova palavra em uma frase.\n\n*Prompt:*\n\n```\nUm \"whatpu\"  um pequeno animal peludo nativo da Tanznia. Exemplo de frase que usaa palavra whatpu :Estvamos viajando pela frica e vimos esses whatpus muito fofos.\"Farduddlear\" significa pular para cima e para baixo muito rpido. Exemplo de frase que usaa palavra farduddlear :\n```\n\n*Sada:*\n\n```\nQuando ganhamos o jogo, todos farduddleamos em festejo.\n```\n\nPodemos observar que o modelo aprendeu de alguma forma como executar a tarefa fornecendo apenas um exemplo (ou seja, 1-shot). Para tarefas mais difceis, podemos experimentar aumentar as demonstraes (por exemplo, 3 tiros, 5 tiros, 10 tiros, etc.).\n\nSeguindo as descobertas de[Min et al. (2022)](https://arxiv.org/abs/2202.12837), aqui esto mais algumas dicas sobre demonstraes/exemplares ao fazer poucos disparos:\n\n- \"o espao do rtulo e a distribuio do texto de entrada especificado pelas demonstraes so importantes (independentemente de os rtulos estarem corretos para entradas individuais)\"\n- o formato que voc usa tambm desempenha um papel fundamental no desempenho, mesmo que voc use apenas rtulos aleatrios, isso  muito melhor do que nenhum rtulo.\n- resultados adicionais mostram que selecionar rtulos aleatrios de uma distribuio verdadeira de rtulos (em vez de uma distribuio uniforme) tambm ajuda.\n\nVamos experimentar alguns exemplos. Vamos primeiro tentar um exemplo com rtulos aleatrios (o que significa que os rtulos Negativo e Positivo so atribudos aleatoriamente s entradas):\n\n*Prompt:*\n\n```\nIsso  incrvel! // NegativoIsto  mau! // PositivoUau, esse filme foi rad! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nAinda obtemos a resposta correta, mesmo que os rtulos tenham sido randomizados. Observe que tambm mantivemos o formato, o que tambm ajuda. Na verdade, com mais experimentao, parece que os modelos GPT mais recentes que estamos experimentando esto se tornando mais robustos at mesmo para formatos aleatrios. Exemplo:\n\n*Prompt:*\n\n```\nPositivo Isso  incrvel!Isto  mau! NegativoUau, esse filme foi rad!PositivoQue espetculo horrvel! --\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nNo h consistncia no formato acima, mas o modelo ainda previu o rtulo correto. Temos que realizar uma anlise mais completa para confirmar se isso vale para tarefas diferentes e mais complexas, incluindo diferentes variaes de prompts.\n\n### **Limitaes da solicitao de poucos disparos**\n\nO prompt padro de poucos disparos funciona bem para muitas tarefas, mas ainda no  uma tcnica perfeita, especialmente ao lidar com tarefas de raciocnio mais complexas. Vamos demonstrar por que esse  o caso. Voc se lembra do exemplo anterior onde fornecemos a seguinte tarefa:\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\nSe tentarmos isso novamente, o modelo produzir o seguinte:\n\n```\nSim, os nmeros mpares neste grupo somam 107, que  um nmero par.\n```\n\nEsta no  a resposta correta, o que no apenas destaca as limitaes desses sistemas, mas tambm a necessidade de uma engenharia imediata mais avanada.\n\nVamos tentar adicionar alguns exemplos para ver se a solicitao de poucos tiros melhora os resultados.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nA resposta  verdadeira.\n```\n\nIsso no funcionou. Parece que a solicitao de poucos disparos no  suficiente para obter respostas confiveis para esse tipo de problema de raciocnio. O exemplo acima fornece informaes bsicas sobre a tarefa. Se voc olhar mais de perto, o tipo de tarefa que introduzimos envolve mais algumas etapas de raciocnio. Em outras palavras, pode ajudar se dividirmos o problema em etapas e demonstrarmos isso ao modelo. Mais recentemente,[insero de cadeia de pensamento (CoT)](https://arxiv.org/abs/2201.11903)foi popularizada para abordar mais aritmtica complexa, senso comum e tarefas de raciocnio simblico.\n\nNo geral, parece que fornecer exemplos  til para resolver algumas tarefas. Quando a solicitao de disparo zero e a solicitao de poucos disparos no so suficientes, isso pode significar que tudo o que foi aprendido pelo modelo no  suficiente para se sair bem na tarefa. A partir daqui,  recomendvel comear a pensar em ajustar seus modelos ou experimentar tcnicas de solicitao mais avanadas. A seguir, falaremos sobre uma das tcnicas populares de sugesto, chamada de sugesto em cadeia de pensamento, que ganhou muita popularidade.\n\n# **Cadeia-de-Pensamento Prompt**\n\n## **Cadeia-de-Pensamento (CoT) Prompting**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n\nFonte da imagem:[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n\nIntroduzido em[Wei et al. (2022)](https://arxiv.org/abs/2201.11903), a solicitao de cadeia de pensamento (CoT) permite recursos de raciocnio complexos por meio de etapas intermedirias de raciocnio. Voc pode combin-lo com prompts de poucos tiros para obter melhores resultados em tarefas mais complexas que exigem raciocnio antes de responder.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.R: Somando todos os nmeros mpares (17, 19) d 36. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.R: Somando todos os nmeros mpares (11, 13) d 24. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.R: Somando todos os nmeros mpares (17, 9, 13) d 39. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nUau! Podemos ver um resultado perfeito quando fornecemos a etapa de raciocnio. Na verdade, podemos resolver essa tarefa fornecendo ainda menos exemplos, ou seja, apenas um exemplo parece suficiente:\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nTenha em mente que os autores afirmam que esta  uma habilidade emergente que surge com modelos de linguagem suficientemente grandes.\n\n## **Prompt COT Zero-Shot**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)\n\nFonte da imagem:[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n\nUma ideia recente que surgiu mais recentemente  a ideia de[zero-shot CoT](https://arxiv.org/abs/2205.11916)(Kojima et al. 2022) que envolve essencialmente adicionar \"Vamos pensar passo a passo\" para a solicitao original. Vamos tentar um problema simples e ver como o modelo funciona:\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?\n```\n\n*Sada:*\n\n```\n11 mas\n```\n\nA resposta est incorreta! Agora vamos tentar com o prompt especial.\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?Vamos pensar passo a passo.\n```\n\n*Sada:*\n\n```\nPrimeiro, voc comeou com 10 mas.Voc deu 2 mas para o vizinho e 2 para o reparador, ento voc tinha 6 mas sobrando.Ento voc comprou mais 5 mas, ento agora voc tinha 11 mas.Finalmente, voc comeu 1 ma, ento ficaria com 10 mas.\n```\n\n impressionante que esse prompt simples seja eficaz nessa tarefa. Isso  particularmente til onde voc no tem muitos exemplos para usar no prompt.\n\"\"\"\n        \n        \n        embedding_model = self.embedding\n\n        # Generate embeddings for the input texts\n        embedding1 = np.array(embedding_model.embed_query(text1))\n        embedding2 = np.array(embedding_model.embed_query(text2))\n\n        # Calculate cosine similarity manually\n        dot_product = np.dot(embedding1, embedding2)\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n        similarity = dot_product / (norm1 * norm2)\n        \n        result = Data(data={\"cosine_similarity\": similarity})\n\n\n        self.status = result\n        return result\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false
                          },
                          "text1": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "text1",
                            "display_name": "Text 1",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "First text input for similarity calculation.",
                            "title_case": false,
                            "type": "str"
                          }
                        },
                        "description": "Calculates cosine similarity between two texts.",
                        "icon": "cosine",
                        "base_classes": [
                          "Data"
                        ],
                        "display_name": "Cosine Similarity",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Data"
                            ],
                            "selected": "Data",
                            "name": "cosine_similarity",
                            "display_name": "Cosine Similarity",
                            "method": "calculate_cosine_similarity",
                            "value": "__UNDEFINED__",
                            "cache": true,
                            "hidden": false
                          }
                        ],
                        "field_order": [
                          "text1",
                          "embedding"
                        ],
                        "beta": false,
                        "edited": true
                      },
                      "id": "CosineSimilarityComponent-gJ3Vw",
                      "description": "Calculates cosine similarity between two texts.",
                      "display_name": "Cosine Similarity"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 357,
                    "positionAbsolute": {
                      "x": 4449.337686675583,
                      "y": 1356.3189405398239
                    },
                    "dragging": false
                  },
                  {
                    "id": "ScoreCalculatorComponent-8nQC0",
                    "type": "genericNode",
                    "position": {
                      "x": 4939.6353751285205,
                      "y": 934.604278998075
                    },
                    "data": {
                      "type": "ScoreCalculatorComponent",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "llm_score": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "llm_score",
                            "display_name": "LLM Score",
                            "advanced": false,
                            "input_types": [
                              "Data"
                            ],
                            "dynamic": false,
                            "info": "Initial LLM score.",
                            "title_case": false,
                            "type": "other"
                          },
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import DataInput, MessageTextInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\n\nclass ScoreCalculatorComponent(Component):\n    display_name = \"Score Calculator Component\"\n    description = \"Calculates a score based on the initial LLM score and the length of the response.\"\n    icon = \"calculator\"\n\n    inputs = [\n        DataInput(\n            name=\"llm_score\",\n            display_name=\"LLM Score\",\n            info=\"Initial LLM score.\",\n        ),\n        MessageTextInput(\n            name=\"resposta\",\n            display_name=\"Resposta\",\n            info=\"Response text for the score calculation.\",\n        ),\n        \n    ]\n\n    outputs = [\n        Output(display_name=\"Final Score\", name=\"final_score\", method=\"calculate_score\"),\n    ]\n\n    def calculate_score(self) -> Data:\n        llm_score = self.llm_score.cosine_similarity\n        resposta = self.resposta\n\n        max_chars = 10000  # Limite mximo de caracteres\n        min_score = 0.0  # Score mnimo\n        max_score = 1.0  # Score mximo\n\n        tamanho_resposta = len(resposta)\n\n        if tamanho_resposta >= max_chars:\n            score_final = min_score\n        else:\n            fator_reducao = (max_chars - tamanho_resposta) / max_chars\n            score_final = llm_score * fator_reducao\n            score_final = max(min_score, min(max_score, score_final))\n\n        result = Data(data={\"score_final\": score_final, \"tamanho_resumo\": tamanho_resposta, \"similaridade\": llm_score, \"fator_reducao\": fator_reducao})\n        self.status = result\n        return result",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "resposta": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "resposta",
                            "display_name": "Resposta",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "Response text for the score calculation.",
                            "title_case": false,
                            "type": "str"
                          }
                        },
                        "description": "Calculates a score based on the initial LLM score and the length of the response.",
                        "icon": "calculator",
                        "base_classes": [
                          "Data"
                        ],
                        "display_name": "Custom Component",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Data"
                            ],
                            "selected": "Data",
                            "name": "final_score",
                            "display_name": "Final Score",
                            "method": "calculate_score",
                            "value": "__UNDEFINED__",
                            "cache": true,
                            "hidden": false
                          }
                        ],
                        "field_order": [
                          "llm_score",
                          "resposta"
                        ],
                        "beta": false,
                        "edited": true
                      },
                      "id": "ScoreCalculatorComponent-8nQC0",
                      "description": "Calculates a score based on the initial LLM score and the length of the response.",
                      "display_name": "Custom Component"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 385,
                    "positionAbsolute": {
                      "x": 4939.6353751285205,
                      "y": 934.604278998075
                    },
                    "dragging": false
                  },
                  {
                    "id": "CustomComponent-gD9lk",
                    "type": "genericNode",
                    "position": {
                      "x": 3916.5450704806644,
                      "y": 991.92079956417
                    },
                    "data": {
                      "type": "CustomComponent",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from axiestudio.custom import Component\nfrom axiestudio.io import MessageInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema.message import Message\nfrom typing import List\nimport numpy as np\n\n\nclass MessagePassThroughComponent(Component):\n    display_name = \"Message Pass-Through Component\"\n    description = \"Passes a message through without any modifications.\"\n    icon = \"message\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to pass through.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        input_message = self.input_message\n        \n        result = Message(text=input_message)\n\n        self.status = result\n        return result\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "input_message": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "input_message",
                            "display_name": "Input Message",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "The message to pass through.",
                            "title_case": false,
                            "type": "str"
                          }
                        },
                        "description": "Passes a message through without any modifications.",
                        "icon": "message",
                        "base_classes": [
                          "Message"
                        ],
                        "display_name": "Message",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Message"
                            ],
                            "selected": "Message",
                            "name": "output_message",
                            "display_name": "Output Message",
                            "method": "pass_message",
                            "value": "__UNDEFINED__",
                            "cache": true,
                            "hidden": false
                          }
                        ],
                        "field_order": [
                          "input_message"
                        ],
                        "beta": false,
                        "edited": true
                      },
                      "id": "CustomComponent-gD9lk",
                      "description": "Passes a message through without any modifications.",
                      "display_name": "Message"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 337,
                    "dragging": false,
                    "positionAbsolute": {
                      "x": 3916.5450704806644,
                      "y": 991.92079956417
                    }
                  },
                  {
                    "id": "ParseData-Ua8zO",
                    "type": "genericNode",
                    "position": {
                      "x": 5565.351914880647,
                      "y": 1233.1663723127
                    },
                    "data": {
                      "type": "ParseData",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "data": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "data",
                            "display_name": "Data",
                            "advanced": false,
                            "input_types": [
                              "Data"
                            ],
                            "dynamic": false,
                            "info": "The data to convert to text.",
                            "title_case": false,
                            "type": "other"
                          },
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "sep": {
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "\n",
                            "name": "sep",
                            "display_name": "Separator",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str"
                          },
                          "template": {
                            "trace_as_input": true,
                            "multiline": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "Score Inicial: {similaridade}\nFator de Reduo: {fator_reducao} (1.0 = sem reduo)\nScore Final: {score_final}\n",
                            "name": "template",
                            "display_name": "Template",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                            "title_case": false,
                            "type": "str"
                          }
                        },
                        "description": "Convert Data into plain text following a specified template.",
                        "icon": "braces",
                        "base_classes": [
                          "Message"
                        ],
                        "display_name": "Parse Data",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Message"
                            ],
                            "selected": "Message",
                            "name": "text",
                            "display_name": "Text",
                            "method": "parse_data",
                            "value": "__UNDEFINED__",
                            "cache": true,
                            "hidden": false
                          }
                        ],
                        "field_order": [
                          "data",
                          "template",
                          "sep"
                        ],
                        "beta": false,
                        "edited": false
                      },
                      "id": "ParseData-Ua8zO"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 385,
                    "positionAbsolute": {
                      "x": 5565.351914880647,
                      "y": 1233.1663723127
                    },
                    "dragging": false
                  }
                ],
                "edges": [
                  {
                    "source": "OpenAIEmbeddings-k2BCE",
                    "sourceHandle": "{dataType:OpenAIEmbeddings,id:OpenAIEmbeddings-k2BCE,name:embeddings,output_types:[Embeddings]}",
                    "target": "CosineSimilarityComponent-gJ3Vw",
                    "targetHandle": "{fieldName:embedding,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Embeddings],type:other}",
                    "data": {
                      "targetHandle": {
                        "fieldName": "embedding",
                        "id": "CosineSimilarityComponent-gJ3Vw",
                        "inputTypes": [
                          "Embeddings"
                        ],
                        "type": "other"
                      },
                      "sourceHandle": {
                        "dataType": "OpenAIEmbeddings",
                        "id": "OpenAIEmbeddings-k2BCE",
                        "name": "embeddings",
                        "output_types": [
                          "Embeddings"
                        ]
                      }
                    },
                    "id": "reactflow__edge-OpenAIEmbeddings-k2BCE{dataType:OpenAIEmbeddings,id:OpenAIEmbeddings-k2BCE,name:embeddings,output_types:[Embeddings]}-CosineSimilarityComponent-gJ3Vw{fieldName:embedding,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Embeddings],type:other}",
                    "selected": true
                  },
                  {
                    "source": "CustomComponent-gD9lk",
                    "sourceHandle": "{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}",
                    "target": "CosineSimilarityComponent-gJ3Vw",
                    "targetHandle": "{fieldName:text1,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Message],type:str}",
                    "data": {
                      "targetHandle": {
                        "fieldName": "text1",
                        "id": "CosineSimilarityComponent-gJ3Vw",
                        "inputTypes": [
                          "Message"
                        ],
                        "type": "str"
                      },
                      "sourceHandle": {
                        "dataType": "CustomComponent",
                        "id": "CustomComponent-gD9lk",
                        "name": "output_message",
                        "output_types": [
                          "Message"
                        ]
                      }
                    },
                    "id": "reactflow__edge-CustomComponent-gD9lk{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}-CosineSimilarityComponent-gJ3Vw{fieldName:text1,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Message],type:str}",
                    "selected": true
                  },
                  {
                    "source": "CustomComponent-gD9lk",
                    "sourceHandle": "{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}",
                    "target": "ScoreCalculatorComponent-8nQC0",
                    "targetHandle": "{fieldName:resposta,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Message],type:str}",
                    "data": {
                      "targetHandle": {
                        "fieldName": "resposta",
                        "id": "ScoreCalculatorComponent-8nQC0",
                        "inputTypes": [
                          "Message"
                        ],
                        "type": "str"
                      },
                      "sourceHandle": {
                        "dataType": "CustomComponent",
                        "id": "CustomComponent-gD9lk",
                        "name": "output_message",
                        "output_types": [
                          "Message"
                        ]
                      }
                    },
                    "id": "reactflow__edge-CustomComponent-gD9lk{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}-ScoreCalculatorComponent-8nQC0{fieldName:resposta,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Message],type:str}",
                    "selected": true
                  },
                  {
                    "source": "ScoreCalculatorComponent-8nQC0",
                    "sourceHandle": "{dataType:ScoreCalculatorComponent,id:ScoreCalculatorComponent-8nQC0,name:final_score,output_types:[Data]}",
                    "target": "ParseData-Ua8zO",
                    "targetHandle": "{fieldName:data,id:ParseData-Ua8zO,inputTypes:[Data],type:other}",
                    "data": {
                      "targetHandle": {
                        "fieldName": "data",
                        "id": "ParseData-Ua8zO",
                        "inputTypes": [
                          "Data"
                        ],
                        "type": "other"
                      },
                      "sourceHandle": {
                        "dataType": "ScoreCalculatorComponent",
                        "id": "ScoreCalculatorComponent-8nQC0",
                        "name": "final_score",
                        "output_types": [
                          "Data"
                        ]
                      }
                    },
                    "id": "reactflow__edge-ScoreCalculatorComponent-8nQC0{dataType:ScoreCalculatorComponent,id:ScoreCalculatorComponent-8nQC0,name:final_score,output_types:[Data]}-ParseData-Ua8zO{fieldName:data,id:ParseData-Ua8zO,inputTypes:[Data],type:other}",
                    "selected": true
                  },
                  {
                    "source": "CosineSimilarityComponent-gJ3Vw",
                    "sourceHandle": "{dataType:CosineSimilarityComponent,id:CosineSimilarityComponent-gJ3Vw,name:cosine_similarity,output_types:[Data]}",
                    "target": "ScoreCalculatorComponent-8nQC0",
                    "targetHandle": "{fieldName:llm_score,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Data],type:other}",
                    "data": {
                      "targetHandle": {
                        "fieldName": "llm_score",
                        "id": "ScoreCalculatorComponent-8nQC0",
                        "inputTypes": [
                          "Data"
                        ],
                        "type": "other"
                      },
                      "sourceHandle": {
                        "dataType": "CosineSimilarityComponent",
                        "id": "CosineSimilarityComponent-gJ3Vw",
                        "name": "cosine_similarity",
                        "output_types": [
                          "Data"
                        ]
                      }
                    },
                    "id": "reactflow__edge-CosineSimilarityComponent-gJ3Vw{dataType:CosineSimilarityComponent,id:CosineSimilarityComponent-gJ3Vw,name:cosine_similarity,output_types:[Data]}-ScoreCalculatorComponent-8nQC0{fieldName:llm_score,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Data],type:other}",
                    "selected": true
                  }
                ],
                "viewport": {
                  "zoom": 1,
                  "x": 0,
                  "y": 0
                }
              },
              "is_component": false,
              "name": "Loving Euler",
              "description": "",
              "id": "L5k23"
            },
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "ParseData-M2bQq_text",
                "display_name": "Text",
                "method": "parse_data",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false,
                "proxy": {
                  "id": "ParseData-Ua8zO",
                  "name": "text",
                  "nodeDisplayName": "Parse Data"
                }
              }
            ]
          }
        },
        "id": "GroupNode-Hr6e9",
        "position": {
          "x": 3620.123719765169,
          "y": -621.8225949365144
        },
        "type": "genericNode",
        "width": 384,
        "height": 675,
        "selected": false,
        "positionAbsolute": {
          "x": 3620.123719765169,
          "y": -621.8225949365144
        },
        "dragging": false
      },
      {
        "id": "OpenAIModel-J62uj",
        "type": "genericNode",
        "position": {
          "x": 2383.893837346442,
          "y": -774.0023646629296
        },
        "data": {
          "type": "OpenAIModel",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.base.models.openai_constants import MODEL_NAMES\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "input_value",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "json_mode": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "json_mode",
                "display_name": "JSON Mode",
                "advanced": true,
                "dynamic": false,
                "info": "If True, it will output JSON regardless of passing a schema.",
                "title_case": false,
                "type": "bool"
              },
              "max_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "max_tokens",
                "display_name": "Max Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "title_case": false,
                "type": "int"
              },
              "model_kwargs": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "model_kwargs",
                "display_name": "Model Kwargs",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "dict"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "gpt-4o",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-3.5-turbo-0125"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "gpt-4o",
                "name": "model_name",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "openai_api_base": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_base",
                "display_name": "OpenAI API Base",
                "advanced": true,
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "title_case": false,
                "type": "str"
              },
              "openai_api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "openai_api_key",
                "display_name": "OpenAI API Key",
                "advanced": false,
                "input_types": [],
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "title_case": false,
                "password": true,
                "type": "str"
              },
              "output_schema": {
                "trace_as_input": true,
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "output_schema",
                "display_name": "Schema",
                "advanced": true,
                "dynamic": false,
                "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.",
                "title_case": false,
                "type": "dict"
              },
              "seed": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 1,
                "name": "seed",
                "display_name": "Seed",
                "advanced": true,
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "title_case": false,
                "type": "int"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "stream",
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool"
              },
              "system_message": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "system_message",
                "display_name": "System Message",
                "advanced": true,
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "0.5",
                "name": "temperature",
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float"
              }
            },
            "description": "Generates text using OpenAI LLMs.",
            "icon": "OpenAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "OpenAI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "output_schema",
              "model_name",
              "openai_api_base",
              "openai_api_key",
              "temperature",
              "stream",
              "system_message",
              "seed"
            ],
            "beta": false,
            "edited": false
          },
          "id": "OpenAIModel-J62uj",
          "description": "Generates text using OpenAI LLMs.",
          "display_name": "OpenAI"
        },
        "selected": false,
        "width": 384,
        "height": 623,
        "positionAbsolute": {
          "x": 2383.893837346442,
          "y": -774.0023646629296
        },
        "dragging": false
      },
      {
        "id": "Prompt-DD9sC",
        "type": "genericNode",
        "position": {
          "x": 1803.7820149644563,
          "y": -774.6946808515046
        },
        "data": {
          "type": "Prompt",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "## Context\nCreate an objective and concise text using markdown syntax to complete and fill in the topics and subtopics. The result must meet the defined quality criteria.\n\n### Metadata Structure Received:\n```Example\n# Title\n## Subtitle\n### Subtitle\n//information segment//\n**Main Topics:** \n**Subtopics:**\n**Named Entities:** \n**Specific Concepts:** \n**Definitions and Examples:** \n**Important Quotes:** \n//\n### Subtitle\n//information segment//\n\n```\n## Criteria\n1. Use the information segments as specified in the topics and subtopics to create an objective text.\n2. The results should be formatted in Markdown.\n3. Do not invent information; use only what is in the segments to create the text.\n4. Ensure that the text reflects all relevant information contained in the segments.\n5. Include titles, subtopics, and essential information from each section in the text. Examples and important quotes should be included succinctly and clearly.\n6. The final text must be returned in Portuguese.\n\n## Objective\nGenerate a clear and objective text, based on the information segments, in Markdown format.\n\n### Content Input\nMetadata provided:\n```\n# **Prompt Formatting**\n## **Basic Prompts**\n//information segment//\n**Main Topics:** Prompt formatting, types of prompts.\n**Subtopics:** Zero-shot prompts, few-shot prompts.\n**Named Entities:** None.\n**Specific Concepts:** Prompt formatting, question and answer, zero-shot and few-shot prompts.\n**Definitions and Examples:** Examples of prompt formats and outputs: \"Q: <Question>? A:\", \"This is amazing! // Positive\", with output \"Negative\".\n**Important Quotes:** \"Some large language models have the capability to perform zero-shot prompts, but this depends on the complexity and knowledge of the task in question.\"\n//\n# **Elements of a Prompt**\n//information segment//\n**Main Topics:** Components of a prompt.\n**Subtopics:** Instruction, context, input data, output indicator.\n**Named Entities:** None.\n**Specific Concepts:** Components of a prompt, importance of each component.\n**Definitions and Examples:** Description of each component: \"Instruction - a specific task or instruction you want the model to perform\".\n**Important Quotes:** \"Not all components are necessary for a prompt, and the format depends on the task in question.\"\n//\n```\n### Output Example\n```\n# Prompt Formatting\n## Basic Prompts\nThe formatting of prompts and the types of prompts, such as zero-shot and few-shot prompts, are essential for effective interactions with language models. Classic examples of prompt formatting include the question and answer structure: \"Q: <Question>? A:\", and simple expressions like \"This is amazing! // Positive\", with the response \"Negative\". As noted, \"Some large language models have the capability to perform zero-shot prompts, but this depends on the complexity and the knowledge of the task in question.\"\n\n## Elements of a Prompt\nThe components of a prompt include instruction, context, input data, and output indicator. Each component has its role, for example, the instruction is a specific task or instruction that you want the model to execute, as described: \"Instruction - a specific task or instruction that you want the model to execute\". It is important to note that \"not all components are necessary for a prompt, and the format depends on the task in question.\"\n```\nInput:\n{input}\n\n### Output:\n",
                "name": "template",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "load_from_db": false
              },
              "input": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "input",
                "display_name": "input",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "input"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": false,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false
          },
          "id": "Prompt-DD9sC",
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt"
        },
        "selected": false,
        "width": 384,
        "height": 423,
        "positionAbsolute": {
          "x": 1803.7820149644563,
          "y": -774.6946808515046
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "Prompt-KzjOn",
        "target": "OpenAIModel-KhjwX",
        "sourceHandle": "{dataType:Prompt,id:Prompt-KzjOn,name:prompt,output_types:[Message]}",
        "targetHandle": "{fieldName:input_value,id:OpenAIModel-KhjwX,inputTypes:[Message],type:str}",
        "id": "reactflow__edge-Prompt-KzjOn{dataType:Prompt,id:Prompt-KzjOn,name:prompt,output_types:[Message]}-OpenAIModel-KhjwX{fieldName:input_value,id:OpenAIModel-KhjwX,inputTypes:[Message],type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OpenAIModel-KhjwX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-KzjOn",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "selected": false,
        "className": ""
      },
      {
        "source": "GroupNode-Hr6e9",
        "sourceHandle": "{dataType:GroupNode,id:GroupNode-Hr6e9,name:ParseData-M2bQq_text,output_types:[Message]}",
        "target": "ChatOutput-MnUY8",
        "targetHandle": "{fieldName:input_value,id:ChatOutput-MnUY8,inputTypes:[Message],type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-MnUY8",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "GroupNode",
            "id": "GroupNode-Hr6e9",
            "name": "ParseData-M2bQq_text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-GroupNode-Hr6e9{dataType:GroupNode,id:GroupNode-Hr6e9,name:ParseData-M2bQq_text,output_types:[Message]}-ChatOutput-MnUY8{fieldName:input_value,id:ChatOutput-MnUY8,inputTypes:[Message],type:str}",
        "className": ""
      },
      {
        "source": "Prompt-DD9sC",
        "sourceHandle": "{dataType:Prompt,id:Prompt-DD9sC,name:prompt,output_types:[Message]}",
        "target": "OpenAIModel-J62uj",
        "targetHandle": "{fieldName:input_value,id:OpenAIModel-J62uj,inputTypes:[Message],type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OpenAIModel-J62uj",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-DD9sC",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-DD9sC{dataType:Prompt,id:Prompt-DD9sC,name:prompt,output_types:[Message]}-OpenAIModel-J62uj{fieldName:input_value,id:OpenAIModel-J62uj,inputTypes:[Message],type:str}",
        "className": ""
      },
      {
        "source": "File-gWYsn",
        "sourceHandle": "{dataType:FileComponent,id:File-gWYsn,name:data,output_types:[Data]}",
        "target": "ParseData-mRcCW",
        "targetHandle": "{fieldName:data,id:ParseData-mRcCW,inputTypes:[Data],type:other}",
        "data": {
          "targetHandle": {
            "fieldName": "data",
            "id": "ParseData-mRcCW",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "FileComponent",
            "id": "File-gWYsn",
            "name": "data",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-File-gWYsn{dataType:FileComponent,id:File-gWYsn,name:data,output_types:[Data]}-ParseData-mRcCW{fieldName:data,id:ParseData-mRcCW,inputTypes:[Data],type:other}",
        "className": ""
      },
      {
        "source": "OpenAIModel-J62uj",
        "sourceHandle": "{dataType:OpenAIModel,id:OpenAIModel-J62uj,name:text_output,output_types:[Message]}",
        "target": "ChatOutput-4Nm3e",
        "targetHandle": "{fieldName:input_value,id:ChatOutput-4Nm3e,inputTypes:[Message],type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-4Nm3e",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "OpenAIModel",
            "id": "OpenAIModel-J62uj",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-OpenAIModel-J62uj{dataType:OpenAIModel,id:OpenAIModel-J62uj,name:text_output,output_types:[Message]}-ChatOutput-4Nm3e{fieldName:input_value,id:ChatOutput-4Nm3e,inputTypes:[Message],type:str}",
        "className": ""
      },
      {
        "source": "ChatOutput-4Nm3e",
        "sourceHandle": "{dataType:ChatOutput,id:ChatOutput-4Nm3e,name:message,output_types:[Message]}",
        "target": "GroupNode-Hr6e9",
        "targetHandle": "{fieldName:input_message_CustomComponent-63VyB,id:GroupNode-Hr6e9,inputTypes:[Message],proxy:{field:input_message,id:CustomComponent-gD9lk},type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "input_message_CustomComponent-63VyB",
            "id": "GroupNode-Hr6e9",
            "inputTypes": [
              "Message"
            ],
            "proxy": {
              "field": "input_message",
              "id": "CustomComponent-gD9lk"
            },
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatOutput",
            "id": "ChatOutput-4Nm3e",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatOutput-4Nm3e{dataType:ChatOutput,id:ChatOutput-4Nm3e,name:message,output_types:[Message]}-GroupNode-Hr6e9{fieldName:input_message_CustomComponent-63VyB,id:GroupNode-Hr6e9,inputTypes:[Message],proxy:{field:input_message,id:CustomComponent-gD9lk},type:str}",
        "className": ""
      },
      {
        "source": "ParseData-mRcCW",
        "sourceHandle": "{dataType:ParseData,id:ParseData-mRcCW,name:text,output_types:[Message]}",
        "target": "Prompt-KzjOn",
        "targetHandle": "{fieldName:content,id:Prompt-KzjOn,inputTypes:[Message,Text],type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "content",
            "id": "Prompt-KzjOn",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-mRcCW",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ParseData-mRcCW{dataType:ParseData,id:ParseData-mRcCW,name:text,output_types:[Message]}-Prompt-KzjOn{fieldName:content,id:Prompt-KzjOn,inputTypes:[Message,Text],type:str}",
        "className": ""
      },
      {
        "source": "OpenAIModel-KhjwX",
        "sourceHandle": "{dataType:OpenAIModel,id:OpenAIModel-KhjwX,name:text_output,output_types:[Message]}",
        "target": "Prompt-DD9sC",
        "targetHandle": "{fieldName:input,id:Prompt-DD9sC,inputTypes:[Message,Text],type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "input",
            "id": "Prompt-DD9sC",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "OpenAIModel",
            "id": "OpenAIModel-KhjwX",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-OpenAIModel-KhjwX{dataType:OpenAIModel,id:OpenAIModel-KhjwX,name:text_output,output_types:[Message]}-Prompt-DD9sC{fieldName:input,id:Prompt-DD9sC,inputTypes:[Message,Text],type:str}",
        "className": ""
      }
    ],
    "viewport": {
      "x": -2995.1251950418773,
      "y": 570.0398890230344,
      "zoom": 0.9609515741974268
    }
  },
  "metadata": {
    "ParseData": {
      "count": 1
    },
    "Prompt": {
      "count": 2
    },
    "OpenAIModel": {
      "count": 2
    },
    "ChatOutput": {
      "count": 2
    },
    "File": {
      "count": 1
    },
    "GroupNode": {
      "count": 1
    },
    "total": 9
  },
  "original": {
    "id": "b9cc331d-7985-41ff-aeb6-3482e2de2c80",
    "name": "IA Devs - Desafio 1",
    "description": "Uncover Business Opportunities with NLP.",
    "is_component": false,
    "liked_by_count": "0",
    "downloads_count": "12",
    "metadata": {
      "ParseData": {
        "count": 1
      },
      "Prompt": {
        "count": 2
      },
      "OpenAIModel": {
        "count": 2
      },
      "ChatOutput": {
        "count": 2
      },
      "File": {
        "count": 1
      },
      "GroupNode": {
        "count": 1
      },
      "total": 9
    },
    "last_tested_version": "1.0.7",
    "private": false,
    "data": {
      "nodes": [
        {
          "id": "ParseData-mRcCW",
          "type": "genericNode",
          "position": {
            "x": -230.95905651722933,
            "y": -551.1053652590484
          },
          "data": {
            "type": "ParseData",
            "node": {
              "template": {
                "_type": "Component",
                "data": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "data",
                  "display_name": "Data",
                  "advanced": false,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "The data to convert to text.",
                  "title_case": false,
                  "type": "other"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "sep": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "\n",
                  "name": "sep",
                  "display_name": "Separator",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "template": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "{text}",
                  "name": "template",
                  "display_name": "Template",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Convert Data into plain text following a specified template.",
              "icon": "braces",
              "base_classes": [
                "Message"
              ],
              "display_name": "Parse Data",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "parse_data",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false
                }
              ],
              "field_order": [
                "data",
                "template",
                "sep"
              ],
              "beta": false,
              "edited": false
            },
            "id": "ParseData-mRcCW",
            "description": "Convert Data into plain text following a specified template.",
            "display_name": "Parse Data"
          },
          "selected": false,
          "width": 384,
          "height": 385,
          "dragging": false,
          "positionAbsolute": {
            "x": -230.95905651722933,
            "y": -551.1053652590484
          }
        },
        {
          "id": "Prompt-KzjOn",
          "type": "genericNode",
          "position": {
            "x": 357.74686005723015,
            "y": -802.22142313877
          },
          "data": {
            "type": "Prompt",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "template": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "## Objective of the Prompting\nThe objective of the prompting is to segment a Markdown text into manageable parts, transforming the original text into a template containing only titles and subtitles, with necessary metadata to reconstruct it in chunks.\n\n## Criteria\n1. **Reconstructibility:** Each chunk should allow the complete reconstruction of the original text segment from the contained sentence and metadata.\n2. **Markdown Formatting:** The segmentation results should be presented in Markdown.\n3. **Output in Portuguese:** The output of this prompting should be in Portuguese.\n4. **Keep the examples:** The examples should be preserved in a reduced form, maintaining the original objective.\n\n### Input\n**Markdown Text:** Text formatted using Markdown syntax.\n\n### Context\nConvert the input text into a template containing only titles and subtitles using Markdown syntax. Segment the content into chunks, preserving the original structure of the text. Add necessary metadata for the faithful reconstruction of the original text.\n\n#### Example\nOriginal text:\n```markdown\n# Prompt Engineering Guide\n---\n# **Introduction**\nPrompt engineering is a new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help better understand the features and limitations of large language models (LLMs). Researchers use prompt engineering to improve the capabilities of LLMs in a wide range of tasks, such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompt techniques that interact with LLMs and other tools.\nThis guide covers the fundamentals of prompts to provide an approximate idea of how to use them to interact with and instruct large language models (LLMs).\nAll examples are tested with `text-davinci-003` (using the OpenAI playground), unless otherwise specified. It uses the default settings, i.e., `temperature=0.7` and `top-p=1`.\n```\n\nConverted text:\n```markdown\n# Prompt Engineering Guide\n# **Introduction**\n//chunk//\n**Main Topics:** Prompt engineering, interaction with LLMs (Large Language Models), NLP (Natural Language Processing) tasks.\n**Subtopics:** Introduction to prompt engineering, applications and research topics. Examples and settings.\n**Named Entities:** LMs, LLMs, text-davinci-003, OpenAI.\n**Specific Concepts:** Interaction and instruction of LLMs. Prompt optimization, model capabilities and limitations. Default settings, temperature, top-p.\n**Definitions and Examples:** Examples of tasks such as question answering and arithmetic reasoning. Fundamentals of using prompts with LLMs. Examples tested with `text-davinci-003`.\n**Important Quotes:** \"Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics.\" \"This guide covers the fundamentals of prompts to provide an approximate idea of how to use them to interact with and instruct large language models (LLMs).\"\n//\n```\n\n### Expected Output Format:\n```markdown\n# Title\n# Subtitle\n## Subtitle\n//chunk//\n**Main Topics:** \n**Subtopics:**\n**Named Entities:** \n**Specific Concepts:** \n**Definitions and Examples:** \n**Important Quotes:** \n//\n## Subtitle\n//chunk//\n```\n\n### Input:\n{content}\n\n### Output:\n",
                  "name": "template",
                  "display_name": "Template",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "prompt",
                  "load_from_db": false
                },
                "content": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "content",
                  "display_name": "content",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "content"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": false,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false
            },
            "id": "Prompt-KzjOn",
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt"
          },
          "selected": false,
          "width": 384,
          "height": 423,
          "dragging": false,
          "positionAbsolute": {
            "x": 357.74686005723015,
            "y": -802.22142313877
          }
        },
        {
          "id": "OpenAIModel-KhjwX",
          "type": "genericNode",
          "position": {
            "x": 1072.5048369395813,
            "y": -1239.2445194401093
          },
          "data": {
            "type": "OpenAIModel",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.base.models.openai_constants import MODEL_NAMES\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "input_value",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "json_mode": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "json_mode",
                  "display_name": "JSON Mode",
                  "advanced": true,
                  "dynamic": false,
                  "info": "If True, it will output JSON regardless of passing a schema.",
                  "title_case": false,
                  "type": "bool"
                },
                "max_tokens": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "max_tokens",
                  "display_name": "Max Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                  "title_case": false,
                  "type": "int"
                },
                "model_kwargs": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "model_kwargs",
                  "display_name": "Model Kwargs",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "dict"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "gpt-4o",
                    "gpt-4-turbo",
                    "gpt-4-turbo-preview",
                    "gpt-4",
                    "gpt-3.5-turbo",
                    "gpt-3.5-turbo-0125"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "gpt-4o",
                  "name": "model_name",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "openai_api_base": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_base",
                  "display_name": "OpenAI API Base",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                  "title_case": false,
                  "type": "str"
                },
                "openai_api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_key",
                  "display_name": "OpenAI API Key",
                  "advanced": false,
                  "input_types": [],
                  "dynamic": false,
                  "info": "The OpenAI API Key to use for the OpenAI model.",
                  "title_case": false,
                  "password": true,
                  "type": "str"
                },
                "output_schema": {
                  "trace_as_input": true,
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "output_schema",
                  "display_name": "Schema",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.",
                  "title_case": false,
                  "type": "dict"
                },
                "seed": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 1,
                  "name": "seed",
                  "display_name": "Seed",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The seed controls the reproducibility of the job.",
                  "title_case": false,
                  "type": "int"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "stream",
                  "display_name": "Stream",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool"
                },
                "system_message": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "system_message",
                  "display_name": "System Message",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "0.5",
                  "name": "temperature",
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float"
                }
              },
              "description": "Generates text using OpenAI LLMs.",
              "icon": "OpenAI",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "OpenAI",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "max_tokens",
                "model_kwargs",
                "json_mode",
                "output_schema",
                "model_name",
                "openai_api_base",
                "openai_api_key",
                "temperature",
                "stream",
                "system_message",
                "seed"
              ],
              "beta": false,
              "edited": false
            },
            "id": "OpenAIModel-KhjwX",
            "description": "Generates text using OpenAI LLMs.",
            "display_name": "OpenAI"
          },
          "selected": false,
          "width": 384,
          "height": 621,
          "dragging": false,
          "positionAbsolute": {
            "x": 1072.5048369395813,
            "y": -1239.2445194401093
          }
        },
        {
          "id": "ChatOutput-4Nm3e",
          "type": "genericNode",
          "position": {
            "x": 3005.5918965457686,
            "y": -439.5486037557737
          },
          "data": {
            "type": "ChatOutput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "data_template": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "{text}",
                  "name": "data_template",
                  "display_name": "Data Template",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "title_case": false,
                  "type": "str"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "input_value",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "title_case": false,
                  "type": "str"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Machine",
                  "name": "sender",
                  "display_name": "Sender Type",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Meu Resumo",
                  "name": "sender_name",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "session_id",
                  "display_name": "Session ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Session ID for the message.",
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Display a chat message in the Playground.",
              "icon": "ChatOutput",
              "base_classes": [
                "Message"
              ],
              "display_name": "Chat Output",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false
                }
              ],
              "field_order": [
                "input_value",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "beta": false,
              "edited": false
            },
            "id": "ChatOutput-4Nm3e",
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output"
          },
          "selected": false,
          "width": 384,
          "height": 309,
          "dragging": false,
          "positionAbsolute": {
            "x": 3005.5918965457686,
            "y": -439.5486037557737
          }
        },
        {
          "id": "File-gWYsn",
          "type": "genericNode",
          "position": {
            "x": -913.1015810068881,
            "y": -711.3327440313428
          },
          "data": {
            "type": "FileComponent",
            "node": {
              "template": {
                "_type": "Component",
                "path": {
                  "trace_as_metadata": true,
                  "file_path": "c76703e5-01ab-4967-a4ff-edb42eeb33f9/Prompt Engineering.md",
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx"
                  ],
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "path",
                  "display_name": "Path",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx",
                  "title_case": false,
                  "type": "file"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from pathlib import Path\n\nfrom axiestudio.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, FileInput, Output\nfrom axiestudio.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data if data else \"No data\"\n        return data or Data()\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "silent_errors": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "silent_errors",
                  "display_name": "Silent Errors",
                  "advanced": true,
                  "dynamic": false,
                  "info": "If true, errors will not raise an exception.",
                  "title_case": false,
                  "type": "bool"
                }
              },
              "description": "A generic file loader.",
              "icon": "file-text",
              "base_classes": [
                "Data"
              ],
              "display_name": "File",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "data",
                  "display_name": "Data",
                  "method": "load_file",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false
                }
              ],
              "field_order": [
                "path",
                "silent_errors"
              ],
              "beta": false,
              "edited": true
            },
            "id": "File-gWYsn",
            "description": "A generic file loader.",
            "display_name": "File",
            "edited": false
          },
          "selected": false,
          "width": 384,
          "height": 301,
          "positionAbsolute": {
            "x": -913.1015810068881,
            "y": -711.3327440313428
          },
          "dragging": false
        },
        {
          "id": "ChatOutput-MnUY8",
          "type": "genericNode",
          "position": {
            "x": 4157.718815242517,
            "y": -271.8661846751068
          },
          "data": {
            "type": "ChatOutput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "data_template": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "{text}",
                  "name": "data_template",
                  "display_name": "Data Template",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "title_case": false,
                  "type": "str"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "input_value",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "title_case": false,
                  "type": "str"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Machine",
                  "name": "sender",
                  "display_name": "Sender Type",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Score",
                  "name": "sender_name",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "session_id",
                  "display_name": "Session ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Session ID for the message.",
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Display a chat message in the Playground.",
              "icon": "ChatOutput",
              "base_classes": [
                "Message"
              ],
              "display_name": "Chat Output",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "beta": false,
              "edited": false
            },
            "id": "ChatOutput-MnUY8",
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output"
          },
          "selected": true,
          "width": 384,
          "height": 309,
          "positionAbsolute": {
            "x": 4157.718815242517,
            "y": -271.8661846751068
          },
          "dragging": false
        },
        {
          "data": {
            "id": "GroupNode-Hr6e9",
            "type": "GroupNode",
            "node": {
              "display_name": "Avaliao",
              "documentation": "",
              "description": "Esse bloco carrega a parte de avaliao do flow.\n\n1. Score de similaridade do resumo com o artigo original\n2. Fator de reduo por nmero de caracteres do resumo\n\nScore Final = Similaridade * Reduo",
              "template": {
                "chunk_size_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 1000,
                  "name": "chunk_size",
                  "display_name": "Chunk Size",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "chunk_size"
                  }
                },
                "client_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "client",
                  "display_name": "Client",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "client"
                  }
                },
                "code_OpenAIEmbeddings-B5dsW": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain_openai.embeddings.base import OpenAIEmbeddings\n\nfrom axiestudio.base.embeddings.model import LCEmbeddingsModel\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        SecretStrInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            tiktoken_enabled=self.tiktoken_enable,\n            default_headers=self.default_headers,\n            default_query=self.default_query,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            deployment=self.deployment,\n            embedding_ctx_length=self.embedding_ctx_length,\n            max_retries=self.max_retries,\n            model=self.model,\n            model_kwargs=self.model_kwargs,\n            base_url=self.openai_api_base,\n            api_key=self.openai_api_key,\n            openai_api_type=self.openai_api_type,\n            api_version=self.openai_api_version,\n            organization=self.openai_organization,\n            openai_proxy=self.openai_proxy,\n            timeout=self.request_timeout or None,\n            show_progress_bar=self.show_progress_bar,\n            skip_empty=self.skip_empty,\n            tiktoken_model_name=self.tiktoken_model_name,\n        )\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "code"
                  }
                },
                "default_headers_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "default_headers",
                  "display_name": "Default Headers",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Default headers to use for the API request.",
                  "title_case": false,
                  "type": "dict",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "default_headers"
                  }
                },
                "default_query_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "default_query",
                  "display_name": "Default Query",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Default query parameters to use for the API request.",
                  "title_case": false,
                  "type": "dict",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "default_query"
                  }
                },
                "deployment_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "deployment",
                  "display_name": "Deployment",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "deployment"
                  }
                },
                "embedding_ctx_length_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 1536,
                  "name": "embedding_ctx_length",
                  "display_name": "Embedding Context Length",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "embedding_ctx_length"
                  }
                },
                "max_retries_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 3,
                  "name": "max_retries",
                  "display_name": "Max Retries",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "max_retries"
                  }
                },
                "model_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "options": [
                    "text-embedding-3-small",
                    "text-embedding-3-large",
                    "text-embedding-ada-002"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "text-embedding-3-small",
                  "name": "model",
                  "display_name": "Model",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "model"
                  }
                },
                "model_kwargs_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "model_kwargs",
                  "display_name": "Model Kwargs",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "dict",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "model_kwargs"
                  }
                },
                "openai_api_base_OpenAIEmbeddings-B5dsW": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_base",
                  "display_name": "OpenAI API Base",
                  "advanced": true,
                  "input_types": [],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "openai_api_base"
                  }
                },
                "openai_api_key_OpenAIEmbeddings-B5dsW": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_key",
                  "display_name": "OpenAI API Key",
                  "advanced": false,
                  "input_types": [],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "openai_api_key"
                  }
                },
                "openai_api_type_OpenAIEmbeddings-B5dsW": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_type",
                  "display_name": "OpenAI API Type",
                  "advanced": true,
                  "input_types": [],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "openai_api_type"
                  }
                },
                "openai_api_version_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_version",
                  "display_name": "OpenAI API Version",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "openai_api_version"
                  }
                },
                "openai_organization_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_organization",
                  "display_name": "OpenAI Organization",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "openai_organization"
                  }
                },
                "openai_proxy_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_proxy",
                  "display_name": "OpenAI Proxy",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "openai_proxy"
                  }
                },
                "request_timeout_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "request_timeout",
                  "display_name": "Request Timeout",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "request_timeout"
                  }
                },
                "show_progress_bar_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "show_progress_bar",
                  "display_name": "Show Progress Bar",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "bool",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "show_progress_bar"
                  }
                },
                "skip_empty_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "skip_empty",
                  "display_name": "Skip Empty",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "bool",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "skip_empty"
                  }
                },
                "tiktoken_enable_OpenAIEmbeddings-B5dsW": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": true,
                  "name": "tiktoken_enable",
                  "display_name": "TikToken Enable",
                  "advanced": true,
                  "dynamic": false,
                  "info": "If False, you must have transformers installed.",
                  "title_case": false,
                  "type": "bool",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "tiktoken_enable"
                  }
                },
                "tiktoken_model_name_OpenAIEmbeddings-B5dsW": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "tiktoken_model_name",
                  "display_name": "TikToken Model Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "OpenAIEmbeddings-k2BCE",
                    "field": "tiktoken_model_name"
                  }
                },
                "code_CosineSimilarityComponent-kFO3J": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import MessageTextInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\nfrom typing import List\nimport numpy as np\n\nclass CosineSimilarityComponent(Component):\n    display_name = \"Cosine Similarity Component\"\n    description = \"Calculates cosine similarity between two texts.\"\n    icon = \"cosine\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"Text 1\",\n            info=\"First text input for similarity calculation.\",\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding Model\",\n            input_types=[\"Embeddings\"],\n            info=\"Model to generate embeddings for the texts.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cosine Similarity\", name=\"cosine_similarity\", method=\"calculate_cosine_similarity\"),\n    ]\n\n    def calculate_cosine_similarity(self) -> Data:\n        text1 = self.text1\n        \n        text2 = \"\"\"# Prompt Engineering Guide\n\n---\n\n# **Introduo**\n\nA engenharia de prompts  uma disciplina relativamente nova para desenvolver e otimizar prompts para usar eficientemente modelos de linguagem (LMs) para uma ampla variedade de aplicativos e tpicos de pesquisa. As habilidades imediatas de engenharia ajudam a entender melhor os recursos e as limitaes dos modelos de linguagem grandes (LLMs). Os pesquisadores usam a engenharia de prompt para melhorar a capacidade dos LLMs em uma ampla gama de tarefas comuns e complexas, como resposta a perguntas e raciocnio aritmtico. Os desenvolvedores usam engenharia de prompt para projetar tcnicas de prompt robustas e eficazes que fazem interface com LLMs e outras ferramentas.\n\nEste guia aborda os fundamentos dos prompts para fornecer uma ideia aproximada de como utiliza-los para interagir e instruir modelos de linguagem grandes (LLMs).\n\nTodos os exemplos so testados com`text-davinci-003`(usando o playground do OpenAI), a menos que especificado de outra forma. Ele usa as configuraes padro, ou seja,`temperatura=0.7`e`top-p=1`.\n\n# **Configuraes LLM**\n\nAo trabalhar com prompts, voc estar interagindo com o LLM diretamente ou por meio de uma API. Voc pode configurar alguns parmetros para obter resultados diferentes para seus prompts.\n\n**Temperatura**- Resumindo, quanto menor a`temperatura`, mais determinsticos so os resultados, no sentido de que o prximo token provvel mais alto  sempre escolhido. O aumento da temperatura pode levar a mais aleatoriedade, incentivando sadas mais diversificadas ou criativas. Estamos essencialmente aumentando os pesos dos outros tokens possveis. Em termos de aplicao, podemos querer usar um valor de temperatura mais baixo para tarefas como controle de qualidade baseado em fatos encorajando respostas mais factuais e concisas. Para gerao de poemas ou outras tarefas criativas, pode ser benfico aumentar o valor da temperatura.\n\n**Top_p**- Da mesma forma, com o`top_p`, uma tcnica de amostragem com temperatura chamada amostragem de ncleo, voc pode controlar o grau de determinismo do modelo na gerao de uma resposta. Se voc est procurando respostas exatas e factuais, mantenha isso baixo. Se voc estiver procurando respostas mais diversificadas, aumente para um valor mais alto.\n\nA recomendao geral  alterar um, no ambos.\n\nAntes de comear com alguns exemplos bsicos, lembre-se de que seus resultados podem variar dependendo da verso do LLM que voc est usando.\n\n# **Noes Bsicas de Prompt**\n\n## **Prompts Bsicos**\n\nVoc pode conseguir muito com prompts simples, mas a qualidade dos resultados depende da quantidade de informaes que voc fornece e de quo bem elaboradas so. Um prompt pode conter informaes como*instruo*ou*pergunta*que voc est passando para o modelo e incluir outros detalhes como*contexto*,*entradas*ou*exemplos*. Voc pode usar esses elementos para instruir melhor o modelo e, como resultado, obter melhores resultados.\n\nVamos comear analisando um exemplo bsico de um prompt simples:\n\n*Prompt*\n\n```\nO cu \n```\n\n*Sada:*\n\n```\nazulO cu  azul em um dia claro. Em um dia nublado, o cu pode ser cinza ou branco.\n```\n\nComo voc pode ver, o modelo de linguagem gera uma continuao de strings que fazem sentido no contexto`\"O cu \"`. A sada pode ser inesperada ou distante da tarefa que queremos realizar.\n\nEste exemplo bsico tambm destaca a necessidade de fornecer mais contexto ou instrues sobre o que especificamente queremos alcanar.\n\nVamos tentar melhorar um pouco:\n\n*Prompt:*\n\n```\nComplete a sentena:O cu \n```\n\n*Sada:*\n\n```\nto lindo.\n```\n\nIsto  melhor? Bem, dissemos ao modelo para completar a frase para que o resultado fique muito melhor, pois segue exatamente o que dissemos para fazer (\"complete a frase\"). Essa abordagem de projetar prompts ideais para instruir o modelo a executar uma tarefa  chamada de**engenharia de prompt**.\n\nO exemplo acima  uma ilustrao bsica do que  possvel com LLMs hoje. Os LLMs de hoje so capazes de executar todos os tipos de tarefas avanadas que variam de resumo de texto a raciocnio matemtico e gerao de cdigo.\n\n## **Formatao de prompt**\n\nTentamos um prompt muito simples acima. Um prompt padro tem o seguinte formato:\n\n```\n<Pergunta>?\n```\n\nou\n\n```\n<Instruo>\n```\n\nIsso pode ser formatado em um formato de resposta a perguntas (QA), que  padro em muitos conjuntos de dados de QA, como segue:\n\n```\nQ: <Pergunta>?A:\n```\n\nAo solicitar como o acima, tambm chamado de*prompt de tiro zero*, ou seja, voc est solicitando diretamente ao modelo uma resposta sem nenhum exemplo ou demonstrao sobre a tarefa que deseja realizar. Alguns modelos de linguagem grandes tm a capacidade de executar prompts zero-shot, mas isso depende da complexidade e do conhecimento da tarefa em questo.\n\nDado o formato padro acima, uma tcnica popular e eficaz para solicitao  chamada de*prompt de poucos tiros*, onde fornecemos exemplos (ou seja, demonstraes). Os prompts de poucos tiros podem ser formatados da seguinte maneira:\n\n```\n<Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?\n```\n\nA verso do formato QA ficaria assim:\n\n```\nQ: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A:\n```\n\nLembre-se de que no  necessrio usar o formato QA. O formato do prompt depende da tarefa em mos. Por exemplo, voc pode executar uma tarefa de classificao simples e fornecer exemplares que demonstrem a tarefa da seguinte forma:\n\n*Prompt:*\n\n```\nIsso  incrvel! // PositivoIsto  mau! // NegativoUau, esse filme foi radical! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nOs prompts de poucos tiros permitem o aprendizado no contexto, que  a capacidade dos modelos de linguagem de aprender tarefas dadas algumas demonstraes.\n\n# **Elementos de um prompt**\n\n medida que abordamos mais e mais exemplos e aplicativos possveis com a engenharia de prompt, voc notar que existem certos elementos que compem um prompt.\n\nUm prompt pode conter qualquer um dos seguintes componentes:\n\n**Instruo**- uma tarefa ou instruo especfica que voc deseja que o modelo execute\n\n**Contexto**- pode envolver informaes externas ou contexto adicional que pode direcionar o modelo para melhores respostas\n\n**Dados de entrada**-  a entrada ou pergunta para a qual estamos interessados em encontrar uma resposta\n\n**Indicador de sada**- indica o tipo ou formato da sada.\n\nNem todos os componentes so necessrios para um prompt e o formato depende da tarefa em questo. Abordaremos exemplos mais concretos nos prximos guias.\n\n# **Dicas gerais para projetar prompts**\n\nAqui esto algumas dicas para manter em mente ao projetar seus prompts:\n\n### **Comece Simples**\n\nAo comear a criar prompts, voc deve ter em mente que  realmente um processo iterativo que requer muita experimentao para obter os melhores resultados. Usar um playground simples como OpenAI ou Cohere's  um bom ponto de partida.\n\nVoc pode comear com prompts simples e continuar adicionando mais elementos e contexto  medida que busca melhores resultados. O controle de verso do seu prompt ao longo do caminho  vital por esse motivo. Ao ler o guia, voc ver muitos exemplos em que a especificidade, a simplicidade e a conciso geralmente lhe daro melhores resultados.\n\nQuando voc tem uma grande tarefa que envolve muitas subtarefas diferentes, pode tentar dividir a tarefa em subtarefas mais simples e continuar aumentando conforme obtm melhores resultados. Isso evita adicionar muita complexidade ao processo de design do prompt no incio.\n\n### **A instruo**\n\nVoc pode criar prompts eficazes para vrias tarefas simples usando comandos para instruir o modelo sobre o que deseja alcanar, como \"Escrever\", \"Classificar\", \"Resumir\", \"Traduzir\", \"Ordenar\" etc.\n\nTenha em mente que voc tambm precisa experimentar muito para ver o que funciona melhor. Experimente instrues diferentes com palavras-chave, contextos e dados diferentes e veja o que funciona melhor para seu caso de uso e tarefa especficos. Normalmente, quanto mais especfico e relevante for o contexto para a tarefa que voc est tentando executar, melhor. Abordaremos a importncia da amostragem e da adio de mais contexto nos prximos guias.\n\nOutros recomendam que as instrues sejam colocadas no incio do prompt. Tambm  recomendado que algum separador claro como \"###\" seja usado para separar a instruo e o contexto.\n\nPor exemplo:\n\n*Prompt:*\n\n```\n### Instruo ###Traduza o texto abaixo para o espanhol:Texto: \"ol!\"\n```\n\n*Sada:*\n\n```\nHola!\n```\n\n### **Especificidade**\n\nSeja muito especfico sobre a instruo e a tarefa que deseja que o modelo execute. Quanto mais descritivo e detalhado for o prompt, melhores sero os resultados. Isso  particularmente importante quando voc tem um resultado desejado ou estilo de gerao que est buscando. No h tokens ou palavras-chave especficas que levem a melhores resultados.  mais importante ter um bom formato e um prompt descritivo. Na verdade, fornecer exemplos no prompt  muito eficaz para obter a sada desejada em formatos especficos.\n\nAo criar prompts, voc tambm deve ter em mente o tamanho do prompt, pois h limitaes em relao a quo grande ele pode ser. Pensar em quo especfico e detalhado voc deve ser  algo a se considerar. Incluir muitos detalhes desnecessrios no  necessariamente uma boa abordagem. Os detalhes devem ser relevantes e contribuir para a tarefa em mos. Isso  algo que voc precisar experimentar muito. Incentivamos muita experimentao e iterao para otimizar os prompts de seus aplicativos.\n\nComo exemplo, vamos tentar um prompt simples para extrair informaes especficas de um texto.\n\n*Prompt:*\n\n```\nExtraia o nome dos lugares no texto a seguir.Formato desejado:Local: <lista_de_nomes_de_empresa_separados_por_vrgula>Input: \"Embora estes desenvolvimentos sejam encorajadores para os investigadores, muito ainda  um mistrio. Muitas vezes temos uma caixa preta entre o crebro e o efeito que vemos na periferia, diz Henrique Veiga-Fernandes, neuroimunologista do Centro Champalimaud para o Desconhecido em Lisboa. Se queremos utiliz-lo no contexto teraputico, precisamos de facto de perceber o mecanismo.\"\n```\n\n*Sada:*\n\n```\nLocal: Centro Champalimaud para o Desconhecido, Lisboa\n```\n\nO texto de entrada  obtido[neste artigo da Nature](https://www.nature.com/articles/d41586-023-00509-z).\n\n### **Evite Imprecises**\n\nDadas as dicas acima sobre como ser detalhado e melhorar o formato,  fcil cair na armadilha de querer ser muito inteligente sobre os prompts e potencialmente criar descries imprecisas. Geralmente  melhor ser especfico e direto. A analogia aqui  muito semelhante  comunicao eficaz - quanto mais direta, mais eficaz a mensagem  transmitida.\n\nPor exemplo, voc pode estar interessado em aprender o conceito de engenharia de prompt. Voc pode tentar algo como:\n\n```\nExplique o conceito de engenharia de prompt. Mantenha a explicao curta, apenas algumas frases, e no seja muito descritivo.\n```\n\nNo est claro no prompt acima quantas frases usar e qual estilo. Voc ainda pode obter uma boa resposta com o prompt acima, mas o melhor prompt seria aquele que  muito especfico, conciso e direto ao ponto. Algo como:\n\n```\nUse 2 a 3 frases para explicar o conceito de engenharia de prompt a um aluno do ensino mdio.\n```\n\n### **Fazer ou no fazer?**\n\nOutra dica comum ao criar prompts  evitar dizer o que no fazer, mas dizer o que fazer. Isso incentiva mais especificidade e concentra-se nos detalhes que levam a boas respostas do modelo.\n\nAqui est um exemplo de um chatbot de recomendao de filme falhando exatamente no que eu no quero que ele faa por causa de como escrevi a instruo -- focando no que no fazer.\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. NO PEA INTERESSES. NO PEA INFORMAES PESSOAIS.Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nClaro, posso recomendar um filme baseado em seus interesses. Que tipo de filme voc gostaria de assistir? Voc prefere ao, comdia, romance ou outra coisa?\n```\n\nAqui est um prompt melhor:\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. O agente  responsvel por recomendar um filme dos principais filmes de tendncias globais. Deve abster-se de perguntar aos usurios sobre suas preferncias e evitar pedir informaes pessoais. Se o agente no tiver um filme para recomendar, ele deve responder \"Desculpe, no foi possvel encontrar um filme para recomendar hoje.\".Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nDesculpe, no tenho nenhuma informao sobre seus interesses. No entanto, aqui est uma lista dos principais filmes de tendncias globais no momento: [lista de filmes]. Espero que encontre algo que goste!\n```\n\nAlguns dos exemplos acima foram adotados do[artigo \"Melhores prticas para engenharia de prompt com OpenAI API\".](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n\n# **Exemplos de Prompts**\n\nNa seo anterior, apresentamos e fornecemos exemplos bsicos de como solicitar LLMs.\n\nNesta seo, forneceremos mais exemplos de como os prompts so usados para realizar diferentes tarefas e introduziremos os principais conceitos ao longo do caminho. Muitas vezes, a melhor maneira de aprender conceitos  atravs de exemplos. Abaixo, abordamos alguns exemplos de como prompts bem elaborados podem ser usados para executar diferentes tipos de tarefas.\n\nTpicos:\n\n- [Resumo de Texto](https://www.promptingguide.ai/pt/introduction/examples#resumo-de-texto)\n- [Extrao de informaes](https://www.promptingguide.ai/pt/introduction/examples#extra%C3%A7%C3%A3o-de-informa%C3%A7%C3%B5es)\n- [Resposta  pergunta](https://www.promptingguide.ai/pt/introduction/examples#resposta-a-pergunta)\n- [Classificao de texto](https://www.promptingguide.ai/pt/introduction/examples#classifica%C3%A7%C3%A3o-de-texto)\n- [Conversao](https://www.promptingguide.ai/pt/introduction/examples#conversa%C3%A7%C3%A3o)\n- [Gerao de cdigo](https://www.promptingguide.ai/pt/introduction/examples#gera%C3%A7%C3%A3o-de-c%C3%B3digo)\n- [Raciocnio](https://www.promptingguide.ai/pt/introduction/examples#racioc%C3%ADnio)\n\n---\n\n## **Resumo de texto**\n\nUma das tarefas padro na gerao de linguagem natural  o resumo de texto. A sumarizao de texto pode incluir preferncias e domnios diferentes. Na verdade, uma das aplicaes mais promissoras dos modelos de linguagem  a capacidade de resumir artigos e conceitos em resumos rpidos e fceis de ler. Vamos tentar uma tarefa bsica de resumo usando prompts.\n\nDigamos que estou interessado em aprender sobre antibiticos, poderia tentar um prompt como este:\n\n*Prompt:*\n\n```\nExplique os antibiticosA:\n```\n\n*Sada:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.\n```\n\nO \"A:\"  um formato de prompt explcito usado para responder perguntas. Eu usei isso aqui para dizer ao modelo que h um futuro esperado. Neste exemplo, no est claro como isso  til ou no, mas deixaremos isso para exemplos posteriores. Vamos apenas supor que isso  muita informao e queremos resumi-la ainda mais. Na verdade, podemos instruir o modelo a resumir em uma frase da seguinte forma:\n\n*Prompt:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.**Explique o que foi dito acima em uma frase**:\n```\n\n*Sada:*\n\n```\nOs antibiticos so medicamentos usados para tratar infeces bacterianas, matando as bactrias ou impedindo-as de se reproduzir, mas no so eficazes contra vrus e o uso excessivo pode levar  resistncia aos antibiticos.\n```\n\nSem prestar muita ateno  preciso da sada acima, algo que abordaremos em um guia posterior, o modelo tentou resumir o pargrafo em uma frase. Voc pode ficar esperto com as instrues, mas deixaremos isso para um captulo posterior. Sinta-se  vontade para fazer uma pausa aqui e experimentar para ver se obtm melhores resultados.\n\n---\n\n## **Extrao de Informaes**\n\nEmbora os modelos de linguagem sejam treinados para executar a gerao de linguagem natural e tarefas relacionadas, eles tambm so muito capazes de realizar classificao e uma srie de outras tarefas de processamento de linguagem natural (NLP).\n\nAqui est um exemplo de um prompt que extrai informaes de um determinado pargrafo.\n\n*Prompt:*\n\n```\nAs declaraes de contribuio do autor e os agradecimentos em trabalhos de pesquisa devem indicar clara e especificamente se, e em que medida, os autores usaram tecnologias de IA, como ChatGPT, na preparao de seus manuscritos e anlises. Eles tambm devem indicar quais LLMs foram usados. Isso alertar os editores e revisores para examinar os manuscritos com mais cuidado em busca de possveis vieses, imprecises e crditos de origem imprprios. Da mesma forma, os peridicos cientficos devem ser transparentes sobre o uso de LLMs, por exemplo, ao selecionar manuscritos enviados.**Mencione o produto baseado em modelo de linguagem grande mencionado no pargrafo acima**:\n```\n\n*Sada:*\n\n```\nO produto baseado em modelo de linguagem grande mencionado no pargrafo acima  o ChatGPT.\n```\n\nExistem muitas maneiras de melhorar os resultados acima, mas isso j  muito til.\n\nAt agora deve ser bvio que voc pode pedir ao modelo para executar diferentes tarefas simplesmente instruindo-o sobre o que fazer. Esse  um recurso poderoso que os desenvolvedores de produtos de IA j esto usando para criar produtos e experincias poderosos.\n\nFonte do pargrafo:[ChatGPT: cinco prioridades para pesquisa](https://www.nature.com/articles/d41586-023-00288-7)\n\n---\n\n## **Resposta a perguntas**\n\nUma das melhores maneiras de fazer com que o modelo responda a respostas especficas  melhorar o formato do prompt. Conforme abordado anteriormente, um prompt pode combinar instrues, contexto, entrada e indicadores de sada para obter melhores resultados.\n\nEmbora esses componentes no sejam necessrios, torna-se uma boa prtica, pois quanto mais especfico voc for com a instruo, melhores resultados obter. Abaixo est um exemplo de como isso ficaria seguindo um prompt mais estruturado.\n\n*Prompt:*\n\n```\nResponda a pergunta com base no contexto abaixo. Mantenha a resposta curta e concisa. Responda \"No tenho certeza sobre a resposta\" se no tiver certeza da resposta.Contexto: Teplizumab tem suas razes em uma empresa farmacutica de Nova Jersey chamada Ortho Pharmaceutical. L, os cientistas geraram uma verso inicial do anticorpo, apelidada de OKT3. Originalmente proveniente de camundongos, a molcula foi capaz de se ligar  superfcie das clulas T e limitar seu potencial de morte celular. Em 1986, foi aprovado para ajudar a prevenir a rejeio de rgos aps transplantes renais, tornando-se o primeiro anticorpo teraputico permitido para uso humano.Pergunta: De onde veio originalmente o OKT3?Responder:\n```\n\n*Sada:*\n\n```\nCamundongos.\n```\n\nContexto obtido da[Nature](https://www.nature.com/articles/d41586-023-00400-x).\n\n---\n\n## **Classificao de texto**\n\nAt agora, usamos instrues simples para executar uma tarefa. Como um engenheiro de prompt, voc precisar melhorar o fornecimento de melhores instrues. Mas isso no  tudo! Voc tambm descobrir que, para casos de uso mais difceis, apenas fornecer instrues no ser suficiente.  aqui que voc precisa pensar mais sobre o contexto e os diferentes elementos que pode usar em um prompt. Outros elementos que voc pode fornecer so`input data`ou`examples`.\n\nVamos tentar demonstrar isso fornecendo um exemplo de classificao de texto.\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nDemos a instruo para classificar o texto e o modelo respondeu com`'Neutro'`que est correto. No h nada de errado nisso, mas digamos que o que realmente precisamos  que o modelo d o rtulo no formato exato que queremos. Portanto, em vez de`Neutral`, queremos que retorne`neutral`. Como alcanamos isso? Existem diferentes maneiras de fazer isso. Ns nos preocupamos com a especificidade aqui, portanto, quanto mais informaes pudermos fornecer, melhores sero os resultados. Podemos tentar fornecer exemplos para especificar o comportamento correto. Vamos tentar de novo:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento: neutroTexto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nneutro\n```\n\nPerfeito! Desta vez, o modelo retornou`neutro`, que  o rtulo especfico que eu estava procurando. Parece que o exemplo fornecido no prompt ajudou o modelo a ser especfico em sua sada. Para destacar por que s vezes ser especfico  importante, confira este exemplo e identifique o problema:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nQual  o problema aqui?\n\n---\n\n## **Conversao**\n\nTalvez uma das coisas mais interessantes que voc pode conseguir com a engenharia imediata seja instruir o sistema LLM sobre como se comportar, sua inteno e sua identidade. Isso  particularmente til quando voc est construindo sistemas de conversao, como chatbots de atendimento ao cliente.\n\nPor exemplo, vamos criar um sistema de conversao capaz de gerar respostas mais tcnicas e cientficas s perguntas. Observe como estamos dizendo explicitamente como se comportar por meio da instruo. s vezes, isso  chamado de*solicitao de funo*.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. O tom assistente  tcnico e cientfico.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Buracos negros so regies do espao-tempo onde a fora gravitacional  to forte que nada, nem mesmo a luz, consegue escapar dela. Eles so criados quando uma estrela muito massiva morre e seu ncleo colapsa sobre si mesmo, formando uma singularidade de densidade infinita. A intensa gravidade da singularidade atrai toda a matria e radiao ao seu redor, criando o buraco negro.\n```\n\nNosso assistente de pesquisa de IA parece um pouco tcnico demais, certo? Ok, vamos mudar esse comportamento e instruir o sistema a dar respostas mais acessveis.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. As respostas do assistente devem ser fceis de entender mesmo por alunos do ensino fundamental.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Os buracos negros so criados quando uma estrela muito massiva fica sem combustvel e colapsa sobre si mesma. A gravidade da estrela  to forte que nada, nem mesmo a luz, pode escapar de sua atrao. Isso cria um ponto no espao-tempo com uma intensa fora gravitacional chamada buraco negro.\n```\n\nAcho que fizemos algum progresso. Voc pode continuar melhorando. Tenho certeza que se voc adicionar mais exemplos voc pode obter resultados ainda melhores.\n\n---\n\n## **Gerao de Cdigo**\n\nUma aplicao em que os LLMs so bastante eficazes  a gerao de cdigo. O Copilot  um timo exemplo disso. H um grande nmero de tarefas de gerao de cdigo que voc pode executar com prompts inteligentes. Vejamos alguns exemplos abaixo.\n\nPrimeiro, vamos tentar um programa simples que sada o usurio.\n\n*Prompt:*\n\n```\n/*Pergunte ao usurio o nome dele e diga \"Ol\"*/\n```\n\n*Sada:*\n\n```\nlet name = prompt(\"Qual  o seu nome?\");console.log(`Ol, ${name}!`);\n```\n\nVoc pode ver que nem precisamos especificar a linguagem a ser usada.\n\nVamos mudar um pouco os nveis. Quero mostrar a voc como os LLMs podem ser poderosos com um pouco mais de esforo no design dos prompts.\n\n*Prompt:*\n\n```\n\"Tabela departamentos, colunas = [DepartmentId, DepartmentName]Alunos da tabela, colunas = [DepartmentId, StudentId, StudentName]Crie uma consulta MySQL para todos os alunos do Departamento de Cincia da Computao\"\n```\n\n*Sada:*\n\n```\nSELECT AlunoId, AlunoNomeDE alunosWHERE DepartmentId IN (SELECT DepartmentId FROM departamentos WHERE DepartmentName = 'Cincia da Computao');\n```\n\nIsso  muito impressionante. Nesse caso, fornecemos dados sobre o esquema do banco de dados e solicitamos que ele gerasse uma consulta MySQL vlida.\n\n---\n\n## **Raciocnio**\n\nTalvez uma das tarefas mais difceis para um LLM hoje seja aquela que requer alguma forma de raciocnio. O raciocnio  uma das reas que mais me entusiasma devido aos tipos de aplicativos complexos que podem surgir dos LLMs.\n\nHouve algumas melhorias em tarefas envolvendo capacidades matemticas. Dito isso,  importante observar que os LLMs atuais lutam para executar tarefas de raciocnio, portanto, isso requer tcnicas de engenharia de prompt ainda mais avanadas. Abordaremos essas tcnicas avanadas no prximo guia. Por enquanto, abordaremos alguns exemplos bsicos para mostrar as capacidades aritmticas.\n\n*Prompt:*\n\n```\nQuanto  9.000 * 9.000?\n```\n\n*Sada:*\n\n```\n81.000.000\n```\n\nVamos tentar algo mais difcil.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada*\n\n```\nNo, os nmeros mpares neste grupo somam um nmero mpar: 119.\n```\n\nIsso  incorreto! Vamos tentar melhorar isso melhorando o prompt.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.Resolva dividindo o problema em etapas. Primeiro, identifique os nmeros mpares, some-os e indique se o resultado  par ou mpar.\n```\n\n*Sada:*\n\n```\nNmeros mpares: 15, 5, 13, 7, 1Total 4141  um nmero mpar.\n```\n\nMuito melhor, certo? A propsito, tentei isso algumas vezes e o sistema s vezes falha. Se voc fornecer instrues melhores combinadas com exemplos, isso pode ajudar a obter resultados mais precisos.\n\nContinuaremos a incluir mais exemplos de aplicativos comuns nesta seo do guia.\n\nNa prxima seo, abordaremos conceitos e tcnicas de engenharia de prompt ainda mais avanados para melhorar o desempenho em todas essas e em tarefas mais difceis.\n\n# **Zero-Shot Prompting**\n\nOs LLMs hoje treinados em grandes quantidades de dados e sintonizados para seguir instrues so capazes de executar tarefas de tiro zero. Tentamos alguns exemplos de tiro zero na seo anterior. Aqui est um dos exemplos que usamos:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nObserve que no prompt acima no fornecemos nenhum exemplo ao modelo -- esses so os recursos de tiro zero em ao.\n\nO ajuste de instruo demonstrou melhorar o aprendizado de tiro zero[Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). O ajuste de instruo  essencialmente o conceito de modelos de ajuste fino em conjuntos de dados descritos por meio de instrues. Alm disso,[RLHF](https://arxiv.org/abs/1706.03741)(aprendizado por reforo a partir de feedback humano) foi adotado para escalar o ajuste de instrues em que o modelo  alinhado para melhor atender s preferncias humanas. Este desenvolvimento recente alimenta modelos como o ChatGPT. Discutiremos todas essas abordagens e mtodos nas prximas sees.\n\nQuando o tiro zero no funciona,  recomendvel fornecer demonstraes ou exemplos no prompt que levam ao prompt de poucos tiros. Na prxima seo, demonstramos a solicitao de poucos disparos.\n\n# **Few-Shot Prompting**\n\nEmbora os modelos de linguagem grande demonstrem recursos notveis de disparo zero, eles ainda ficam aqum em tarefas mais complexas ao usar a configurao de disparo zero. O prompt de poucos disparos pode ser usado como uma tcnica para permitir o aprendizado no contexto, onde fornecemos demonstraes no prompt para direcionar o modelo para um melhor desempenho. As demonstraes servem de condicionamento para exemplos subsequentes onde gostaramos que o modelo gerasse uma resposta.\n\nDe acordo com[Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf)poucas propriedades de tiro apareceram pela primeira vez quando os modelos foram dimensionados para um tamanho suficiente[(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).\n\nVamos demonstrar a solicitao de poucos disparos por meio de um exemplo apresentado em[Brown et al. 2020](https://arxiv.org/abs/2005.14165). No exemplo, a tarefa  usar corretamente uma nova palavra em uma frase.\n\n*Prompt:*\n\n```\nUm \"whatpu\"  um pequeno animal peludo nativo da Tanznia. Exemplo de frase que usaa palavra whatpu :Estvamos viajando pela frica e vimos esses whatpus muito fofos.\"Farduddlear\" significa pular para cima e para baixo muito rpido. Exemplo de frase que usaa palavra farduddlear :\n```\n\n*Sada:*\n\n```\nQuando ganhamos o jogo, todos farduddleamos em festejo.\n```\n\nPodemos observar que o modelo aprendeu de alguma forma como executar a tarefa fornecendo apenas um exemplo (ou seja, 1-shot). Para tarefas mais difceis, podemos experimentar aumentar as demonstraes (por exemplo, 3 tiros, 5 tiros, 10 tiros, etc.).\n\nSeguindo as descobertas de[Min et al. (2022)](https://arxiv.org/abs/2202.12837), aqui esto mais algumas dicas sobre demonstraes/exemplares ao fazer poucos disparos:\n\n- \"o espao do rtulo e a distribuio do texto de entrada especificado pelas demonstraes so importantes (independentemente de os rtulos estarem corretos para entradas individuais)\"\n- o formato que voc usa tambm desempenha um papel fundamental no desempenho, mesmo que voc use apenas rtulos aleatrios, isso  muito melhor do que nenhum rtulo.\n- resultados adicionais mostram que selecionar rtulos aleatrios de uma distribuio verdadeira de rtulos (em vez de uma distribuio uniforme) tambm ajuda.\n\nVamos experimentar alguns exemplos. Vamos primeiro tentar um exemplo com rtulos aleatrios (o que significa que os rtulos Negativo e Positivo so atribudos aleatoriamente s entradas):\n\n*Prompt:*\n\n```\nIsso  incrvel! // NegativoIsto  mau! // PositivoUau, esse filme foi rad! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nAinda obtemos a resposta correta, mesmo que os rtulos tenham sido randomizados. Observe que tambm mantivemos o formato, o que tambm ajuda. Na verdade, com mais experimentao, parece que os modelos GPT mais recentes que estamos experimentando esto se tornando mais robustos at mesmo para formatos aleatrios. Exemplo:\n\n*Prompt:*\n\n```\nPositivo Isso  incrvel!Isto  mau! NegativoUau, esse filme foi rad!PositivoQue espetculo horrvel! --\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nNo h consistncia no formato acima, mas o modelo ainda previu o rtulo correto. Temos que realizar uma anlise mais completa para confirmar se isso vale para tarefas diferentes e mais complexas, incluindo diferentes variaes de prompts.\n\n### **Limitaes da solicitao de poucos disparos**\n\nO prompt padro de poucos disparos funciona bem para muitas tarefas, mas ainda no  uma tcnica perfeita, especialmente ao lidar com tarefas de raciocnio mais complexas. Vamos demonstrar por que esse  o caso. Voc se lembra do exemplo anterior onde fornecemos a seguinte tarefa:\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\nSe tentarmos isso novamente, o modelo produzir o seguinte:\n\n```\nSim, os nmeros mpares neste grupo somam 107, que  um nmero par.\n```\n\nEsta no  a resposta correta, o que no apenas destaca as limitaes desses sistemas, mas tambm a necessidade de uma engenharia imediata mais avanada.\n\nVamos tentar adicionar alguns exemplos para ver se a solicitao de poucos tiros melhora os resultados.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nA resposta  verdadeira.\n```\n\nIsso no funcionou. Parece que a solicitao de poucos disparos no  suficiente para obter respostas confiveis para esse tipo de problema de raciocnio. O exemplo acima fornece informaes bsicas sobre a tarefa. Se voc olhar mais de perto, o tipo de tarefa que introduzimos envolve mais algumas etapas de raciocnio. Em outras palavras, pode ajudar se dividirmos o problema em etapas e demonstrarmos isso ao modelo. Mais recentemente,[insero de cadeia de pensamento (CoT)](https://arxiv.org/abs/2201.11903)foi popularizada para abordar mais aritmtica complexa, senso comum e tarefas de raciocnio simblico.\n\nNo geral, parece que fornecer exemplos  til para resolver algumas tarefas. Quando a solicitao de disparo zero e a solicitao de poucos disparos no so suficientes, isso pode significar que tudo o que foi aprendido pelo modelo no  suficiente para se sair bem na tarefa. A partir daqui,  recomendvel comear a pensar em ajustar seus modelos ou experimentar tcnicas de solicitao mais avanadas. A seguir, falaremos sobre uma das tcnicas populares de sugesto, chamada de sugesto em cadeia de pensamento, que ganhou muita popularidade.\n\n# **Cadeia-de-Pensamento Prompt**\n\n## **Cadeia-de-Pensamento (CoT) Prompting**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n\nFonte da imagem:[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n\nIntroduzido em[Wei et al. (2022)](https://arxiv.org/abs/2201.11903), a solicitao de cadeia de pensamento (CoT) permite recursos de raciocnio complexos por meio de etapas intermedirias de raciocnio. Voc pode combin-lo com prompts de poucos tiros para obter melhores resultados em tarefas mais complexas que exigem raciocnio antes de responder.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.R: Somando todos os nmeros mpares (17, 19) d 36. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.R: Somando todos os nmeros mpares (11, 13) d 24. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.R: Somando todos os nmeros mpares (17, 9, 13) d 39. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nUau! Podemos ver um resultado perfeito quando fornecemos a etapa de raciocnio. Na verdade, podemos resolver essa tarefa fornecendo ainda menos exemplos, ou seja, apenas um exemplo parece suficiente:\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nTenha em mente que os autores afirmam que esta  uma habilidade emergente que surge com modelos de linguagem suficientemente grandes.\n\n## **Prompt COT Zero-Shot**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)\n\nFonte da imagem:[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n\nUma ideia recente que surgiu mais recentemente  a ideia de[zero-shot CoT](https://arxiv.org/abs/2205.11916)(Kojima et al. 2022) que envolve essencialmente adicionar \"Vamos pensar passo a passo\" para a solicitao original. Vamos tentar um problema simples e ver como o modelo funciona:\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?\n```\n\n*Sada:*\n\n```\n11 mas\n```\n\nA resposta est incorreta! Agora vamos tentar com o prompt especial.\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?Vamos pensar passo a passo.\n```\n\n*Sada:*\n\n```\nPrimeiro, voc comeou com 10 mas.Voc deu 2 mas para o vizinho e 2 para o reparador, ento voc tinha 6 mas sobrando.Ento voc comprou mais 5 mas, ento agora voc tinha 11 mas.Finalmente, voc comeu 1 ma, ento ficaria com 10 mas.\n```\n\n impressionante que esse prompt simples seja eficaz nessa tarefa. Isso  particularmente til onde voc no tem muitos exemplos para usar no prompt.\n\"\"\"\n        \n        \n        embedding_model = self.embedding\n\n        # Generate embeddings for the input texts\n        embedding1 = np.array(embedding_model.embed_query(text1))\n        embedding2 = np.array(embedding_model.embed_query(text2))\n\n        # Calculate cosine similarity manually\n        dot_product = np.dot(embedding1, embedding2)\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n        similarity = dot_product / (norm1 * norm2)\n        \n        result = Data(data={\"cosine_similarity\": similarity})\n\n\n        self.status = result\n        return result\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "proxy": {
                    "id": "CosineSimilarityComponent-gJ3Vw",
                    "field": "code"
                  },
                  "display_name": "Code"
                },
                "code_ScoreCalculatorComponent-yUVde": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import DataInput, MessageTextInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\n\nclass ScoreCalculatorComponent(Component):\n    display_name = \"Score Calculator Component\"\n    description = \"Calculates a score based on the initial LLM score and the length of the response.\"\n    icon = \"calculator\"\n\n    inputs = [\n        DataInput(\n            name=\"llm_score\",\n            display_name=\"LLM Score\",\n            info=\"Initial LLM score.\",\n        ),\n        MessageTextInput(\n            name=\"resposta\",\n            display_name=\"Resposta\",\n            info=\"Response text for the score calculation.\",\n        ),\n        \n    ]\n\n    outputs = [\n        Output(display_name=\"Final Score\", name=\"final_score\", method=\"calculate_score\"),\n    ]\n\n    def calculate_score(self) -> Data:\n        llm_score = self.llm_score.cosine_similarity\n        resposta = self.resposta\n\n        max_chars = 10000  # Limite mximo de caracteres\n        min_score = 0.0  # Score mnimo\n        max_score = 1.0  # Score mximo\n\n        tamanho_resposta = len(resposta)\n\n        if tamanho_resposta >= max_chars:\n            score_final = min_score\n        else:\n            fator_reducao = (max_chars - tamanho_resposta) / max_chars\n            score_final = llm_score * fator_reducao\n            score_final = max(min_score, min(max_score, score_final))\n\n        result = Data(data={\"score_final\": score_final, \"tamanho_resumo\": tamanho_resposta, \"similaridade\": llm_score, \"fator_reducao\": fator_reducao})\n        self.status = result\n        return result",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "ScoreCalculatorComponent-8nQC0",
                    "field": "code"
                  }
                },
                "code_CustomComponent-63VyB": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.io import MessageInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema.message import Message\nfrom typing import List\nimport numpy as np\n\n\nclass MessagePassThroughComponent(Component):\n    display_name = \"Message Pass-Through Component\"\n    description = \"Passes a message through without any modifications.\"\n    icon = \"message\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to pass through.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        input_message = self.input_message\n        \n        result = Message(text=input_message)\n\n        self.status = result\n        return result\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "CustomComponent-gD9lk",
                    "field": "code"
                  }
                },
                "input_message_CustomComponent-63VyB": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "input_message",
                  "display_name": "Input Message",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The message to pass through.",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "CustomComponent-gD9lk",
                    "field": "input_message"
                  }
                },
                "code_ParseData-M2bQq": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "ParseData-Ua8zO",
                    "field": "code"
                  }
                },
                "sep_ParseData-M2bQq": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "\n",
                  "name": "sep",
                  "display_name": "Separator",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "ParseData-Ua8zO",
                    "field": "sep"
                  }
                },
                "template_ParseData-M2bQq": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Score Inicial: {similaridade}\nFator de Reduo: {fator_reducao} (1.0 = sem reduo)\nScore Final: {score_final}\n",
                  "name": "template",
                  "display_name": "Template",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "ParseData-Ua8zO",
                    "field": "template"
                  }
                }
              },
              "flow": {
                "data": {
                  "nodes": [
                    {
                      "id": "OpenAIEmbeddings-k2BCE",
                      "type": "genericNode",
                      "position": {
                        "x": 3930.7495819264814,
                        "y": 1579.261560325239
                      },
                      "data": {
                        "type": "OpenAIEmbeddings",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "chunk_size": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": 1000,
                              "name": "chunk_size",
                              "display_name": "Chunk Size",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "int"
                            },
                            "client": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "client",
                              "display_name": "Client",
                              "advanced": true,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            },
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from langchain_openai.embeddings.base import OpenAIEmbeddings\n\nfrom axiestudio.base.embeddings.model import LCEmbeddingsModel\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=[\n                \"text-embedding-3-small\",\n                \"text-embedding-3-large\",\n                \"text-embedding-ada-002\",\n            ],\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        SecretStrInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            tiktoken_enabled=self.tiktoken_enable,\n            default_headers=self.default_headers,\n            default_query=self.default_query,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            deployment=self.deployment,\n            embedding_ctx_length=self.embedding_ctx_length,\n            max_retries=self.max_retries,\n            model=self.model,\n            model_kwargs=self.model_kwargs,\n            base_url=self.openai_api_base,\n            api_key=self.openai_api_key,\n            openai_api_type=self.openai_api_type,\n            api_version=self.openai_api_version,\n            organization=self.openai_organization,\n            openai_proxy=self.openai_proxy,\n            timeout=self.request_timeout or None,\n            show_progress_bar=self.show_progress_bar,\n            skip_empty=self.skip_empty,\n            tiktoken_model_name=self.tiktoken_model_name,\n        )\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "default_headers": {
                              "trace_as_input": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": {},
                              "name": "default_headers",
                              "display_name": "Default Headers",
                              "advanced": true,
                              "dynamic": false,
                              "info": "Default headers to use for the API request.",
                              "title_case": false,
                              "type": "dict"
                            },
                            "default_query": {
                              "trace_as_input": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": {},
                              "name": "default_query",
                              "display_name": "Default Query",
                              "advanced": true,
                              "dynamic": false,
                              "info": "Default query parameters to use for the API request.",
                              "title_case": false,
                              "type": "dict"
                            },
                            "deployment": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "deployment",
                              "display_name": "Deployment",
                              "advanced": true,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            },
                            "embedding_ctx_length": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": 1536,
                              "name": "embedding_ctx_length",
                              "display_name": "Embedding Context Length",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "int"
                            },
                            "max_retries": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": 3,
                              "name": "max_retries",
                              "display_name": "Max Retries",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "int"
                            },
                            "model": {
                              "trace_as_metadata": true,
                              "options": [
                                "text-embedding-3-small",
                                "text-embedding-3-large",
                                "text-embedding-ada-002"
                              ],
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "text-embedding-3-small",
                              "name": "model",
                              "display_name": "Model",
                              "advanced": false,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            },
                            "model_kwargs": {
                              "trace_as_input": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": {},
                              "name": "model_kwargs",
                              "display_name": "Model Kwargs",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "dict"
                            },
                            "openai_api_base": {
                              "load_from_db": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "openai_api_base",
                              "display_name": "OpenAI API Base",
                              "advanced": true,
                              "input_types": [],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "password": true,
                              "type": "str"
                            },
                            "openai_api_key": {
                              "load_from_db": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "openai_api_key",
                              "display_name": "OpenAI API Key",
                              "advanced": false,
                              "input_types": [],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "password": true,
                              "type": "str"
                            },
                            "openai_api_type": {
                              "load_from_db": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "openai_api_type",
                              "display_name": "OpenAI API Type",
                              "advanced": true,
                              "input_types": [],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "password": true,
                              "type": "str"
                            },
                            "openai_api_version": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "openai_api_version",
                              "display_name": "OpenAI API Version",
                              "advanced": true,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            },
                            "openai_organization": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "openai_organization",
                              "display_name": "OpenAI Organization",
                              "advanced": true,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            },
                            "openai_proxy": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "openai_proxy",
                              "display_name": "OpenAI Proxy",
                              "advanced": true,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            },
                            "request_timeout": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "request_timeout",
                              "display_name": "Request Timeout",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "float"
                            },
                            "show_progress_bar": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": false,
                              "name": "show_progress_bar",
                              "display_name": "Show Progress Bar",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "bool"
                            },
                            "skip_empty": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": false,
                              "name": "skip_empty",
                              "display_name": "Skip Empty",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "bool"
                            },
                            "tiktoken_enable": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": true,
                              "name": "tiktoken_enable",
                              "display_name": "TikToken Enable",
                              "advanced": true,
                              "dynamic": false,
                              "info": "If False, you must have transformers installed.",
                              "title_case": false,
                              "type": "bool"
                            },
                            "tiktoken_model_name": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "tiktoken_model_name",
                              "display_name": "TikToken Model Name",
                              "advanced": true,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            }
                          },
                          "description": "Generate embeddings using OpenAI models.",
                          "icon": "OpenAI",
                          "base_classes": [
                            "Embeddings"
                          ],
                          "display_name": "OpenAI Embeddings",
                          "documentation": "",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Embeddings"
                              ],
                              "selected": "Embeddings",
                              "name": "embeddings",
                              "display_name": "Embeddings",
                              "method": "build_embeddings",
                              "value": "__UNDEFINED__",
                              "cache": true,
                              "hidden": false
                            }
                          ],
                          "field_order": [
                            "default_headers",
                            "default_query",
                            "chunk_size",
                            "client",
                            "deployment",
                            "embedding_ctx_length",
                            "max_retries",
                            "model",
                            "model_kwargs",
                            "openai_api_base",
                            "openai_api_key",
                            "openai_api_type",
                            "openai_api_version",
                            "openai_organization",
                            "openai_proxy",
                            "request_timeout",
                            "show_progress_bar",
                            "skip_empty",
                            "tiktoken_model_name",
                            "tiktoken_enable"
                          ],
                          "beta": false,
                          "edited": false
                        },
                        "id": "OpenAIEmbeddings-k2BCE"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 395,
                      "positionAbsolute": {
                        "x": 3930.7495819264814,
                        "y": 1579.261560325239
                      },
                      "dragging": false
                    },
                    {
                      "id": "CosineSimilarityComponent-gJ3Vw",
                      "type": "genericNode",
                      "position": {
                        "x": 4449.337686675583,
                        "y": 1356.3189405398239
                      },
                      "data": {
                        "type": "CosineSimilarityComponent",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "embedding": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "embedding",
                              "display_name": "Embedding Model",
                              "advanced": false,
                              "input_types": [
                                "Embeddings"
                              ],
                              "dynamic": false,
                              "info": "Model to generate embeddings for the texts.",
                              "title_case": false,
                              "type": "other"
                            },
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import MessageTextInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\nfrom typing import List\nimport numpy as np\n\nclass CosineSimilarityComponent(Component):\n    display_name = \"Cosine Similarity Component\"\n    description = \"Calculates cosine similarity between two texts.\"\n    icon = \"cosine\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"Text 1\",\n            info=\"First text input for similarity calculation.\",\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding Model\",\n            input_types=[\"Embeddings\"],\n            info=\"Model to generate embeddings for the texts.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Cosine Similarity\", name=\"cosine_similarity\", method=\"calculate_cosine_similarity\"),\n    ]\n\n    def calculate_cosine_similarity(self) -> Data:\n        text1 = self.text1\n        \n        text2 = \"\"\"# Prompt Engineering Guide\n\n---\n\n# **Introduo**\n\nA engenharia de prompts  uma disciplina relativamente nova para desenvolver e otimizar prompts para usar eficientemente modelos de linguagem (LMs) para uma ampla variedade de aplicativos e tpicos de pesquisa. As habilidades imediatas de engenharia ajudam a entender melhor os recursos e as limitaes dos modelos de linguagem grandes (LLMs). Os pesquisadores usam a engenharia de prompt para melhorar a capacidade dos LLMs em uma ampla gama de tarefas comuns e complexas, como resposta a perguntas e raciocnio aritmtico. Os desenvolvedores usam engenharia de prompt para projetar tcnicas de prompt robustas e eficazes que fazem interface com LLMs e outras ferramentas.\n\nEste guia aborda os fundamentos dos prompts para fornecer uma ideia aproximada de como utiliza-los para interagir e instruir modelos de linguagem grandes (LLMs).\n\nTodos os exemplos so testados com`text-davinci-003`(usando o playground do OpenAI), a menos que especificado de outra forma. Ele usa as configuraes padro, ou seja,`temperatura=0.7`e`top-p=1`.\n\n# **Configuraes LLM**\n\nAo trabalhar com prompts, voc estar interagindo com o LLM diretamente ou por meio de uma API. Voc pode configurar alguns parmetros para obter resultados diferentes para seus prompts.\n\n**Temperatura**- Resumindo, quanto menor a`temperatura`, mais determinsticos so os resultados, no sentido de que o prximo token provvel mais alto  sempre escolhido. O aumento da temperatura pode levar a mais aleatoriedade, incentivando sadas mais diversificadas ou criativas. Estamos essencialmente aumentando os pesos dos outros tokens possveis. Em termos de aplicao, podemos querer usar um valor de temperatura mais baixo para tarefas como controle de qualidade baseado em fatos encorajando respostas mais factuais e concisas. Para gerao de poemas ou outras tarefas criativas, pode ser benfico aumentar o valor da temperatura.\n\n**Top_p**- Da mesma forma, com o`top_p`, uma tcnica de amostragem com temperatura chamada amostragem de ncleo, voc pode controlar o grau de determinismo do modelo na gerao de uma resposta. Se voc est procurando respostas exatas e factuais, mantenha isso baixo. Se voc estiver procurando respostas mais diversificadas, aumente para um valor mais alto.\n\nA recomendao geral  alterar um, no ambos.\n\nAntes de comear com alguns exemplos bsicos, lembre-se de que seus resultados podem variar dependendo da verso do LLM que voc est usando.\n\n# **Noes Bsicas de Prompt**\n\n## **Prompts Bsicos**\n\nVoc pode conseguir muito com prompts simples, mas a qualidade dos resultados depende da quantidade de informaes que voc fornece e de quo bem elaboradas so. Um prompt pode conter informaes como*instruo*ou*pergunta*que voc est passando para o modelo e incluir outros detalhes como*contexto*,*entradas*ou*exemplos*. Voc pode usar esses elementos para instruir melhor o modelo e, como resultado, obter melhores resultados.\n\nVamos comear analisando um exemplo bsico de um prompt simples:\n\n*Prompt*\n\n```\nO cu \n```\n\n*Sada:*\n\n```\nazulO cu  azul em um dia claro. Em um dia nublado, o cu pode ser cinza ou branco.\n```\n\nComo voc pode ver, o modelo de linguagem gera uma continuao de strings que fazem sentido no contexto`\"O cu \"`. A sada pode ser inesperada ou distante da tarefa que queremos realizar.\n\nEste exemplo bsico tambm destaca a necessidade de fornecer mais contexto ou instrues sobre o que especificamente queremos alcanar.\n\nVamos tentar melhorar um pouco:\n\n*Prompt:*\n\n```\nComplete a sentena:O cu \n```\n\n*Sada:*\n\n```\nto lindo.\n```\n\nIsto  melhor? Bem, dissemos ao modelo para completar a frase para que o resultado fique muito melhor, pois segue exatamente o que dissemos para fazer (\"complete a frase\"). Essa abordagem de projetar prompts ideais para instruir o modelo a executar uma tarefa  chamada de**engenharia de prompt**.\n\nO exemplo acima  uma ilustrao bsica do que  possvel com LLMs hoje. Os LLMs de hoje so capazes de executar todos os tipos de tarefas avanadas que variam de resumo de texto a raciocnio matemtico e gerao de cdigo.\n\n## **Formatao de prompt**\n\nTentamos um prompt muito simples acima. Um prompt padro tem o seguinte formato:\n\n```\n<Pergunta>?\n```\n\nou\n\n```\n<Instruo>\n```\n\nIsso pode ser formatado em um formato de resposta a perguntas (QA), que  padro em muitos conjuntos de dados de QA, como segue:\n\n```\nQ: <Pergunta>?A:\n```\n\nAo solicitar como o acima, tambm chamado de*prompt de tiro zero*, ou seja, voc est solicitando diretamente ao modelo uma resposta sem nenhum exemplo ou demonstrao sobre a tarefa que deseja realizar. Alguns modelos de linguagem grandes tm a capacidade de executar prompts zero-shot, mas isso depende da complexidade e do conhecimento da tarefa em questo.\n\nDado o formato padro acima, uma tcnica popular e eficaz para solicitao  chamada de*prompt de poucos tiros*, onde fornecemos exemplos (ou seja, demonstraes). Os prompts de poucos tiros podem ser formatados da seguinte maneira:\n\n```\n<Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?<Resposta><Pergunta>?\n```\n\nA verso do formato QA ficaria assim:\n\n```\nQ: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A: <Resposta>Q: <Pergunta>?A:\n```\n\nLembre-se de que no  necessrio usar o formato QA. O formato do prompt depende da tarefa em mos. Por exemplo, voc pode executar uma tarefa de classificao simples e fornecer exemplares que demonstrem a tarefa da seguinte forma:\n\n*Prompt:*\n\n```\nIsso  incrvel! // PositivoIsto  mau! // NegativoUau, esse filme foi radical! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nOs prompts de poucos tiros permitem o aprendizado no contexto, que  a capacidade dos modelos de linguagem de aprender tarefas dadas algumas demonstraes.\n\n# **Elementos de um prompt**\n\n medida que abordamos mais e mais exemplos e aplicativos possveis com a engenharia de prompt, voc notar que existem certos elementos que compem um prompt.\n\nUm prompt pode conter qualquer um dos seguintes componentes:\n\n**Instruo**- uma tarefa ou instruo especfica que voc deseja que o modelo execute\n\n**Contexto**- pode envolver informaes externas ou contexto adicional que pode direcionar o modelo para melhores respostas\n\n**Dados de entrada**-  a entrada ou pergunta para a qual estamos interessados em encontrar uma resposta\n\n**Indicador de sada**- indica o tipo ou formato da sada.\n\nNem todos os componentes so necessrios para um prompt e o formato depende da tarefa em questo. Abordaremos exemplos mais concretos nos prximos guias.\n\n# **Dicas gerais para projetar prompts**\n\nAqui esto algumas dicas para manter em mente ao projetar seus prompts:\n\n### **Comece Simples**\n\nAo comear a criar prompts, voc deve ter em mente que  realmente um processo iterativo que requer muita experimentao para obter os melhores resultados. Usar um playground simples como OpenAI ou Cohere's  um bom ponto de partida.\n\nVoc pode comear com prompts simples e continuar adicionando mais elementos e contexto  medida que busca melhores resultados. O controle de verso do seu prompt ao longo do caminho  vital por esse motivo. Ao ler o guia, voc ver muitos exemplos em que a especificidade, a simplicidade e a conciso geralmente lhe daro melhores resultados.\n\nQuando voc tem uma grande tarefa que envolve muitas subtarefas diferentes, pode tentar dividir a tarefa em subtarefas mais simples e continuar aumentando conforme obtm melhores resultados. Isso evita adicionar muita complexidade ao processo de design do prompt no incio.\n\n### **A instruo**\n\nVoc pode criar prompts eficazes para vrias tarefas simples usando comandos para instruir o modelo sobre o que deseja alcanar, como \"Escrever\", \"Classificar\", \"Resumir\", \"Traduzir\", \"Ordenar\" etc.\n\nTenha em mente que voc tambm precisa experimentar muito para ver o que funciona melhor. Experimente instrues diferentes com palavras-chave, contextos e dados diferentes e veja o que funciona melhor para seu caso de uso e tarefa especficos. Normalmente, quanto mais especfico e relevante for o contexto para a tarefa que voc est tentando executar, melhor. Abordaremos a importncia da amostragem e da adio de mais contexto nos prximos guias.\n\nOutros recomendam que as instrues sejam colocadas no incio do prompt. Tambm  recomendado que algum separador claro como \"###\" seja usado para separar a instruo e o contexto.\n\nPor exemplo:\n\n*Prompt:*\n\n```\n### Instruo ###Traduza o texto abaixo para o espanhol:Texto: \"ol!\"\n```\n\n*Sada:*\n\n```\nHola!\n```\n\n### **Especificidade**\n\nSeja muito especfico sobre a instruo e a tarefa que deseja que o modelo execute. Quanto mais descritivo e detalhado for o prompt, melhores sero os resultados. Isso  particularmente importante quando voc tem um resultado desejado ou estilo de gerao que est buscando. No h tokens ou palavras-chave especficas que levem a melhores resultados.  mais importante ter um bom formato e um prompt descritivo. Na verdade, fornecer exemplos no prompt  muito eficaz para obter a sada desejada em formatos especficos.\n\nAo criar prompts, voc tambm deve ter em mente o tamanho do prompt, pois h limitaes em relao a quo grande ele pode ser. Pensar em quo especfico e detalhado voc deve ser  algo a se considerar. Incluir muitos detalhes desnecessrios no  necessariamente uma boa abordagem. Os detalhes devem ser relevantes e contribuir para a tarefa em mos. Isso  algo que voc precisar experimentar muito. Incentivamos muita experimentao e iterao para otimizar os prompts de seus aplicativos.\n\nComo exemplo, vamos tentar um prompt simples para extrair informaes especficas de um texto.\n\n*Prompt:*\n\n```\nExtraia o nome dos lugares no texto a seguir.Formato desejado:Local: <lista_de_nomes_de_empresa_separados_por_vrgula>Input: \"Embora estes desenvolvimentos sejam encorajadores para os investigadores, muito ainda  um mistrio. Muitas vezes temos uma caixa preta entre o crebro e o efeito que vemos na periferia, diz Henrique Veiga-Fernandes, neuroimunologista do Centro Champalimaud para o Desconhecido em Lisboa. Se queremos utiliz-lo no contexto teraputico, precisamos de facto de perceber o mecanismo.\"\n```\n\n*Sada:*\n\n```\nLocal: Centro Champalimaud para o Desconhecido, Lisboa\n```\n\nO texto de entrada  obtido[neste artigo da Nature](https://www.nature.com/articles/d41586-023-00509-z).\n\n### **Evite Imprecises**\n\nDadas as dicas acima sobre como ser detalhado e melhorar o formato,  fcil cair na armadilha de querer ser muito inteligente sobre os prompts e potencialmente criar descries imprecisas. Geralmente  melhor ser especfico e direto. A analogia aqui  muito semelhante  comunicao eficaz - quanto mais direta, mais eficaz a mensagem  transmitida.\n\nPor exemplo, voc pode estar interessado em aprender o conceito de engenharia de prompt. Voc pode tentar algo como:\n\n```\nExplique o conceito de engenharia de prompt. Mantenha a explicao curta, apenas algumas frases, e no seja muito descritivo.\n```\n\nNo est claro no prompt acima quantas frases usar e qual estilo. Voc ainda pode obter uma boa resposta com o prompt acima, mas o melhor prompt seria aquele que  muito especfico, conciso e direto ao ponto. Algo como:\n\n```\nUse 2 a 3 frases para explicar o conceito de engenharia de prompt a um aluno do ensino mdio.\n```\n\n### **Fazer ou no fazer?**\n\nOutra dica comum ao criar prompts  evitar dizer o que no fazer, mas dizer o que fazer. Isso incentiva mais especificidade e concentra-se nos detalhes que levam a boas respostas do modelo.\n\nAqui est um exemplo de um chatbot de recomendao de filme falhando exatamente no que eu no quero que ele faa por causa de como escrevi a instruo -- focando no que no fazer.\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. NO PEA INTERESSES. NO PEA INFORMAES PESSOAIS.Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nClaro, posso recomendar um filme baseado em seus interesses. Que tipo de filme voc gostaria de assistir? Voc prefere ao, comdia, romance ou outra coisa?\n```\n\nAqui est um prompt melhor:\n\n*Prompt:*\n\n```\nO agente a seguir recomenda filmes para um cliente. O agente  responsvel por recomendar um filme dos principais filmes de tendncias globais. Deve abster-se de perguntar aos usurios sobre suas preferncias e evitar pedir informaes pessoais. Se o agente no tiver um filme para recomendar, ele deve responder \"Desculpe, no foi possvel encontrar um filme para recomendar hoje.\".Cliente: Por favor, recomende um filme baseado nos meus interesses.Agente:\n```\n\n*Sada:*\n\n```\nDesculpe, no tenho nenhuma informao sobre seus interesses. No entanto, aqui est uma lista dos principais filmes de tendncias globais no momento: [lista de filmes]. Espero que encontre algo que goste!\n```\n\nAlguns dos exemplos acima foram adotados do[artigo \"Melhores prticas para engenharia de prompt com OpenAI API\".](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n\n# **Exemplos de Prompts**\n\nNa seo anterior, apresentamos e fornecemos exemplos bsicos de como solicitar LLMs.\n\nNesta seo, forneceremos mais exemplos de como os prompts so usados para realizar diferentes tarefas e introduziremos os principais conceitos ao longo do caminho. Muitas vezes, a melhor maneira de aprender conceitos  atravs de exemplos. Abaixo, abordamos alguns exemplos de como prompts bem elaborados podem ser usados para executar diferentes tipos de tarefas.\n\nTpicos:\n\n- [Resumo de Texto](https://www.promptingguide.ai/pt/introduction/examples#resumo-de-texto)\n- [Extrao de informaes](https://www.promptingguide.ai/pt/introduction/examples#extra%C3%A7%C3%A3o-de-informa%C3%A7%C3%B5es)\n- [Resposta  pergunta](https://www.promptingguide.ai/pt/introduction/examples#resposta-a-pergunta)\n- [Classificao de texto](https://www.promptingguide.ai/pt/introduction/examples#classifica%C3%A7%C3%A3o-de-texto)\n- [Conversao](https://www.promptingguide.ai/pt/introduction/examples#conversa%C3%A7%C3%A3o)\n- [Gerao de cdigo](https://www.promptingguide.ai/pt/introduction/examples#gera%C3%A7%C3%A3o-de-c%C3%B3digo)\n- [Raciocnio](https://www.promptingguide.ai/pt/introduction/examples#racioc%C3%ADnio)\n\n---\n\n## **Resumo de texto**\n\nUma das tarefas padro na gerao de linguagem natural  o resumo de texto. A sumarizao de texto pode incluir preferncias e domnios diferentes. Na verdade, uma das aplicaes mais promissoras dos modelos de linguagem  a capacidade de resumir artigos e conceitos em resumos rpidos e fceis de ler. Vamos tentar uma tarefa bsica de resumo usando prompts.\n\nDigamos que estou interessado em aprender sobre antibiticos, poderia tentar um prompt como este:\n\n*Prompt:*\n\n```\nExplique os antibiticosA:\n```\n\n*Sada:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.\n```\n\nO \"A:\"  um formato de prompt explcito usado para responder perguntas. Eu usei isso aqui para dizer ao modelo que h um futuro esperado. Neste exemplo, no est claro como isso  til ou no, mas deixaremos isso para exemplos posteriores. Vamos apenas supor que isso  muita informao e queremos resumi-la ainda mais. Na verdade, podemos instruir o modelo a resumir em uma frase da seguinte forma:\n\n*Prompt:*\n\n```\nOs antibiticos so um tipo de medicamento usado para tratar infeces bacterianas. Eles funcionam matando as bactrias ou impedindo-as de se reproduzir, permitindo que o sistema imunolgico do corpo combata a infeco. Os antibiticos so geralmente tomados por via oral na forma de comprimidos, cpsulas ou solues lquidas, ou s vezes administrados por via intravenosa. Eles no so eficazes contra infeces virais e us-los de forma inadequada pode levar  resistncia aos antibiticos.**Explique o que foi dito acima em uma frase**:\n```\n\n*Sada:*\n\n```\nOs antibiticos so medicamentos usados para tratar infeces bacterianas, matando as bactrias ou impedindo-as de se reproduzir, mas no so eficazes contra vrus e o uso excessivo pode levar  resistncia aos antibiticos.\n```\n\nSem prestar muita ateno  preciso da sada acima, algo que abordaremos em um guia posterior, o modelo tentou resumir o pargrafo em uma frase. Voc pode ficar esperto com as instrues, mas deixaremos isso para um captulo posterior. Sinta-se  vontade para fazer uma pausa aqui e experimentar para ver se obtm melhores resultados.\n\n---\n\n## **Extrao de Informaes**\n\nEmbora os modelos de linguagem sejam treinados para executar a gerao de linguagem natural e tarefas relacionadas, eles tambm so muito capazes de realizar classificao e uma srie de outras tarefas de processamento de linguagem natural (NLP).\n\nAqui est um exemplo de um prompt que extrai informaes de um determinado pargrafo.\n\n*Prompt:*\n\n```\nAs declaraes de contribuio do autor e os agradecimentos em trabalhos de pesquisa devem indicar clara e especificamente se, e em que medida, os autores usaram tecnologias de IA, como ChatGPT, na preparao de seus manuscritos e anlises. Eles tambm devem indicar quais LLMs foram usados. Isso alertar os editores e revisores para examinar os manuscritos com mais cuidado em busca de possveis vieses, imprecises e crditos de origem imprprios. Da mesma forma, os peridicos cientficos devem ser transparentes sobre o uso de LLMs, por exemplo, ao selecionar manuscritos enviados.**Mencione o produto baseado em modelo de linguagem grande mencionado no pargrafo acima**:\n```\n\n*Sada:*\n\n```\nO produto baseado em modelo de linguagem grande mencionado no pargrafo acima  o ChatGPT.\n```\n\nExistem muitas maneiras de melhorar os resultados acima, mas isso j  muito til.\n\nAt agora deve ser bvio que voc pode pedir ao modelo para executar diferentes tarefas simplesmente instruindo-o sobre o que fazer. Esse  um recurso poderoso que os desenvolvedores de produtos de IA j esto usando para criar produtos e experincias poderosos.\n\nFonte do pargrafo:[ChatGPT: cinco prioridades para pesquisa](https://www.nature.com/articles/d41586-023-00288-7)\n\n---\n\n## **Resposta a perguntas**\n\nUma das melhores maneiras de fazer com que o modelo responda a respostas especficas  melhorar o formato do prompt. Conforme abordado anteriormente, um prompt pode combinar instrues, contexto, entrada e indicadores de sada para obter melhores resultados.\n\nEmbora esses componentes no sejam necessrios, torna-se uma boa prtica, pois quanto mais especfico voc for com a instruo, melhores resultados obter. Abaixo est um exemplo de como isso ficaria seguindo um prompt mais estruturado.\n\n*Prompt:*\n\n```\nResponda a pergunta com base no contexto abaixo. Mantenha a resposta curta e concisa. Responda \"No tenho certeza sobre a resposta\" se no tiver certeza da resposta.Contexto: Teplizumab tem suas razes em uma empresa farmacutica de Nova Jersey chamada Ortho Pharmaceutical. L, os cientistas geraram uma verso inicial do anticorpo, apelidada de OKT3. Originalmente proveniente de camundongos, a molcula foi capaz de se ligar  superfcie das clulas T e limitar seu potencial de morte celular. Em 1986, foi aprovado para ajudar a prevenir a rejeio de rgos aps transplantes renais, tornando-se o primeiro anticorpo teraputico permitido para uso humano.Pergunta: De onde veio originalmente o OKT3?Responder:\n```\n\n*Sada:*\n\n```\nCamundongos.\n```\n\nContexto obtido da[Nature](https://www.nature.com/articles/d41586-023-00400-x).\n\n---\n\n## **Classificao de texto**\n\nAt agora, usamos instrues simples para executar uma tarefa. Como um engenheiro de prompt, voc precisar melhorar o fornecimento de melhores instrues. Mas isso no  tudo! Voc tambm descobrir que, para casos de uso mais difceis, apenas fornecer instrues no ser suficiente.  aqui que voc precisa pensar mais sobre o contexto e os diferentes elementos que pode usar em um prompt. Outros elementos que voc pode fornecer so`input data`ou`examples`.\n\nVamos tentar demonstrar isso fornecendo um exemplo de classificao de texto.\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nDemos a instruo para classificar o texto e o modelo respondeu com`'Neutro'`que est correto. No h nada de errado nisso, mas digamos que o que realmente precisamos  que o modelo d o rtulo no formato exato que queremos. Portanto, em vez de`Neutral`, queremos que retorne`neutral`. Como alcanamos isso? Existem diferentes maneiras de fazer isso. Ns nos preocupamos com a especificidade aqui, portanto, quanto mais informaes pudermos fornecer, melhores sero os resultados. Podemos tentar fornecer exemplos para especificar o comportamento correto. Vamos tentar de novo:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento: neutroTexto: Acho que a comida estava boa.Sentimento:\n```\n\n*Sada:*\n\n```\nneutro\n```\n\nPerfeito! Desta vez, o modelo retornou`neutro`, que  o rtulo especfico que eu estava procurando. Parece que o exemplo fornecido no prompt ajudou o modelo a ser especfico em sua sada. Para destacar por que s vezes ser especfico  importante, confira este exemplo e identifique o problema:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nQual  o problema aqui?\n\n---\n\n## **Conversao**\n\nTalvez uma das coisas mais interessantes que voc pode conseguir com a engenharia imediata seja instruir o sistema LLM sobre como se comportar, sua inteno e sua identidade. Isso  particularmente til quando voc est construindo sistemas de conversao, como chatbots de atendimento ao cliente.\n\nPor exemplo, vamos criar um sistema de conversao capaz de gerar respostas mais tcnicas e cientficas s perguntas. Observe como estamos dizendo explicitamente como se comportar por meio da instruo. s vezes, isso  chamado de*solicitao de funo*.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. O tom assistente  tcnico e cientfico.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Buracos negros so regies do espao-tempo onde a fora gravitacional  to forte que nada, nem mesmo a luz, consegue escapar dela. Eles so criados quando uma estrela muito massiva morre e seu ncleo colapsa sobre si mesmo, formando uma singularidade de densidade infinita. A intensa gravidade da singularidade atrai toda a matria e radiao ao seu redor, criando o buraco negro.\n```\n\nNosso assistente de pesquisa de IA parece um pouco tcnico demais, certo? Ok, vamos mudar esse comportamento e instruir o sistema a dar respostas mais acessveis.\n\n*Prompt:*\n\n```\nA seguir, uma conversa com um assistente de pesquisa de IA. As respostas do assistente devem ser fceis de entender mesmo por alunos do ensino fundamental.Humano: Ol, quem  voc?AI: Saudaes! Eu sou um assistente de pesquisa de IA. Como posso te ajudar hoje?Humano: Voc pode me falar sobre a criao de buracos negros?IA:\n```\n\n*Sada:*\n\n```\nClaro! Os buracos negros so criados quando uma estrela muito massiva fica sem combustvel e colapsa sobre si mesma. A gravidade da estrela  to forte que nada, nem mesmo a luz, pode escapar de sua atrao. Isso cria um ponto no espao-tempo com uma intensa fora gravitacional chamada buraco negro.\n```\n\nAcho que fizemos algum progresso. Voc pode continuar melhorando. Tenho certeza que se voc adicionar mais exemplos voc pode obter resultados ainda melhores.\n\n---\n\n## **Gerao de Cdigo**\n\nUma aplicao em que os LLMs so bastante eficazes  a gerao de cdigo. O Copilot  um timo exemplo disso. H um grande nmero de tarefas de gerao de cdigo que voc pode executar com prompts inteligentes. Vejamos alguns exemplos abaixo.\n\nPrimeiro, vamos tentar um programa simples que sada o usurio.\n\n*Prompt:*\n\n```\n/*Pergunte ao usurio o nome dele e diga \"Ol\"*/\n```\n\n*Sada:*\n\n```\nlet name = prompt(\"Qual  o seu nome?\");console.log(`Ol, ${name}!`);\n```\n\nVoc pode ver que nem precisamos especificar a linguagem a ser usada.\n\nVamos mudar um pouco os nveis. Quero mostrar a voc como os LLMs podem ser poderosos com um pouco mais de esforo no design dos prompts.\n\n*Prompt:*\n\n```\n\"Tabela departamentos, colunas = [DepartmentId, DepartmentName]Alunos da tabela, colunas = [DepartmentId, StudentId, StudentName]Crie uma consulta MySQL para todos os alunos do Departamento de Cincia da Computao\"\n```\n\n*Sada:*\n\n```\nSELECT AlunoId, AlunoNomeDE alunosWHERE DepartmentId IN (SELECT DepartmentId FROM departamentos WHERE DepartmentName = 'Cincia da Computao');\n```\n\nIsso  muito impressionante. Nesse caso, fornecemos dados sobre o esquema do banco de dados e solicitamos que ele gerasse uma consulta MySQL vlida.\n\n---\n\n## **Raciocnio**\n\nTalvez uma das tarefas mais difceis para um LLM hoje seja aquela que requer alguma forma de raciocnio. O raciocnio  uma das reas que mais me entusiasma devido aos tipos de aplicativos complexos que podem surgir dos LLMs.\n\nHouve algumas melhorias em tarefas envolvendo capacidades matemticas. Dito isso,  importante observar que os LLMs atuais lutam para executar tarefas de raciocnio, portanto, isso requer tcnicas de engenharia de prompt ainda mais avanadas. Abordaremos essas tcnicas avanadas no prximo guia. Por enquanto, abordaremos alguns exemplos bsicos para mostrar as capacidades aritmticas.\n\n*Prompt:*\n\n```\nQuanto  9.000 * 9.000?\n```\n\n*Sada:*\n\n```\n81.000.000\n```\n\nVamos tentar algo mais difcil.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada*\n\n```\nNo, os nmeros mpares neste grupo somam um nmero mpar: 119.\n```\n\nIsso  incorreto! Vamos tentar melhorar isso melhorando o prompt.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.Resolva dividindo o problema em etapas. Primeiro, identifique os nmeros mpares, some-os e indique se o resultado  par ou mpar.\n```\n\n*Sada:*\n\n```\nNmeros mpares: 15, 5, 13, 7, 1Total 4141  um nmero mpar.\n```\n\nMuito melhor, certo? A propsito, tentei isso algumas vezes e o sistema s vezes falha. Se voc fornecer instrues melhores combinadas com exemplos, isso pode ajudar a obter resultados mais precisos.\n\nContinuaremos a incluir mais exemplos de aplicativos comuns nesta seo do guia.\n\nNa prxima seo, abordaremos conceitos e tcnicas de engenharia de prompt ainda mais avanados para melhorar o desempenho em todas essas e em tarefas mais difceis.\n\n# **Zero-Shot Prompting**\n\nOs LLMs hoje treinados em grandes quantidades de dados e sintonizados para seguir instrues so capazes de executar tarefas de tiro zero. Tentamos alguns exemplos de tiro zero na seo anterior. Aqui est um dos exemplos que usamos:\n\n*Prompt:*\n\n```\nClassifique o texto em neutro, negativo ou positivo.Texto: Acho que as frias esto boas.Sentimento:\n```\n\n*Sada:*\n\n```\nNeutro\n```\n\nObserve que no prompt acima no fornecemos nenhum exemplo ao modelo -- esses so os recursos de tiro zero em ao.\n\nO ajuste de instruo demonstrou melhorar o aprendizado de tiro zero[Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf). O ajuste de instruo  essencialmente o conceito de modelos de ajuste fino em conjuntos de dados descritos por meio de instrues. Alm disso,[RLHF](https://arxiv.org/abs/1706.03741)(aprendizado por reforo a partir de feedback humano) foi adotado para escalar o ajuste de instrues em que o modelo  alinhado para melhor atender s preferncias humanas. Este desenvolvimento recente alimenta modelos como o ChatGPT. Discutiremos todas essas abordagens e mtodos nas prximas sees.\n\nQuando o tiro zero no funciona,  recomendvel fornecer demonstraes ou exemplos no prompt que levam ao prompt de poucos tiros. Na prxima seo, demonstramos a solicitao de poucos disparos.\n\n# **Few-Shot Prompting**\n\nEmbora os modelos de linguagem grande demonstrem recursos notveis de disparo zero, eles ainda ficam aqum em tarefas mais complexas ao usar a configurao de disparo zero. O prompt de poucos disparos pode ser usado como uma tcnica para permitir o aprendizado no contexto, onde fornecemos demonstraes no prompt para direcionar o modelo para um melhor desempenho. As demonstraes servem de condicionamento para exemplos subsequentes onde gostaramos que o modelo gerasse uma resposta.\n\nDe acordo com[Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf)poucas propriedades de tiro apareceram pela primeira vez quando os modelos foram dimensionados para um tamanho suficiente[(Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361).\n\nVamos demonstrar a solicitao de poucos disparos por meio de um exemplo apresentado em[Brown et al. 2020](https://arxiv.org/abs/2005.14165). No exemplo, a tarefa  usar corretamente uma nova palavra em uma frase.\n\n*Prompt:*\n\n```\nUm \"whatpu\"  um pequeno animal peludo nativo da Tanznia. Exemplo de frase que usaa palavra whatpu :Estvamos viajando pela frica e vimos esses whatpus muito fofos.\"Farduddlear\" significa pular para cima e para baixo muito rpido. Exemplo de frase que usaa palavra farduddlear :\n```\n\n*Sada:*\n\n```\nQuando ganhamos o jogo, todos farduddleamos em festejo.\n```\n\nPodemos observar que o modelo aprendeu de alguma forma como executar a tarefa fornecendo apenas um exemplo (ou seja, 1-shot). Para tarefas mais difceis, podemos experimentar aumentar as demonstraes (por exemplo, 3 tiros, 5 tiros, 10 tiros, etc.).\n\nSeguindo as descobertas de[Min et al. (2022)](https://arxiv.org/abs/2202.12837), aqui esto mais algumas dicas sobre demonstraes/exemplares ao fazer poucos disparos:\n\n- \"o espao do rtulo e a distribuio do texto de entrada especificado pelas demonstraes so importantes (independentemente de os rtulos estarem corretos para entradas individuais)\"\n- o formato que voc usa tambm desempenha um papel fundamental no desempenho, mesmo que voc use apenas rtulos aleatrios, isso  muito melhor do que nenhum rtulo.\n- resultados adicionais mostram que selecionar rtulos aleatrios de uma distribuio verdadeira de rtulos (em vez de uma distribuio uniforme) tambm ajuda.\n\nVamos experimentar alguns exemplos. Vamos primeiro tentar um exemplo com rtulos aleatrios (o que significa que os rtulos Negativo e Positivo so atribudos aleatoriamente s entradas):\n\n*Prompt:*\n\n```\nIsso  incrvel! // NegativoIsto  mau! // PositivoUau, esse filme foi rad! // PositivoQue espetculo horrvel! //\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nAinda obtemos a resposta correta, mesmo que os rtulos tenham sido randomizados. Observe que tambm mantivemos o formato, o que tambm ajuda. Na verdade, com mais experimentao, parece que os modelos GPT mais recentes que estamos experimentando esto se tornando mais robustos at mesmo para formatos aleatrios. Exemplo:\n\n*Prompt:*\n\n```\nPositivo Isso  incrvel!Isto  mau! NegativoUau, esse filme foi rad!PositivoQue espetculo horrvel! --\n```\n\n*Sada:*\n\n```\nNegativo\n```\n\nNo h consistncia no formato acima, mas o modelo ainda previu o rtulo correto. Temos que realizar uma anlise mais completa para confirmar se isso vale para tarefas diferentes e mais complexas, incluindo diferentes variaes de prompts.\n\n### **Limitaes da solicitao de poucos disparos**\n\nO prompt padro de poucos disparos funciona bem para muitas tarefas, mas ainda no  uma tcnica perfeita, especialmente ao lidar com tarefas de raciocnio mais complexas. Vamos demonstrar por que esse  o caso. Voc se lembra do exemplo anterior onde fornecemos a seguinte tarefa:\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\nSe tentarmos isso novamente, o modelo produzir o seguinte:\n\n```\nSim, os nmeros mpares neste grupo somam 107, que  um nmero par.\n```\n\nEsta no  a resposta correta, o que no apenas destaca as limitaes desses sistemas, mas tambm a necessidade de uma engenharia imediata mais avanada.\n\nVamos tentar adicionar alguns exemplos para ver se a solicitao de poucos tiros melhora os resultados.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.A: A resposta  Verdadeira.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.A: A resposta  Falsa.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nA resposta  verdadeira.\n```\n\nIsso no funcionou. Parece que a solicitao de poucos disparos no  suficiente para obter respostas confiveis para esse tipo de problema de raciocnio. O exemplo acima fornece informaes bsicas sobre a tarefa. Se voc olhar mais de perto, o tipo de tarefa que introduzimos envolve mais algumas etapas de raciocnio. Em outras palavras, pode ajudar se dividirmos o problema em etapas e demonstrarmos isso ao modelo. Mais recentemente,[insero de cadeia de pensamento (CoT)](https://arxiv.org/abs/2201.11903)foi popularizada para abordar mais aritmtica complexa, senso comum e tarefas de raciocnio simblico.\n\nNo geral, parece que fornecer exemplos  til para resolver algumas tarefas. Quando a solicitao de disparo zero e a solicitao de poucos disparos no so suficientes, isso pode significar que tudo o que foi aprendido pelo modelo no  suficiente para se sair bem na tarefa. A partir daqui,  recomendvel comear a pensar em ajustar seus modelos ou experimentar tcnicas de solicitao mais avanadas. A seguir, falaremos sobre uma das tcnicas populares de sugesto, chamada de sugesto em cadeia de pensamento, que ganhou muita popularidade.\n\n# **Cadeia-de-Pensamento Prompt**\n\n## **Cadeia-de-Pensamento (CoT) Prompting**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n\nFonte da imagem:[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n\nIntroduzido em[Wei et al. (2022)](https://arxiv.org/abs/2201.11903), a solicitao de cadeia de pensamento (CoT) permite recursos de raciocnio complexos por meio de etapas intermedirias de raciocnio. Voc pode combin-lo com prompts de poucos tiros para obter melhores resultados em tarefas mais complexas que exigem raciocnio antes de responder.\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 17, 10, 19, 4, 8, 12, 24.R: Somando todos os nmeros mpares (17, 19) d 36. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 16, 11, 14, 4, 8, 13, 24.R: Somando todos os nmeros mpares (11, 13) d 24. A resposta  Verdadeiro.Os nmeros mpares neste grupo somam um nmero par: 17, 9, 10, 12, 13, 4, 2.R: Somando todos os nmeros mpares (17, 9, 13) d 39. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nUau! Podemos ver um resultado perfeito quando fornecemos a etapa de raciocnio. Na verdade, podemos resolver essa tarefa fornecendo ainda menos exemplos, ou seja, apenas um exemplo parece suficiente:\n\n*Prompt:*\n\n```\nOs nmeros mpares neste grupo somam um nmero par: 4, 8, 9, 15, 12, 2, 1.R: Somando todos os nmeros mpares (9, 15, 1) d 25. A resposta  Falso.Os nmeros mpares neste grupo somam um nmero par: 15, 32, 5, 13, 82, 7, 1.A:\n```\n\n*Sada:*\n\n```\nSomando todos os nmeros mpares (15, 5, 13, 7, 1) d 41. A resposta  Falso.\n```\n\nTenha em mente que os autores afirmam que esta  uma habilidade emergente que surge com modelos de linguagem suficientemente grandes.\n\n## **Prompt COT Zero-Shot**\n\n[https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)\n\nFonte da imagem:[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n\nUma ideia recente que surgiu mais recentemente  a ideia de[zero-shot CoT](https://arxiv.org/abs/2205.11916)(Kojima et al. 2022) que envolve essencialmente adicionar \"Vamos pensar passo a passo\" para a solicitao original. Vamos tentar um problema simples e ver como o modelo funciona:\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?\n```\n\n*Sada:*\n\n```\n11 mas\n```\n\nA resposta est incorreta! Agora vamos tentar com o prompt especial.\n\n*Prompt:*\n\n```\nFui ao mercado e comprei 10 mas. Dei 2 mas ao vizinho e 2 ao reparador. Ento fui comprar mais 5 mas e comi 1. Com quantas mas fiquei?Vamos pensar passo a passo.\n```\n\n*Sada:*\n\n```\nPrimeiro, voc comeou com 10 mas.Voc deu 2 mas para o vizinho e 2 para o reparador, ento voc tinha 6 mas sobrando.Ento voc comprou mais 5 mas, ento agora voc tinha 11 mas.Finalmente, voc comeu 1 ma, ento ficaria com 10 mas.\n```\n\n impressionante que esse prompt simples seja eficaz nessa tarefa. Isso  particularmente til onde voc no tem muitos exemplos para usar no prompt.\n\"\"\"\n        \n        \n        embedding_model = self.embedding\n\n        # Generate embeddings for the input texts\n        embedding1 = np.array(embedding_model.embed_query(text1))\n        embedding2 = np.array(embedding_model.embed_query(text2))\n\n        # Calculate cosine similarity manually\n        dot_product = np.dot(embedding1, embedding2)\n        norm1 = np.linalg.norm(embedding1)\n        norm2 = np.linalg.norm(embedding2)\n        similarity = dot_product / (norm1 * norm2)\n        \n        result = Data(data={\"cosine_similarity\": similarity})\n\n\n        self.status = result\n        return result\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false
                            },
                            "text1": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "text1",
                              "display_name": "Text 1",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "First text input for similarity calculation.",
                              "title_case": false,
                              "type": "str"
                            }
                          },
                          "description": "Calculates cosine similarity between two texts.",
                          "icon": "cosine",
                          "base_classes": [
                            "Data"
                          ],
                          "display_name": "Cosine Similarity",
                          "documentation": "",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Data"
                              ],
                              "selected": "Data",
                              "name": "cosine_similarity",
                              "display_name": "Cosine Similarity",
                              "method": "calculate_cosine_similarity",
                              "value": "__UNDEFINED__",
                              "cache": true,
                              "hidden": false
                            }
                          ],
                          "field_order": [
                            "text1",
                            "embedding"
                          ],
                          "beta": false,
                          "edited": true
                        },
                        "id": "CosineSimilarityComponent-gJ3Vw",
                        "description": "Calculates cosine similarity between two texts.",
                        "display_name": "Cosine Similarity"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 357,
                      "positionAbsolute": {
                        "x": 4449.337686675583,
                        "y": 1356.3189405398239
                      },
                      "dragging": false
                    },
                    {
                      "id": "ScoreCalculatorComponent-8nQC0",
                      "type": "genericNode",
                      "position": {
                        "x": 4939.6353751285205,
                        "y": 934.604278998075
                      },
                      "data": {
                        "type": "ScoreCalculatorComponent",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "llm_score": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "llm_score",
                              "display_name": "LLM Score",
                              "advanced": false,
                              "input_types": [
                                "Data"
                              ],
                              "dynamic": false,
                              "info": "Initial LLM score.",
                              "title_case": false,
                              "type": "other"
                            },
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from axiestudio.custom import Component\nfrom axiestudio.inputs import DataInput, MessageTextInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema import Data\n\nclass ScoreCalculatorComponent(Component):\n    display_name = \"Score Calculator Component\"\n    description = \"Calculates a score based on the initial LLM score and the length of the response.\"\n    icon = \"calculator\"\n\n    inputs = [\n        DataInput(\n            name=\"llm_score\",\n            display_name=\"LLM Score\",\n            info=\"Initial LLM score.\",\n        ),\n        MessageTextInput(\n            name=\"resposta\",\n            display_name=\"Resposta\",\n            info=\"Response text for the score calculation.\",\n        ),\n        \n    ]\n\n    outputs = [\n        Output(display_name=\"Final Score\", name=\"final_score\", method=\"calculate_score\"),\n    ]\n\n    def calculate_score(self) -> Data:\n        llm_score = self.llm_score.cosine_similarity\n        resposta = self.resposta\n\n        max_chars = 10000  # Limite mximo de caracteres\n        min_score = 0.0  # Score mnimo\n        max_score = 1.0  # Score mximo\n\n        tamanho_resposta = len(resposta)\n\n        if tamanho_resposta >= max_chars:\n            score_final = min_score\n        else:\n            fator_reducao = (max_chars - tamanho_resposta) / max_chars\n            score_final = llm_score * fator_reducao\n            score_final = max(min_score, min(max_score, score_final))\n\n        result = Data(data={\"score_final\": score_final, \"tamanho_resumo\": tamanho_resposta, \"similaridade\": llm_score, \"fator_reducao\": fator_reducao})\n        self.status = result\n        return result",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "resposta": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "resposta",
                              "display_name": "Resposta",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "Response text for the score calculation.",
                              "title_case": false,
                              "type": "str"
                            }
                          },
                          "description": "Calculates a score based on the initial LLM score and the length of the response.",
                          "icon": "calculator",
                          "base_classes": [
                            "Data"
                          ],
                          "display_name": "Custom Component",
                          "documentation": "",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Data"
                              ],
                              "selected": "Data",
                              "name": "final_score",
                              "display_name": "Final Score",
                              "method": "calculate_score",
                              "value": "__UNDEFINED__",
                              "cache": true,
                              "hidden": false
                            }
                          ],
                          "field_order": [
                            "llm_score",
                            "resposta"
                          ],
                          "beta": false,
                          "edited": true
                        },
                        "id": "ScoreCalculatorComponent-8nQC0",
                        "description": "Calculates a score based on the initial LLM score and the length of the response.",
                        "display_name": "Custom Component"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 385,
                      "positionAbsolute": {
                        "x": 4939.6353751285205,
                        "y": 934.604278998075
                      },
                      "dragging": false
                    },
                    {
                      "id": "CustomComponent-gD9lk",
                      "type": "genericNode",
                      "position": {
                        "x": 3916.5450704806644,
                        "y": 991.92079956417
                      },
                      "data": {
                        "type": "CustomComponent",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from axiestudio.custom import Component\nfrom axiestudio.io import MessageInput, HandleInput\nfrom axiestudio.template import Output\nfrom axiestudio.schema.message import Message\nfrom typing import List\nimport numpy as np\n\n\nclass MessagePassThroughComponent(Component):\n    display_name = \"Message Pass-Through Component\"\n    description = \"Passes a message through without any modifications.\"\n    icon = \"message\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_message\",\n            display_name=\"Input Message\",\n            info=\"The message to pass through.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output Message\", name=\"output_message\", method=\"pass_message\"),\n    ]\n\n    def pass_message(self) -> Message:\n        input_message = self.input_message\n        \n        result = Message(text=input_message)\n\n        self.status = result\n        return result\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "input_message": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "input_message",
                              "display_name": "Input Message",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "The message to pass through.",
                              "title_case": false,
                              "type": "str"
                            }
                          },
                          "description": "Passes a message through without any modifications.",
                          "icon": "message",
                          "base_classes": [
                            "Message"
                          ],
                          "display_name": "Message",
                          "documentation": "",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Message"
                              ],
                              "selected": "Message",
                              "name": "output_message",
                              "display_name": "Output Message",
                              "method": "pass_message",
                              "value": "__UNDEFINED__",
                              "cache": true,
                              "hidden": false
                            }
                          ],
                          "field_order": [
                            "input_message"
                          ],
                          "beta": false,
                          "edited": true
                        },
                        "id": "CustomComponent-gD9lk",
                        "description": "Passes a message through without any modifications.",
                        "display_name": "Message"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 337,
                      "dragging": false,
                      "positionAbsolute": {
                        "x": 3916.5450704806644,
                        "y": 991.92079956417
                      }
                    },
                    {
                      "id": "ParseData-Ua8zO",
                      "type": "genericNode",
                      "position": {
                        "x": 5565.351914880647,
                        "y": 1233.1663723127
                      },
                      "data": {
                        "type": "ParseData",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "data": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "data",
                              "display_name": "Data",
                              "advanced": false,
                              "input_types": [
                                "Data"
                              ],
                              "dynamic": false,
                              "info": "The data to convert to text.",
                              "title_case": false,
                              "type": "other"
                            },
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "sep": {
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "\n",
                              "name": "sep",
                              "display_name": "Separator",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str"
                            },
                            "template": {
                              "trace_as_input": true,
                              "multiline": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "Score Inicial: {similaridade}\nFator de Reduo: {fator_reducao} (1.0 = sem reduo)\nScore Final: {score_final}\n",
                              "name": "template",
                              "display_name": "Template",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                              "title_case": false,
                              "type": "str"
                            }
                          },
                          "description": "Convert Data into plain text following a specified template.",
                          "icon": "braces",
                          "base_classes": [
                            "Message"
                          ],
                          "display_name": "Parse Data",
                          "documentation": "",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Message"
                              ],
                              "selected": "Message",
                              "name": "text",
                              "display_name": "Text",
                              "method": "parse_data",
                              "value": "__UNDEFINED__",
                              "cache": true,
                              "hidden": false
                            }
                          ],
                          "field_order": [
                            "data",
                            "template",
                            "sep"
                          ],
                          "beta": false,
                          "edited": false
                        },
                        "id": "ParseData-Ua8zO"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 385,
                      "positionAbsolute": {
                        "x": 5565.351914880647,
                        "y": 1233.1663723127
                      },
                      "dragging": false
                    }
                  ],
                  "edges": [
                    {
                      "source": "OpenAIEmbeddings-k2BCE",
                      "sourceHandle": "{dataType:OpenAIEmbeddings,id:OpenAIEmbeddings-k2BCE,name:embeddings,output_types:[Embeddings]}",
                      "target": "CosineSimilarityComponent-gJ3Vw",
                      "targetHandle": "{fieldName:embedding,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Embeddings],type:other}",
                      "data": {
                        "targetHandle": {
                          "fieldName": "embedding",
                          "id": "CosineSimilarityComponent-gJ3Vw",
                          "inputTypes": [
                            "Embeddings"
                          ],
                          "type": "other"
                        },
                        "sourceHandle": {
                          "dataType": "OpenAIEmbeddings",
                          "id": "OpenAIEmbeddings-k2BCE",
                          "name": "embeddings",
                          "output_types": [
                            "Embeddings"
                          ]
                        }
                      },
                      "id": "reactflow__edge-OpenAIEmbeddings-k2BCE{dataType:OpenAIEmbeddings,id:OpenAIEmbeddings-k2BCE,name:embeddings,output_types:[Embeddings]}-CosineSimilarityComponent-gJ3Vw{fieldName:embedding,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Embeddings],type:other}",
                      "selected": true
                    },
                    {
                      "source": "CustomComponent-gD9lk",
                      "sourceHandle": "{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}",
                      "target": "CosineSimilarityComponent-gJ3Vw",
                      "targetHandle": "{fieldName:text1,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Message],type:str}",
                      "data": {
                        "targetHandle": {
                          "fieldName": "text1",
                          "id": "CosineSimilarityComponent-gJ3Vw",
                          "inputTypes": [
                            "Message"
                          ],
                          "type": "str"
                        },
                        "sourceHandle": {
                          "dataType": "CustomComponent",
                          "id": "CustomComponent-gD9lk",
                          "name": "output_message",
                          "output_types": [
                            "Message"
                          ]
                        }
                      },
                      "id": "reactflow__edge-CustomComponent-gD9lk{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}-CosineSimilarityComponent-gJ3Vw{fieldName:text1,id:CosineSimilarityComponent-gJ3Vw,inputTypes:[Message],type:str}",
                      "selected": true
                    },
                    {
                      "source": "CustomComponent-gD9lk",
                      "sourceHandle": "{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}",
                      "target": "ScoreCalculatorComponent-8nQC0",
                      "targetHandle": "{fieldName:resposta,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Message],type:str}",
                      "data": {
                        "targetHandle": {
                          "fieldName": "resposta",
                          "id": "ScoreCalculatorComponent-8nQC0",
                          "inputTypes": [
                            "Message"
                          ],
                          "type": "str"
                        },
                        "sourceHandle": {
                          "dataType": "CustomComponent",
                          "id": "CustomComponent-gD9lk",
                          "name": "output_message",
                          "output_types": [
                            "Message"
                          ]
                        }
                      },
                      "id": "reactflow__edge-CustomComponent-gD9lk{dataType:CustomComponent,id:CustomComponent-gD9lk,name:output_message,output_types:[Message]}-ScoreCalculatorComponent-8nQC0{fieldName:resposta,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Message],type:str}",
                      "selected": true
                    },
                    {
                      "source": "ScoreCalculatorComponent-8nQC0",
                      "sourceHandle": "{dataType:ScoreCalculatorComponent,id:ScoreCalculatorComponent-8nQC0,name:final_score,output_types:[Data]}",
                      "target": "ParseData-Ua8zO",
                      "targetHandle": "{fieldName:data,id:ParseData-Ua8zO,inputTypes:[Data],type:other}",
                      "data": {
                        "targetHandle": {
                          "fieldName": "data",
                          "id": "ParseData-Ua8zO",
                          "inputTypes": [
                            "Data"
                          ],
                          "type": "other"
                        },
                        "sourceHandle": {
                          "dataType": "ScoreCalculatorComponent",
                          "id": "ScoreCalculatorComponent-8nQC0",
                          "name": "final_score",
                          "output_types": [
                            "Data"
                          ]
                        }
                      },
                      "id": "reactflow__edge-ScoreCalculatorComponent-8nQC0{dataType:ScoreCalculatorComponent,id:ScoreCalculatorComponent-8nQC0,name:final_score,output_types:[Data]}-ParseData-Ua8zO{fieldName:data,id:ParseData-Ua8zO,inputTypes:[Data],type:other}",
                      "selected": true
                    },
                    {
                      "source": "CosineSimilarityComponent-gJ3Vw",
                      "sourceHandle": "{dataType:CosineSimilarityComponent,id:CosineSimilarityComponent-gJ3Vw,name:cosine_similarity,output_types:[Data]}",
                      "target": "ScoreCalculatorComponent-8nQC0",
                      "targetHandle": "{fieldName:llm_score,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Data],type:other}",
                      "data": {
                        "targetHandle": {
                          "fieldName": "llm_score",
                          "id": "ScoreCalculatorComponent-8nQC0",
                          "inputTypes": [
                            "Data"
                          ],
                          "type": "other"
                        },
                        "sourceHandle": {
                          "dataType": "CosineSimilarityComponent",
                          "id": "CosineSimilarityComponent-gJ3Vw",
                          "name": "cosine_similarity",
                          "output_types": [
                            "Data"
                          ]
                        }
                      },
                      "id": "reactflow__edge-CosineSimilarityComponent-gJ3Vw{dataType:CosineSimilarityComponent,id:CosineSimilarityComponent-gJ3Vw,name:cosine_similarity,output_types:[Data]}-ScoreCalculatorComponent-8nQC0{fieldName:llm_score,id:ScoreCalculatorComponent-8nQC0,inputTypes:[Data],type:other}",
                      "selected": true
                    }
                  ],
                  "viewport": {
                    "zoom": 1,
                    "x": 0,
                    "y": 0
                  }
                },
                "is_component": false,
                "name": "Loving Euler",
                "description": "",
                "id": "L5k23"
              },
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "ParseData-M2bQq_text",
                  "display_name": "Text",
                  "method": "parse_data",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false,
                  "proxy": {
                    "id": "ParseData-Ua8zO",
                    "name": "text",
                    "nodeDisplayName": "Parse Data"
                  }
                }
              ]
            }
          },
          "id": "GroupNode-Hr6e9",
          "position": {
            "x": 3620.123719765169,
            "y": -621.8225949365144
          },
          "type": "genericNode",
          "width": 384,
          "height": 675,
          "selected": false,
          "positionAbsolute": {
            "x": 3620.123719765169,
            "y": -621.8225949365144
          },
          "dragging": false
        },
        {
          "id": "OpenAIModel-J62uj",
          "type": "genericNode",
          "position": {
            "x": 2383.893837346442,
            "y": -774.0023646629296
          },
          "data": {
            "type": "OpenAIModel",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.base.models.openai_constants import MODEL_NAMES\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "input_value",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "json_mode": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "json_mode",
                  "display_name": "JSON Mode",
                  "advanced": true,
                  "dynamic": false,
                  "info": "If True, it will output JSON regardless of passing a schema.",
                  "title_case": false,
                  "type": "bool"
                },
                "max_tokens": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "max_tokens",
                  "display_name": "Max Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                  "title_case": false,
                  "type": "int"
                },
                "model_kwargs": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "model_kwargs",
                  "display_name": "Model Kwargs",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "dict"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "gpt-4o",
                    "gpt-4-turbo",
                    "gpt-4-turbo-preview",
                    "gpt-4",
                    "gpt-3.5-turbo",
                    "gpt-3.5-turbo-0125"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "gpt-4o",
                  "name": "model_name",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "openai_api_base": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_base",
                  "display_name": "OpenAI API Base",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                  "title_case": false,
                  "type": "str"
                },
                "openai_api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "openai_api_key",
                  "display_name": "OpenAI API Key",
                  "advanced": false,
                  "input_types": [],
                  "dynamic": false,
                  "info": "The OpenAI API Key to use for the OpenAI model.",
                  "title_case": false,
                  "password": true,
                  "type": "str"
                },
                "output_schema": {
                  "trace_as_input": true,
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "output_schema",
                  "display_name": "Schema",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.",
                  "title_case": false,
                  "type": "dict"
                },
                "seed": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 1,
                  "name": "seed",
                  "display_name": "Seed",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The seed controls the reproducibility of the job.",
                  "title_case": false,
                  "type": "int"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "stream",
                  "display_name": "Stream",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool"
                },
                "system_message": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "system_message",
                  "display_name": "System Message",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "0.5",
                  "name": "temperature",
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float"
                }
              },
              "description": "Generates text using OpenAI LLMs.",
              "icon": "OpenAI",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "OpenAI",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "max_tokens",
                "model_kwargs",
                "json_mode",
                "output_schema",
                "model_name",
                "openai_api_base",
                "openai_api_key",
                "temperature",
                "stream",
                "system_message",
                "seed"
              ],
              "beta": false,
              "edited": false
            },
            "id": "OpenAIModel-J62uj",
            "description": "Generates text using OpenAI LLMs.",
            "display_name": "OpenAI"
          },
          "selected": false,
          "width": 384,
          "height": 623,
          "positionAbsolute": {
            "x": 2383.893837346442,
            "y": -774.0023646629296
          },
          "dragging": false
        },
        {
          "id": "Prompt-DD9sC",
          "type": "genericNode",
          "position": {
            "x": 1803.7820149644563,
            "y": -774.6946808515046
          },
          "data": {
            "type": "Prompt",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "template": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "## Context\nCreate an objective and concise text using markdown syntax to complete and fill in the topics and subtopics. The result must meet the defined quality criteria.\n\n### Metadata Structure Received:\n```Example\n# Title\n## Subtitle\n### Subtitle\n//information segment//\n**Main Topics:** \n**Subtopics:**\n**Named Entities:** \n**Specific Concepts:** \n**Definitions and Examples:** \n**Important Quotes:** \n//\n### Subtitle\n//information segment//\n\n```\n## Criteria\n1. Use the information segments as specified in the topics and subtopics to create an objective text.\n2. The results should be formatted in Markdown.\n3. Do not invent information; use only what is in the segments to create the text.\n4. Ensure that the text reflects all relevant information contained in the segments.\n5. Include titles, subtopics, and essential information from each section in the text. Examples and important quotes should be included succinctly and clearly.\n6. The final text must be returned in Portuguese.\n\n## Objective\nGenerate a clear and objective text, based on the information segments, in Markdown format.\n\n### Content Input\nMetadata provided:\n```\n# **Prompt Formatting**\n## **Basic Prompts**\n//information segment//\n**Main Topics:** Prompt formatting, types of prompts.\n**Subtopics:** Zero-shot prompts, few-shot prompts.\n**Named Entities:** None.\n**Specific Concepts:** Prompt formatting, question and answer, zero-shot and few-shot prompts.\n**Definitions and Examples:** Examples of prompt formats and outputs: \"Q: <Question>? A:\", \"This is amazing! // Positive\", with output \"Negative\".\n**Important Quotes:** \"Some large language models have the capability to perform zero-shot prompts, but this depends on the complexity and knowledge of the task in question.\"\n//\n# **Elements of a Prompt**\n//information segment//\n**Main Topics:** Components of a prompt.\n**Subtopics:** Instruction, context, input data, output indicator.\n**Named Entities:** None.\n**Specific Concepts:** Components of a prompt, importance of each component.\n**Definitions and Examples:** Description of each component: \"Instruction - a specific task or instruction you want the model to perform\".\n**Important Quotes:** \"Not all components are necessary for a prompt, and the format depends on the task in question.\"\n//\n```\n### Output Example\n```\n# Prompt Formatting\n## Basic Prompts\nThe formatting of prompts and the types of prompts, such as zero-shot and few-shot prompts, are essential for effective interactions with language models. Classic examples of prompt formatting include the question and answer structure: \"Q: <Question>? A:\", and simple expressions like \"This is amazing! // Positive\", with the response \"Negative\". As noted, \"Some large language models have the capability to perform zero-shot prompts, but this depends on the complexity and the knowledge of the task in question.\"\n\n## Elements of a Prompt\nThe components of a prompt include instruction, context, input data, and output indicator. Each component has its role, for example, the instruction is a specific task or instruction that you want the model to execute, as described: \"Instruction - a specific task or instruction that you want the model to execute\". It is important to note that \"not all components are necessary for a prompt, and the format depends on the task in question.\"\n```\nInput:\n{input}\n\n### Output:\n",
                  "name": "template",
                  "display_name": "Template",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "prompt",
                  "load_from_db": false
                },
                "input": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "input",
                  "display_name": "input",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "input"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": false,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false
            },
            "id": "Prompt-DD9sC",
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt"
          },
          "selected": false,
          "width": 384,
          "height": 423,
          "positionAbsolute": {
            "x": 1803.7820149644563,
            "y": -774.6946808515046
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "source": "Prompt-KzjOn",
          "target": "OpenAIModel-KhjwX",
          "sourceHandle": "{dataType:Prompt,id:Prompt-KzjOn,name:prompt,output_types:[Message]}",
          "targetHandle": "{fieldName:input_value,id:OpenAIModel-KhjwX,inputTypes:[Message],type:str}",
          "id": "reactflow__edge-Prompt-KzjOn{dataType:Prompt,id:Prompt-KzjOn,name:prompt,output_types:[Message]}-OpenAIModel-KhjwX{fieldName:input_value,id:OpenAIModel-KhjwX,inputTypes:[Message],type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "OpenAIModel-KhjwX",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-KzjOn",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "selected": false,
          "className": ""
        },
        {
          "source": "GroupNode-Hr6e9",
          "sourceHandle": "{dataType:GroupNode,id:GroupNode-Hr6e9,name:ParseData-M2bQq_text,output_types:[Message]}",
          "target": "ChatOutput-MnUY8",
          "targetHandle": "{fieldName:input_value,id:ChatOutput-MnUY8,inputTypes:[Message],type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-MnUY8",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "GroupNode",
              "id": "GroupNode-Hr6e9",
              "name": "ParseData-M2bQq_text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-GroupNode-Hr6e9{dataType:GroupNode,id:GroupNode-Hr6e9,name:ParseData-M2bQq_text,output_types:[Message]}-ChatOutput-MnUY8{fieldName:input_value,id:ChatOutput-MnUY8,inputTypes:[Message],type:str}",
          "className": ""
        },
        {
          "source": "Prompt-DD9sC",
          "sourceHandle": "{dataType:Prompt,id:Prompt-DD9sC,name:prompt,output_types:[Message]}",
          "target": "OpenAIModel-J62uj",
          "targetHandle": "{fieldName:input_value,id:OpenAIModel-J62uj,inputTypes:[Message],type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "OpenAIModel-J62uj",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-DD9sC",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Prompt-DD9sC{dataType:Prompt,id:Prompt-DD9sC,name:prompt,output_types:[Message]}-OpenAIModel-J62uj{fieldName:input_value,id:OpenAIModel-J62uj,inputTypes:[Message],type:str}",
          "className": ""
        },
        {
          "source": "File-gWYsn",
          "sourceHandle": "{dataType:FileComponent,id:File-gWYsn,name:data,output_types:[Data]}",
          "target": "ParseData-mRcCW",
          "targetHandle": "{fieldName:data,id:ParseData-mRcCW,inputTypes:[Data],type:other}",
          "data": {
            "targetHandle": {
              "fieldName": "data",
              "id": "ParseData-mRcCW",
              "inputTypes": [
                "Data"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "FileComponent",
              "id": "File-gWYsn",
              "name": "data",
              "output_types": [
                "Data"
              ]
            }
          },
          "id": "reactflow__edge-File-gWYsn{dataType:FileComponent,id:File-gWYsn,name:data,output_types:[Data]}-ParseData-mRcCW{fieldName:data,id:ParseData-mRcCW,inputTypes:[Data],type:other}",
          "className": ""
        },
        {
          "source": "OpenAIModel-J62uj",
          "sourceHandle": "{dataType:OpenAIModel,id:OpenAIModel-J62uj,name:text_output,output_types:[Message]}",
          "target": "ChatOutput-4Nm3e",
          "targetHandle": "{fieldName:input_value,id:ChatOutput-4Nm3e,inputTypes:[Message],type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-4Nm3e",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "OpenAIModel",
              "id": "OpenAIModel-J62uj",
              "name": "text_output",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-OpenAIModel-J62uj{dataType:OpenAIModel,id:OpenAIModel-J62uj,name:text_output,output_types:[Message]}-ChatOutput-4Nm3e{fieldName:input_value,id:ChatOutput-4Nm3e,inputTypes:[Message],type:str}",
          "className": ""
        },
        {
          "source": "ChatOutput-4Nm3e",
          "sourceHandle": "{dataType:ChatOutput,id:ChatOutput-4Nm3e,name:message,output_types:[Message]}",
          "target": "GroupNode-Hr6e9",
          "targetHandle": "{fieldName:input_message_CustomComponent-63VyB,id:GroupNode-Hr6e9,inputTypes:[Message],proxy:{field:input_message,id:CustomComponent-gD9lk},type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "input_message_CustomComponent-63VyB",
              "id": "GroupNode-Hr6e9",
              "inputTypes": [
                "Message"
              ],
              "proxy": {
                "field": "input_message",
                "id": "CustomComponent-gD9lk"
              },
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatOutput",
              "id": "ChatOutput-4Nm3e",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatOutput-4Nm3e{dataType:ChatOutput,id:ChatOutput-4Nm3e,name:message,output_types:[Message]}-GroupNode-Hr6e9{fieldName:input_message_CustomComponent-63VyB,id:GroupNode-Hr6e9,inputTypes:[Message],proxy:{field:input_message,id:CustomComponent-gD9lk},type:str}",
          "className": ""
        },
        {
          "source": "ParseData-mRcCW",
          "sourceHandle": "{dataType:ParseData,id:ParseData-mRcCW,name:text,output_types:[Message]}",
          "target": "Prompt-KzjOn",
          "targetHandle": "{fieldName:content,id:Prompt-KzjOn,inputTypes:[Message,Text],type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "content",
              "id": "Prompt-KzjOn",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ParseData",
              "id": "ParseData-mRcCW",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ParseData-mRcCW{dataType:ParseData,id:ParseData-mRcCW,name:text,output_types:[Message]}-Prompt-KzjOn{fieldName:content,id:Prompt-KzjOn,inputTypes:[Message,Text],type:str}",
          "className": ""
        },
        {
          "source": "OpenAIModel-KhjwX",
          "sourceHandle": "{dataType:OpenAIModel,id:OpenAIModel-KhjwX,name:text_output,output_types:[Message]}",
          "target": "Prompt-DD9sC",
          "targetHandle": "{fieldName:input,id:Prompt-DD9sC,inputTypes:[Message,Text],type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "input",
              "id": "Prompt-DD9sC",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "OpenAIModel",
              "id": "OpenAIModel-KhjwX",
              "name": "text_output",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-OpenAIModel-KhjwX{dataType:OpenAIModel,id:OpenAIModel-KhjwX,name:text_output,output_types:[Message]}-Prompt-DD9sC{fieldName:input,id:Prompt-DD9sC,inputTypes:[Message,Text],type:str}",
          "className": ""
        }
      ],
      "viewport": {
        "x": -2995.1251950418773,
        "y": 570.0398890230344,
        "zoom": 0.9609515741974268
      }
    },
    "date_created": "2024-07-12T23:33:34.511Z",
    "date_updated": "2024-07-13T01:09:08.696Z",
    "status": "Public",
    "sort": null,
    "user_updated": "f6d08889-7aff-4f6f-9651-a096910f88ab",
    "user_created": {
      "username": "jr3564",
      "first_name": "Rodolfo",
      "last_name": "Martins",
      "id": "f6d08889-7aff-4f6f-9651-a096910f88ab"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:04.567Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 144,
    "converter_version": "1.0.0"
  }
}