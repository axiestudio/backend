{
  "id": "a7df6359-2858-4537-9f11-33cf0d117af9",
  "name": "LLM Data Iterator",
  "description": "Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, store the result, and optionally save the results to a CSV file.\n\n- Define a prompt template using placeholders for data keys (e.g., `{key_1}`, `{key_2}`).\n- Specify the key under which the LLM output will be stored in each data item.\n- Optional: Provide a path to save the processed data as a CSV file.\n (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "COMPONENT",
  "is_component": true,
  "author": {
    "username": "wrench1997",
    "first_name": "ÂÆ∂Êûó",
    "last_name": "Âàò",
    "id": "6b574962-c600-4487-bbba-fc55cde9b0fc",
    "full_name": "ÂÆ∂Êûó Âàò"
  },
  "store_url": "https://www.langflow.store/store/component/a7df6359-2858-4537-9f11-33cf0d117af9",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-08-30T21:35:08.768Z",
    "updated": "2024-08-30T21:35:08.807Z",
    "downloaded": "2025-08-19T17:50:06.050Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.17",
    "private": true,
    "status": "Public"
  },
  "data": {
    "edges": [],
    "nodes": [
      {
        "data": {
          "type": "ToolCallingAgent",
          "node": {
            "template": {
              "_type": "Component",
              "data_list": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "data_list",
                "display_name": "Data List",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "The list of data to iterate over.",
                "title_case": false,
                "type": "other"
              },
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "llm",
                "display_name": "LLM",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "The language model to use for processing.",
                "title_case": false,
                "type": "other"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import csv\r\nimport os\r\nfrom typing import List\r\nfrom loguru import logger\r\n\r\nfrom axiestudio.custom import Component\r\nfrom axiestudio.helpers.data import data_to_text\r\nfrom axiestudio.io import (\r\n    MessageTextInput,\r\n    IntInput,\r\n    DataInput,\r\n    Output,\r\n    MultilineInput,\r\n    HandleInput,\r\n    StrInput,\r\n)\r\nfrom axiestudio.schema import Data\r\nfrom axiestudio.schema.message import Message\r\n\r\n\r\nclass DynamicLLMDataIteratorComponent(Component):\r\n    display_name = \"Dynamic LLM Data Iterator\"\r\n    description = (\r\n        \"Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, \"\r\n        \"store the result, and optionally save the results to a CSV file.\\n\\n\"\r\n        \"- Define a prompt template using placeholders for data keys (e.g., `{key_1}`, `{key_2}`).\\n\"\r\n        \"- Specify the key under which the LLM output will be stored in each data item.\\n\"\r\n        \"- Optional: Provide a path to save the processed data as a CSV file.\\n\"\r\n    )\r\n    icon = \"üîÅ\"\r\n    base_type = \"component\"\r\n\r\n    inputs = [\r\n        DataInput(name=\"data_list\", display_name=\"Data List\", info=\"The list of data to iterate over.\", is_list=True),\r\n        IntInput(name=\"start_index\", display_name=\"Start Index\", info=\"The index to start processing from.\", value=0),\r\n        IntInput(name=\"iteration_count\", display_name=\"Iteration Count\", info=\"Number of items to process. Set to 0 to process all remaining items.\", value=0),\r\n        MultilineInput(\r\n            name=\"prompt_template\",\r\n            display_name=\"Prompt Template\",\r\n            info=\"The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.\",\r\n            value=\"Based on the domain {Domain} and the meta {Meta}, do X\",\r\n        ),\r\n        MessageTextInput(name=\"output_key\", display_name=\"Output Key\", info=\"The key to store the LLM output in the data.\"),\r\n        HandleInput(\r\n            name=\"llm\",\r\n            display_name=\"LLM\",\r\n            input_types=[\"LanguageModel\"],\r\n            info=\"The language model to use for processing.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"file_path\",\r\n            display_name=\"CSV File Path\",\r\n            info=\"The path to save the CSV file. If not provided, no CSV will be saved.\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Processed Data\", name=\"processed_data\", method=\"process_data\"),\r\n    ]\r\n\r\n    def append_to_csv(self, file_path: str, data: dict):\r\n        file_exists = os.path.isfile(file_path)\r\n        \r\n        with open(file_path, 'a', newline='') as csvfile:\r\n            writer = csv.DictWriter(csvfile, fieldnames=data.keys())\r\n            \r\n            if not file_exists:\r\n                writer.writeheader()\r\n            \r\n            writer.writerow(data)\r\n\r\n    def construct_prompt(self, template: str, data: dict) -> str:\r\n        return template.format(**data)\r\n\r\n    def process_data(self) -> List[Data]:\r\n        logger.info(\"Starting data processing\")\r\n        data_list: List[Data] = self.data_list\r\n        start_index: int = max(0, self.start_index)  # Ensure start_index is not negative\r\n        iteration_count: int = self.iteration_count\r\n        prompt_template: str = self.prompt_template\r\n        output_key: str = self.output_key\r\n        file_path: str = self.file_path\r\n\r\n        logger.debug(f\"Number of data items: {len(data_list)}\")\r\n        logger.debug(f\"Start index: {start_index}\")\r\n        logger.debug(f\"Iteration count: {iteration_count}\")\r\n        logger.debug(f\"Prompt template: {prompt_template}\")\r\n        logger.debug(f\"Output key: {output_key}\")\r\n        logger.debug(f\"CSV file path: {file_path}\")\r\n\r\n        llm = self.llm\r\n\r\n        # Adjust iteration_count based on start_index and remaining items\r\n        if iteration_count <= 0 or (start_index + iteration_count) > len(data_list):\r\n            iteration_count = len(data_list) - start_index\r\n        \r\n        end_index = start_index + iteration_count\r\n        logger.info(f\"Will process items from index {start_index} to {end_index - 1}\")\r\n\r\n        processed_data = []\r\n\r\n        for index, item in enumerate(data_list[start_index:end_index], start=start_index):\r\n            logger.debug(f\"Processing item {index}/{end_index - 1}\")\r\n            if not isinstance(item.data, dict):\r\n                logger.error(f\"Data item {index} is not a dictionary\")\r\n                raise ValueError(f\"Data item {index} is not a dictionary\")\r\n\r\n            prompt = self.construct_prompt(prompt_template, item.data)\r\n            logger.debug(f\"Generated prompt: {prompt}\")\r\n            \r\n            try:\r\n                logger.debug(\"Invoking LLM\")\r\n                llm_output = llm.invoke(prompt).content\r\n                logger.debug(f\"LLM output: {llm_output}\")\r\n                new_item = item.data.copy()\r\n                new_item[output_key] = llm_output\r\n                processed_data.append(Data(data=new_item))\r\n                logger.info(f\"Successfully processed item {index}\")\r\n                \r\n                if file_path:\r\n                    self.append_to_csv(file_path, new_item)\r\n                    logger.info(f\"Appended item {index} to CSV\")\r\n            except Exception as e:\r\n                logger.error(f\"Error processing item {index}: {str(e)}\")\r\n                self.status = f\"Error processing item {index}: {str(e)}\"\r\n                return processed_data\r\n\r\n        self.status = processed_data\r\n        logger.info(f\"Finished processing. {len(processed_data)} items processed successfully.\")\r\n        return processed_data",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "file_path": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "test",
                "name": "file_path",
                "display_name": "CSV File Path",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The path to save the CSV file. If not provided, no CSV will be saved.",
                "title_case": false,
                "type": "str"
              },
              "iteration_count": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "0",
                "name": "iteration_count",
                "display_name": "Iteration Count",
                "advanced": false,
                "dynamic": false,
                "info": "Number of items to process. Set to 0 to process all remaining items.",
                "title_case": false,
                "type": "int"
              },
              "output_key": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Output",
                "name": "output_key",
                "display_name": "Output Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The key to store the LLM output in the data.",
                "title_case": false,
                "type": "str"
              },
              "prompt_template": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Based on the domain {Domain}, your response should contain ONLY the name of the country of origin of that domain.",
                "name": "prompt_template",
                "display_name": "Prompt Template",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.",
                "title_case": false,
                "type": "str"
              },
              "start_index": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "0",
                "name": "start_index",
                "display_name": "Start Index",
                "advanced": false,
                "dynamic": false,
                "info": "The index to start processing from.",
                "title_case": false,
                "type": "int"
              }
            },
            "description": "Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, store the result, and optionally save the results to a CSV file.\n\n- Define a prompt template using placeholders for data keys (e.g., `{key_1}`, `{key_2}`).\n- Specify the key under which the LLM output will be stored in each data item.\n- Optional: Provide a path to save the processed data as a CSV file.\n",
            "icon": "üîÅ",
            "base_classes": [
              "Data"
            ],
            "display_name": "LLM Data Iterator",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "processed_data",
                "display_name": "Processed Data",
                "method": "process_data",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "data_list",
              "start_index",
              "iteration_count",
              "prompt_template",
              "output_key",
              "llm",
              "file_path"
            ],
            "beta": false,
            "edited": true,
            "official": false
          },
          "id": "ToolCallingAgent-nbr4t"
        },
        "id": "ToolCallingAgent-nbr4t",
        "position": {
          "x": 0,
          "y": 0
        },
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 1,
      "y": 1,
      "zoom": 1
    }
  },
  "metadata": {
    "ToolCallingAgent": {
      "count": 1
    },
    "total": 1
  },
  "original": {
    "id": "a7df6359-2858-4537-9f11-33cf0d117af9",
    "name": "LLM Data Iterator",
    "description": "Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, store the result, and optionally save the results to a CSV file.\n\n- Define a prompt template using placeholders for data keys (e.g., `{key_1}`, `{key_2}`).\n- Specify the key under which the LLM output will be stored in each data item.\n- Optional: Provide a path to save the processed data as a CSV file.\n",
    "is_component": true,
    "liked_by_count": "0",
    "downloads_count": "0",
    "metadata": {
      "ToolCallingAgent": {
        "count": 1
      },
      "total": 1
    },
    "last_tested_version": "1.0.17",
    "private": true,
    "data": {
      "edges": [],
      "nodes": [
        {
          "data": {
            "type": "ToolCallingAgent",
            "node": {
              "template": {
                "_type": "Component",
                "data_list": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "data_list",
                  "display_name": "Data List",
                  "advanced": false,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "The list of data to iterate over.",
                  "title_case": false,
                  "type": "other"
                },
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "llm",
                  "display_name": "LLM",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "The language model to use for processing.",
                  "title_case": false,
                  "type": "other"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import csv\r\nimport os\r\nfrom typing import List\r\nfrom loguru import logger\r\n\r\nfrom axiestudio.custom import Component\r\nfrom axiestudio.helpers.data import data_to_text\r\nfrom axiestudio.io import (\r\n    MessageTextInput,\r\n    IntInput,\r\n    DataInput,\r\n    Output,\r\n    MultilineInput,\r\n    HandleInput,\r\n    StrInput,\r\n)\r\nfrom axiestudio.schema import Data\r\nfrom axiestudio.schema.message import Message\r\n\r\n\r\nclass DynamicLLMDataIteratorComponent(Component):\r\n    display_name = \"Dynamic LLM Data Iterator\"\r\n    description = (\r\n        \"Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, \"\r\n        \"store the result, and optionally save the results to a CSV file.\\n\\n\"\r\n        \"- Define a prompt template using placeholders for data keys (e.g., `{key_1}`, `{key_2}`).\\n\"\r\n        \"- Specify the key under which the LLM output will be stored in each data item.\\n\"\r\n        \"- Optional: Provide a path to save the processed data as a CSV file.\\n\"\r\n    )\r\n    icon = \"üîÅ\"\r\n    base_type = \"component\"\r\n\r\n    inputs = [\r\n        DataInput(name=\"data_list\", display_name=\"Data List\", info=\"The list of data to iterate over.\", is_list=True),\r\n        IntInput(name=\"start_index\", display_name=\"Start Index\", info=\"The index to start processing from.\", value=0),\r\n        IntInput(name=\"iteration_count\", display_name=\"Iteration Count\", info=\"Number of items to process. Set to 0 to process all remaining items.\", value=0),\r\n        MultilineInput(\r\n            name=\"prompt_template\",\r\n            display_name=\"Prompt Template\",\r\n            info=\"The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.\",\r\n            value=\"Based on the domain {Domain} and the meta {Meta}, do X\",\r\n        ),\r\n        MessageTextInput(name=\"output_key\", display_name=\"Output Key\", info=\"The key to store the LLM output in the data.\"),\r\n        HandleInput(\r\n            name=\"llm\",\r\n            display_name=\"LLM\",\r\n            input_types=[\"LanguageModel\"],\r\n            info=\"The language model to use for processing.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"file_path\",\r\n            display_name=\"CSV File Path\",\r\n            info=\"The path to save the CSV file. If not provided, no CSV will be saved.\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Processed Data\", name=\"processed_data\", method=\"process_data\"),\r\n    ]\r\n\r\n    def append_to_csv(self, file_path: str, data: dict):\r\n        file_exists = os.path.isfile(file_path)\r\n        \r\n        with open(file_path, 'a', newline='') as csvfile:\r\n            writer = csv.DictWriter(csvfile, fieldnames=data.keys())\r\n            \r\n            if not file_exists:\r\n                writer.writeheader()\r\n            \r\n            writer.writerow(data)\r\n\r\n    def construct_prompt(self, template: str, data: dict) -> str:\r\n        return template.format(**data)\r\n\r\n    def process_data(self) -> List[Data]:\r\n        logger.info(\"Starting data processing\")\r\n        data_list: List[Data] = self.data_list\r\n        start_index: int = max(0, self.start_index)  # Ensure start_index is not negative\r\n        iteration_count: int = self.iteration_count\r\n        prompt_template: str = self.prompt_template\r\n        output_key: str = self.output_key\r\n        file_path: str = self.file_path\r\n\r\n        logger.debug(f\"Number of data items: {len(data_list)}\")\r\n        logger.debug(f\"Start index: {start_index}\")\r\n        logger.debug(f\"Iteration count: {iteration_count}\")\r\n        logger.debug(f\"Prompt template: {prompt_template}\")\r\n        logger.debug(f\"Output key: {output_key}\")\r\n        logger.debug(f\"CSV file path: {file_path}\")\r\n\r\n        llm = self.llm\r\n\r\n        # Adjust iteration_count based on start_index and remaining items\r\n        if iteration_count <= 0 or (start_index + iteration_count) > len(data_list):\r\n            iteration_count = len(data_list) - start_index\r\n        \r\n        end_index = start_index + iteration_count\r\n        logger.info(f\"Will process items from index {start_index} to {end_index - 1}\")\r\n\r\n        processed_data = []\r\n\r\n        for index, item in enumerate(data_list[start_index:end_index], start=start_index):\r\n            logger.debug(f\"Processing item {index}/{end_index - 1}\")\r\n            if not isinstance(item.data, dict):\r\n                logger.error(f\"Data item {index} is not a dictionary\")\r\n                raise ValueError(f\"Data item {index} is not a dictionary\")\r\n\r\n            prompt = self.construct_prompt(prompt_template, item.data)\r\n            logger.debug(f\"Generated prompt: {prompt}\")\r\n            \r\n            try:\r\n                logger.debug(\"Invoking LLM\")\r\n                llm_output = llm.invoke(prompt).content\r\n                logger.debug(f\"LLM output: {llm_output}\")\r\n                new_item = item.data.copy()\r\n                new_item[output_key] = llm_output\r\n                processed_data.append(Data(data=new_item))\r\n                logger.info(f\"Successfully processed item {index}\")\r\n                \r\n                if file_path:\r\n                    self.append_to_csv(file_path, new_item)\r\n                    logger.info(f\"Appended item {index} to CSV\")\r\n            except Exception as e:\r\n                logger.error(f\"Error processing item {index}: {str(e)}\")\r\n                self.status = f\"Error processing item {index}: {str(e)}\"\r\n                return processed_data\r\n\r\n        self.status = processed_data\r\n        logger.info(f\"Finished processing. {len(processed_data)} items processed successfully.\")\r\n        return processed_data",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "file_path": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "test",
                  "name": "file_path",
                  "display_name": "CSV File Path",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The path to save the CSV file. If not provided, no CSV will be saved.",
                  "title_case": false,
                  "type": "str"
                },
                "iteration_count": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "0",
                  "name": "iteration_count",
                  "display_name": "Iteration Count",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Number of items to process. Set to 0 to process all remaining items.",
                  "title_case": false,
                  "type": "int"
                },
                "output_key": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Output",
                  "name": "output_key",
                  "display_name": "Output Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The key to store the LLM output in the data.",
                  "title_case": false,
                  "type": "str"
                },
                "prompt_template": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Based on the domain {Domain}, your response should contain ONLY the name of the country of origin of that domain.",
                  "name": "prompt_template",
                  "display_name": "Prompt Template",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.",
                  "title_case": false,
                  "type": "str"
                },
                "start_index": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "0",
                  "name": "start_index",
                  "display_name": "Start Index",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The index to start processing from.",
                  "title_case": false,
                  "type": "int"
                }
              },
              "description": "Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, store the result, and optionally save the results to a CSV file.\n\n- Define a prompt template using placeholders for data keys (e.g., `{key_1}`, `{key_2}`).\n- Specify the key under which the LLM output will be stored in each data item.\n- Optional: Provide a path to save the processed data as a CSV file.\n",
              "icon": "üîÅ",
              "base_classes": [
                "Data"
              ],
              "display_name": "LLM Data Iterator",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "processed_data",
                  "display_name": "Processed Data",
                  "method": "process_data",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "data_list",
                "start_index",
                "iteration_count",
                "prompt_template",
                "output_key",
                "llm",
                "file_path"
              ],
              "beta": false,
              "edited": true,
              "official": false
            },
            "id": "ToolCallingAgent-nbr4t"
          },
          "id": "ToolCallingAgent-nbr4t",
          "position": {
            "x": 0,
            "y": 0
          },
          "type": "genericNode"
        }
      ],
      "viewport": {
        "x": 1,
        "y": 1,
        "zoom": 1
      }
    },
    "date_created": "2024-08-30T21:35:08.768Z",
    "date_updated": "2024-08-30T21:35:08.807Z",
    "status": "Public",
    "sort": null,
    "user_updated": "6b574962-c600-4487-bbba-fc55cde9b0fc",
    "user_created": {
      "username": "wrench1997",
      "first_name": "ÂÆ∂Êûó",
      "last_name": "Âàò",
      "id": "6b574962-c600-4487-bbba-fc55cde9b0fc"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:11.342Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 10,
    "converter_version": "1.0.0"
  }
}