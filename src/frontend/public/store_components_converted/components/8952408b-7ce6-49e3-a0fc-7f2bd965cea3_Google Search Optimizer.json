{
  "id": "8952408b-7ce6-49e3-a0fc-7f2bd965cea3",
  "name": "Google Search Optimizer",
  "description": "",
  "type": "COMPONENT",
  "is_component": true,
  "author": {
    "username": "appoks",
    "first_name": "Johnathan Rad端nz",
    "last_name": "Rad端nz",
    "id": "91fd1e9c-c965-4845-a8ce-b81d962e690b",
    "full_name": "Johnathan Rad端nz Rad端nz"
  },
  "store_url": "https://www.langflow.store/store/component/8952408b-7ce6-49e3-a0fc-7f2bd965cea3",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-08-04T15:47:04.421Z",
    "updated": "2024-08-04T15:47:04.603Z",
    "downloaded": "2025-08-19T17:50:05.013Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.14",
    "private": true,
    "status": "Public"
  },
  "data": {
    "edges": [],
    "nodes": [
      {
        "data": {
          "id": "groupComponent-cEuEn",
          "type": "GroupNode",
          "node": {
            "display_name": "Google Search Optimizer",
            "documentation": "",
            "description": "",
            "template": {
              "aiml_api_key_AIMLModelComponent-dYv1J": {
                "load_from_db": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "aiml_api_key",
                "display_name": "AI/ML API Key",
                "advanced": false,
                "input_types": [],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "aiml_api_key"
                }
              },
              "code_AIMLModelComponent-dYv1J": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import json\nimport httpx\nfrom axiestudio.base.models.aiml_constants import AIML_CHAT_MODELS\nfrom axiestudio.custom.custom_component.component import Component\n\nfrom axiestudio.inputs.inputs import FloatInput, IntInput, MessageInput, SecretStrInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.field.base import Output\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.inputs import (\n    DropdownInput,\n    StrInput,\n)\n\n\nclass AIMLModelComponent(Component):\n    display_name = \"AI/ML API\"\n    description = \"Generates text using the AI/ML API\"\n    icon = \"AI/ML\"\n    chat_completion_url = \"https://api.aimlapi.com/v1/chat/completions\"\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text_output\", method=\"make_request\"),\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=AIML_CHAT_MODELS,\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"aiml_api_key\",\n            display_name=\"AI/ML API Key\",\n            value=\"AIML_API_KEY\",\n        ),\n        MessageInput(name=\"input_value\", display_name=\"Input\", required=True),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def make_request(self) -> Message:\n        api_key = SecretStr(self.aiml_api_key) if self.aiml_api_key else None\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key.get_secret_value()}\" if api_key else \"\",\n        }\n\n        messages = []\n        if self.system_message:\n            messages.append({\"role\": \"system\", \"content\": self.system_message})\n\n        if self.input_value:\n            if isinstance(self.input_value, Message):\n                # Though we aren't using langchain here, the helper method is useful\n                message = self.input_value.to_lc_message()\n                if message.type == \"human\":\n                    messages.append({\"role\": \"user\", \"content\": message.content})\n                else:\n                    raise ValueError(f\"Expected user message, saw: {message.type}\")\n            else:\n                raise TypeError(f\"Expected Message type, saw: {type(self.input_value)}\")\n        else:\n            raise ValueError(\"Please provide an input value\")\n\n        payload = {\n            \"model\": self.model_name,\n            \"messages\": messages,\n            \"max_tokens\": self.max_tokens or None,\n            \"temperature\": self.temperature or 0.2,\n            \"top_k\": self.top_k or 40,\n            \"top_p\": self.top_p or 0.9,\n            \"repeat_penalty\": self.repeat_penalty or 1.1,\n            \"stop_tokens\": self.stop_tokens or None,\n        }\n\n        try:\n            response = httpx.post(self.chat_completion_url, headers=headers, json=payload)\n            try:\n                response.raise_for_status()\n                result_data = response.json()\n                choice = result_data[\"choices\"][0]\n                result = choice[\"message\"][\"content\"]\n            except httpx.HTTPStatusError as http_err:\n                logger.error(f\"HTTP error occurred: {http_err}\")\n                raise http_err\n            except httpx.RequestError as req_err:\n                logger.error(f\"Request error occurred: {req_err}\")\n                raise req_err\n            except json.JSONDecodeError:\n                logger.warning(\"Failed to decode JSON, response text: {response.text}\")\n                result = response.text\n            except KeyError as key_err:\n                logger.warning(f\"Key error: {key_err}, response content: {result_data}\")\n                raise key_err\n\n            self.status = result\n        except httpx.TimeoutException:\n            return Message(text=\"Request timed out.\")\n        except Exception as exc:\n            logger.error(f\"Error: {exc}\")\n            raise\n\n        return Message(text=result)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "code"
                }
              },
              "max_tokens_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "max_tokens",
                "display_name": "Max Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "max_tokens"
                }
              },
              "model_name_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "options": [
                  "zero-one-ai/Yi-34B-Chat",
                  "allenai/OLMo-7B-Instruct",
                  "allenai/OLMo-7B-Twin-2T",
                  "allenai/OLMo-7B",
                  "Austism/chronos-hermes-13b",
                  "cognitivecomputations/dolphin-2.5-mixtral-8x7b",
                  "deepseek-ai/deepseek-coder-33b-instruct",
                  "deepseek-ai/deepseek-llm-67b-chat",
                  "garage-bAInd/Platypus2-70B-instruct",
                  "google/gemma-2b-it",
                  "google/gemma-7b-it",
                  "Gryphe/MythoMax-L2-13b",
                  "lmsys/vicuna-13b-v1.5",
                  "lmsys/vicuna-7b-v1.5",
                  "codellama/CodeLlama-13b-Instruct-hf",
                  "codellama/CodeLlama-34b-Instruct-hf",
                  "codellama/CodeLlama-70b-Instruct-hf",
                  "codellama/CodeLlama-7b-Instruct-hf",
                  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
                  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
                  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
                  "meta-llama/Llama-2-70b-chat-hf",
                  "meta-llama/Llama-2-13b-chat-hf",
                  "meta-llama/Llama-2-7b-chat-hf",
                  "mistralai/Mistral-7B-Instruct-v0.1",
                  "mistralai/Mistral-7B-Instruct-v0.2",
                  "mistralai/Mixtral-8x7B-Instruct-v0.1",
                  "NousResearch/Nous-Capybara-7B-V1p9",
                  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
                  "NousResearch/Nous-Hermes-llama-2-7b",
                  "NousResearch/Nous-Hermes-Llama2-13b",
                  "NousResearch/Nous-Hermes-2-Yi-34B",
                  "openchat/openchat-3.5-1210",
                  "Open-Orca/Mistral-7B-OpenOrca",
                  "togethercomputer/Qwen-7B-Chat",
                  "Qwen/Qwen1.5-0.5B-Chat",
                  "Qwen/Qwen1.5-1.8B-Chat",
                  "Qwen/Qwen1.5-4B-Chat",
                  "Qwen/Qwen1.5-7B-Chat",
                  "Qwen/Qwen1.5-14B-Chat",
                  "Qwen/Qwen1.5-72B-Chat",
                  "snorkelai/Snorkel-Mistral-PairRM-DPO",
                  "togethercomputer/alpaca-7b",
                  "teknium/OpenHermes-2-Mistral-7B",
                  "teknium/OpenHermes-2p5-Mistral-7B",
                  "togethercomputer/falcon-40b-instruct",
                  "togethercomputer/falcon-7b-instruct",
                  "togethercomputer/Llama-2-7B-32K-Instruct",
                  "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                  "togethercomputer/RedPajama-INCITE-7B-Chat",
                  "togethercomputer/StripedHyena-Nous-7B",
                  "Undi95/ReMM-SLERP-L2-13B",
                  "Undi95/Toppy-M-7B",
                  "WizardLM/WizardLM-13B-V1.2",
                  "upstage/SOLAR-10.7B-Instruct-v1.0",
                  "gpt-4",
                  "gpt-4-turbo",
                  "gpt-4-0613",
                  "gpt-4-32k",
                  "gpt-4-32k-0613",
                  "gpt-3.5-turbo-0125",
                  "gpt-3.5-turbo",
                  "gpt-3.5-turbo-1106",
                  "gpt-3.5-turbo-instruct",
                  "gpt-3.5-turbo-16k",
                  "gpt-3.5-turbo-0613",
                  "gpt-3.5-turbo-16k-0613",
                  "gpt-4o",
                  "claude-3-opus-20240229",
                  "claude-3-sonnet-20240229",
                  "claude-3-haiku-20240307"
                ],
                "combobox": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "value": "gpt-4o",
                "name": "model_name",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "model_name"
                }
              },
              "repeat_penalty_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "repeat_penalty",
                "display_name": "Repeat Penalty",
                "advanced": true,
                "dynamic": false,
                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "repeat_penalty"
                }
              },
              "stop_tokens_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "stop_tokens",
                "display_name": "Stop Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "stop_tokens"
                }
              },
              "system_message_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "system_message",
                "display_name": "System Message",
                "advanced": true,
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "system_message"
                }
              },
              "temperature_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 0.2,
                "name": "temperature",
                "display_name": "Temperature",
                "advanced": true,
                "dynamic": false,
                "info": "Controls the creativity of model responses.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "temperature"
                }
              },
              "top_k_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "top_k",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Limits token selection to top K. (Default: 40)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "top_k"
                }
              },
              "top_p_AIMLModelComponent-dYv1J": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "top_p",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "Works together with top-k. (Default: 0.9)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput",
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "field": "top_p"
                }
              },
              "code_Prompt-cZqxZ": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "Prompt-cZqxZ",
                  "field": "code"
                }
              },
              "template_Prompt-cZqxZ": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "You are tasked with reformatting a user's question to optimize it for a RAG (Retrieval-Augmented Generation) system that handles books. The goal is to convert the original question into a format that will be more effective for similarity searches within the book database.\n\nWhen reformatting the question, follow these guidelines:\n1. Remove any unnecessary words or phrases that don't contribute to the core meaning.\n2. Focus on key concepts, characters, themes, or plot elements mentioned in the question.\n3. Use clear, concise language that captures the essence of the query.\n4. Avoid personal pronouns or conversational language.\n5. If the original question is vague, try to make it more specific based on the context provided.\n6. Ensure that the reformatted question maintains the original intent of the user's query.\n7. Use the same language/idiom as the user.\n\nHere is the user's original question:\n<user_question>\n{user_question}\n</user_question>\n\nPlease reformat this question according to the guidelines above. Output your reformatted question in plain text.\n\nHere are some examples of good reformatting:\n\nOriginal: \"Can you tell me about the main character in To Kill a Mockingbird? I forgot her name.\"\nReformatted: Main character name and description in To Kill a Mockingbird\n\nOriginal: \"I'm looking for books that are similar to 1984 by George Orwell. Any suggestions?\"\nReformatted: Books similar to 1984 by George Orwell dystopian themes\n\nOriginal: \"What's that famous Shakespeare play where the guy talks to a skull?\"\nReformatted: Shakespeare play character speaks to skull scene\n\nOriginal: \"What's the author says about an orange and an apple similarity?\"\nReformatted: Orange and apple similarity\n\nNow, please provide your reformatted version of the user's question.",
                "name": "template",
                "display_name": "Template",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "proxy": {
                  "id": "Prompt-cZqxZ",
                  "field": "template"
                }
              },
              "user_question_Prompt-cZqxZ": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "user_question",
                "display_name": "user_question",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str",
                "proxy": {
                  "id": "Prompt-cZqxZ",
                  "field": "user_question"
                }
              }
            },
            "flow": {
              "data": {
                "nodes": [
                  {
                    "id": "AIMLModelComponent-dYv1J",
                    "type": "genericNode",
                    "position": {
                      "x": -173.7796863862614,
                      "y": -546.2285077366605
                    },
                    "data": {
                      "type": "AIMLModelComponent",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "aiml_api_key": {
                            "load_from_db": true,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "LF_AIML_API_KEY",
                            "name": "aiml_api_key",
                            "display_name": "AI/ML API Key",
                            "advanced": false,
                            "input_types": [],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "password": true,
                            "type": "str",
                            "_input_type": "SecretStrInput"
                          },
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "import json\nimport httpx\nfrom axiestudio.base.models.aiml_constants import AIML_CHAT_MODELS\nfrom axiestudio.custom.custom_component.component import Component\n\nfrom axiestudio.inputs.inputs import FloatInput, IntInput, MessageInput, SecretStrInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.field.base import Output\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.inputs import (\n    DropdownInput,\n    StrInput,\n)\n\n\nclass AIMLModelComponent(Component):\n    display_name = \"AI/ML API\"\n    description = \"Generates text using the AI/ML API\"\n    icon = \"AI/ML\"\n    chat_completion_url = \"https://api.aimlapi.com/v1/chat/completions\"\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text_output\", method=\"make_request\"),\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=AIML_CHAT_MODELS,\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"aiml_api_key\",\n            display_name=\"AI/ML API Key\",\n            value=\"AIML_API_KEY\",\n        ),\n        MessageInput(name=\"input_value\", display_name=\"Input\", required=True),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def make_request(self) -> Message:\n        api_key = SecretStr(self.aiml_api_key) if self.aiml_api_key else None\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key.get_secret_value()}\" if api_key else \"\",\n        }\n\n        messages = []\n        if self.system_message:\n            messages.append({\"role\": \"system\", \"content\": self.system_message})\n\n        if self.input_value:\n            if isinstance(self.input_value, Message):\n                # Though we aren't using langchain here, the helper method is useful\n                message = self.input_value.to_lc_message()\n                if message.type == \"human\":\n                    messages.append({\"role\": \"user\", \"content\": message.content})\n                else:\n                    raise ValueError(f\"Expected user message, saw: {message.type}\")\n            else:\n                raise TypeError(f\"Expected Message type, saw: {type(self.input_value)}\")\n        else:\n            raise ValueError(\"Please provide an input value\")\n\n        payload = {\n            \"model\": self.model_name,\n            \"messages\": messages,\n            \"max_tokens\": self.max_tokens or None,\n            \"temperature\": self.temperature or 0.2,\n            \"top_k\": self.top_k or 40,\n            \"top_p\": self.top_p or 0.9,\n            \"repeat_penalty\": self.repeat_penalty or 1.1,\n            \"stop_tokens\": self.stop_tokens or None,\n        }\n\n        try:\n            response = httpx.post(self.chat_completion_url, headers=headers, json=payload)\n            try:\n                response.raise_for_status()\n                result_data = response.json()\n                choice = result_data[\"choices\"][0]\n                result = choice[\"message\"][\"content\"]\n            except httpx.HTTPStatusError as http_err:\n                logger.error(f\"HTTP error occurred: {http_err}\")\n                raise http_err\n            except httpx.RequestError as req_err:\n                logger.error(f\"Request error occurred: {req_err}\")\n                raise req_err\n            except json.JSONDecodeError:\n                logger.warning(\"Failed to decode JSON, response text: {response.text}\")\n                result = response.text\n            except KeyError as key_err:\n                logger.warning(f\"Key error: {key_err}, response content: {result_data}\")\n                raise key_err\n\n            self.status = result\n        except httpx.TimeoutException:\n            return Message(text=\"Request timed out.\")\n        except Exception as exc:\n            logger.error(f\"Error: {exc}\")\n            raise\n\n        return Message(text=result)\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "input_value": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": true,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "input_value",
                            "display_name": "Input",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "MessageInput"
                          },
                          "max_tokens": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "max_tokens",
                            "display_name": "Max Tokens",
                            "advanced": true,
                            "dynamic": false,
                            "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                            "title_case": false,
                            "type": "int",
                            "_input_type": "IntInput"
                          },
                          "model_name": {
                            "trace_as_metadata": true,
                            "options": [
                              "zero-one-ai/Yi-34B-Chat",
                              "allenai/OLMo-7B-Instruct",
                              "allenai/OLMo-7B-Twin-2T",
                              "allenai/OLMo-7B",
                              "Austism/chronos-hermes-13b",
                              "cognitivecomputations/dolphin-2.5-mixtral-8x7b",
                              "deepseek-ai/deepseek-coder-33b-instruct",
                              "deepseek-ai/deepseek-llm-67b-chat",
                              "garage-bAInd/Platypus2-70B-instruct",
                              "google/gemma-2b-it",
                              "google/gemma-7b-it",
                              "Gryphe/MythoMax-L2-13b",
                              "lmsys/vicuna-13b-v1.5",
                              "lmsys/vicuna-7b-v1.5",
                              "codellama/CodeLlama-13b-Instruct-hf",
                              "codellama/CodeLlama-34b-Instruct-hf",
                              "codellama/CodeLlama-70b-Instruct-hf",
                              "codellama/CodeLlama-7b-Instruct-hf",
                              "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
                              "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
                              "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
                              "meta-llama/Llama-2-70b-chat-hf",
                              "meta-llama/Llama-2-13b-chat-hf",
                              "meta-llama/Llama-2-7b-chat-hf",
                              "mistralai/Mistral-7B-Instruct-v0.1",
                              "mistralai/Mistral-7B-Instruct-v0.2",
                              "mistralai/Mixtral-8x7B-Instruct-v0.1",
                              "NousResearch/Nous-Capybara-7B-V1p9",
                              "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                              "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
                              "NousResearch/Nous-Hermes-llama-2-7b",
                              "NousResearch/Nous-Hermes-Llama2-13b",
                              "NousResearch/Nous-Hermes-2-Yi-34B",
                              "openchat/openchat-3.5-1210",
                              "Open-Orca/Mistral-7B-OpenOrca",
                              "togethercomputer/Qwen-7B-Chat",
                              "Qwen/Qwen1.5-0.5B-Chat",
                              "Qwen/Qwen1.5-1.8B-Chat",
                              "Qwen/Qwen1.5-4B-Chat",
                              "Qwen/Qwen1.5-7B-Chat",
                              "Qwen/Qwen1.5-14B-Chat",
                              "Qwen/Qwen1.5-72B-Chat",
                              "snorkelai/Snorkel-Mistral-PairRM-DPO",
                              "togethercomputer/alpaca-7b",
                              "teknium/OpenHermes-2-Mistral-7B",
                              "teknium/OpenHermes-2p5-Mistral-7B",
                              "togethercomputer/falcon-40b-instruct",
                              "togethercomputer/falcon-7b-instruct",
                              "togethercomputer/Llama-2-7B-32K-Instruct",
                              "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                              "togethercomputer/RedPajama-INCITE-7B-Chat",
                              "togethercomputer/StripedHyena-Nous-7B",
                              "Undi95/ReMM-SLERP-L2-13B",
                              "Undi95/Toppy-M-7B",
                              "WizardLM/WizardLM-13B-V1.2",
                              "upstage/SOLAR-10.7B-Instruct-v1.0",
                              "gpt-4",
                              "gpt-4-turbo",
                              "gpt-4-0613",
                              "gpt-4-32k",
                              "gpt-4-32k-0613",
                              "gpt-3.5-turbo-0125",
                              "gpt-3.5-turbo",
                              "gpt-3.5-turbo-1106",
                              "gpt-3.5-turbo-instruct",
                              "gpt-3.5-turbo-16k",
                              "gpt-3.5-turbo-0613",
                              "gpt-3.5-turbo-16k-0613",
                              "gpt-4o",
                              "claude-3-opus-20240229",
                              "claude-3-sonnet-20240229",
                              "claude-3-haiku-20240307"
                            ],
                            "combobox": false,
                            "required": true,
                            "placeholder": "",
                            "show": true,
                            "value": "gpt-4o",
                            "name": "model_name",
                            "display_name": "Model Name",
                            "advanced": false,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "DropdownInput"
                          },
                          "repeat_penalty": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "repeat_penalty",
                            "display_name": "Repeat Penalty",
                            "advanced": true,
                            "dynamic": false,
                            "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                            "title_case": false,
                            "type": "float",
                            "_input_type": "FloatInput"
                          },
                          "stop_tokens": {
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "stop_tokens",
                            "display_name": "Stop Tokens",
                            "advanced": true,
                            "dynamic": false,
                            "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "StrInput"
                          },
                          "system_message": {
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "system_message",
                            "display_name": "System Message",
                            "advanced": true,
                            "dynamic": false,
                            "info": "System message to pass to the model.",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "StrInput"
                          },
                          "temperature": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": 0.2,
                            "name": "temperature",
                            "display_name": "Temperature",
                            "advanced": false,
                            "dynamic": false,
                            "info": "Controls the creativity of model responses.",
                            "title_case": false,
                            "type": "float",
                            "_input_type": "FloatInput"
                          },
                          "top_k": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "top_k",
                            "display_name": "Top K",
                            "advanced": true,
                            "dynamic": false,
                            "info": "Limits token selection to top K. (Default: 40)",
                            "title_case": false,
                            "type": "int",
                            "_input_type": "IntInput"
                          },
                          "top_p": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "",
                            "name": "top_p",
                            "display_name": "Top P",
                            "advanced": true,
                            "dynamic": false,
                            "info": "Works together with top-k. (Default: 0.9)",
                            "title_case": false,
                            "type": "float",
                            "_input_type": "FloatInput"
                          }
                        },
                        "description": "Generates text using the AI/ML API",
                        "icon": "AI/ML",
                        "base_classes": [
                          "Message"
                        ],
                        "display_name": "AI/ML API",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Message"
                            ],
                            "selected": "Message",
                            "name": "text_output",
                            "display_name": "Text",
                            "method": "make_request",
                            "value": "__UNDEFINED__",
                            "cache": true
                          }
                        ],
                        "field_order": [
                          "model_name",
                          "aiml_api_key",
                          "input_value",
                          "max_tokens",
                          "stop_tokens",
                          "top_k",
                          "top_p",
                          "repeat_penalty",
                          "temperature",
                          "system_message"
                        ],
                        "beta": false,
                        "edited": false
                      },
                      "id": "AIMLModelComponent-dYv1J"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 583,
                    "positionAbsolute": {
                      "x": -173.7796863862614,
                      "y": -546.2285077366605
                    },
                    "dragging": false
                  },
                  {
                    "id": "Prompt-cZqxZ",
                    "type": "genericNode",
                    "position": {
                      "x": -705.0094599682845,
                      "y": -674.438457674672
                    },
                    "data": {
                      "type": "Prompt",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "template": {
                            "trace_as_input": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "value": "You are tasked with reformatting a user's question to optimize it for a RAG (Retrieval-Augmented Generation) system that handles books. The goal is to convert the original question into a format that will be more effective for similarity searches within the book database.\n\nWhen reformatting the question, follow these guidelines:\n1. Remove any unnecessary words or phrases that don't contribute to the core meaning.\n2. Focus on key concepts, characters, themes, or plot elements mentioned in the question.\n3. Use clear, concise language that captures the essence of the query.\n4. Avoid personal pronouns or conversational language.\n5. If the original question is vague, try to make it more specific based on the context provided.\n6. Ensure that the reformatted question maintains the original intent of the user's query.\n7. Use the same language/idiom as the user.\n\nHere is the user's original question:\n<user_question>\n{user_question}\n</user_question>\n\nPlease reformat this question according to the guidelines above. Output your reformatted question in plain text.\n\nHere are some examples of good reformatting:\n\nOriginal: \"Can you tell me about the main character in To Kill a Mockingbird? I forgot her name.\"\nReformatted: Main character name and description in To Kill a Mockingbird\n\nOriginal: \"I'm looking for books that are similar to 1984 by George Orwell. Any suggestions?\"\nReformatted: Books similar to 1984 by George Orwell dystopian themes\n\nOriginal: \"What's that famous Shakespeare play where the guy talks to a skull?\"\nReformatted: Shakespeare play character speaks to skull scene\n\nOriginal: \"What's the author says about an orange and an apple similarity?\"\nReformatted: Orange and apple similarity\n\nNow, please provide your reformatted version of the user's question.",
                            "name": "template",
                            "display_name": "Template",
                            "advanced": false,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "prompt",
                            "_input_type": "PromptInput"
                          },
                          "user_question": {
                            "field_type": "str",
                            "required": false,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "user_question",
                            "display_name": "user_question",
                            "advanced": false,
                            "input_types": [
                              "Message",
                              "Text"
                            ],
                            "dynamic": false,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "type": "str"
                          }
                        },
                        "description": "Create a prompt template with dynamic variables.",
                        "icon": "prompts",
                        "is_input": null,
                        "is_output": null,
                        "is_composition": null,
                        "base_classes": [
                          "Message"
                        ],
                        "name": "",
                        "display_name": "Prompt",
                        "documentation": "",
                        "custom_fields": {
                          "template": [
                            "user_question"
                          ]
                        },
                        "output_types": [],
                        "full_path": null,
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Message"
                            ],
                            "selected": "Message",
                            "name": "prompt",
                            "hidden": null,
                            "display_name": "Prompt Message",
                            "method": "build_prompt",
                            "value": "__UNDEFINED__",
                            "cache": true
                          }
                        ],
                        "field_order": [
                          "template"
                        ],
                        "beta": false,
                        "error": null,
                        "edited": false
                      },
                      "id": "Prompt-cZqxZ"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 431,
                    "positionAbsolute": {
                      "x": -705.0094599682845,
                      "y": -674.438457674672
                    },
                    "dragging": false
                  }
                ],
                "edges": [
                  {
                    "source": "Prompt-cZqxZ",
                    "sourceHandle": "{dataType:Prompt,id:Prompt-cZqxZ,name:prompt,output_types:[Message]}",
                    "target": "AIMLModelComponent-dYv1J",
                    "targetHandle": "{fieldName:input_value,id:AIMLModelComponent-dYv1J,inputTypes:[Message],type:str}",
                    "data": {
                      "targetHandle": {
                        "fieldName": "input_value",
                        "id": "AIMLModelComponent-dYv1J",
                        "inputTypes": [
                          "Message"
                        ],
                        "type": "str"
                      },
                      "sourceHandle": {
                        "dataType": "Prompt",
                        "id": "Prompt-cZqxZ",
                        "name": "prompt",
                        "output_types": [
                          "Message"
                        ]
                      }
                    },
                    "id": "reactflow__edge-Prompt-cZqxZ{dataType:Prompt,id:Prompt-cZqxZ,name:prompt,output_types:[Message]}-AIMLModelComponent-dYv1J{fieldName:input_value,id:AIMLModelComponent-dYv1J,inputTypes:[Message],type:str}"
                  }
                ],
                "viewport": {
                  "zoom": 1,
                  "x": 0,
                  "y": 0
                }
              },
              "is_component": false,
              "name": "Lively Tesla",
              "description": "",
              "id": "kVqfq"
            },
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "AIMLModelComponent-dYv1J_text_output",
                "display_name": "Text",
                "method": "make_request",
                "value": "__UNDEFINED__",
                "cache": true,
                "proxy": {
                  "id": "AIMLModelComponent-dYv1J",
                  "name": "text_output",
                  "nodeDisplayName": "AI/ML API"
                }
              }
            ],
            "official": false
          }
        },
        "id": "groupComponent-cEuEn",
        "position": {
          "x": 0,
          "y": 0
        },
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 1,
      "y": 1,
      "zoom": 1
    }
  },
  "metadata": {
    "groupComponent": {
      "count": 1
    },
    "total": 1
  },
  "original": {
    "id": "8952408b-7ce6-49e3-a0fc-7f2bd965cea3",
    "name": "Google Search Optimizer",
    "description": "",
    "is_component": true,
    "liked_by_count": "0",
    "downloads_count": "0",
    "metadata": {
      "groupComponent": {
        "count": 1
      },
      "total": 1
    },
    "last_tested_version": "1.0.14",
    "private": true,
    "data": {
      "edges": [],
      "nodes": [
        {
          "data": {
            "id": "groupComponent-cEuEn",
            "type": "GroupNode",
            "node": {
              "display_name": "Google Search Optimizer",
              "documentation": "",
              "description": "",
              "template": {
                "aiml_api_key_AIMLModelComponent-dYv1J": {
                  "load_from_db": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "aiml_api_key",
                  "display_name": "AI/ML API Key",
                  "advanced": false,
                  "input_types": [],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "aiml_api_key"
                  }
                },
                "code_AIMLModelComponent-dYv1J": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import json\nimport httpx\nfrom axiestudio.base.models.aiml_constants import AIML_CHAT_MODELS\nfrom axiestudio.custom.custom_component.component import Component\n\nfrom axiestudio.inputs.inputs import FloatInput, IntInput, MessageInput, SecretStrInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.field.base import Output\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.inputs import (\n    DropdownInput,\n    StrInput,\n)\n\n\nclass AIMLModelComponent(Component):\n    display_name = \"AI/ML API\"\n    description = \"Generates text using the AI/ML API\"\n    icon = \"AI/ML\"\n    chat_completion_url = \"https://api.aimlapi.com/v1/chat/completions\"\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text_output\", method=\"make_request\"),\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=AIML_CHAT_MODELS,\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"aiml_api_key\",\n            display_name=\"AI/ML API Key\",\n            value=\"AIML_API_KEY\",\n        ),\n        MessageInput(name=\"input_value\", display_name=\"Input\", required=True),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def make_request(self) -> Message:\n        api_key = SecretStr(self.aiml_api_key) if self.aiml_api_key else None\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key.get_secret_value()}\" if api_key else \"\",\n        }\n\n        messages = []\n        if self.system_message:\n            messages.append({\"role\": \"system\", \"content\": self.system_message})\n\n        if self.input_value:\n            if isinstance(self.input_value, Message):\n                # Though we aren't using langchain here, the helper method is useful\n                message = self.input_value.to_lc_message()\n                if message.type == \"human\":\n                    messages.append({\"role\": \"user\", \"content\": message.content})\n                else:\n                    raise ValueError(f\"Expected user message, saw: {message.type}\")\n            else:\n                raise TypeError(f\"Expected Message type, saw: {type(self.input_value)}\")\n        else:\n            raise ValueError(\"Please provide an input value\")\n\n        payload = {\n            \"model\": self.model_name,\n            \"messages\": messages,\n            \"max_tokens\": self.max_tokens or None,\n            \"temperature\": self.temperature or 0.2,\n            \"top_k\": self.top_k or 40,\n            \"top_p\": self.top_p or 0.9,\n            \"repeat_penalty\": self.repeat_penalty or 1.1,\n            \"stop_tokens\": self.stop_tokens or None,\n        }\n\n        try:\n            response = httpx.post(self.chat_completion_url, headers=headers, json=payload)\n            try:\n                response.raise_for_status()\n                result_data = response.json()\n                choice = result_data[\"choices\"][0]\n                result = choice[\"message\"][\"content\"]\n            except httpx.HTTPStatusError as http_err:\n                logger.error(f\"HTTP error occurred: {http_err}\")\n                raise http_err\n            except httpx.RequestError as req_err:\n                logger.error(f\"Request error occurred: {req_err}\")\n                raise req_err\n            except json.JSONDecodeError:\n                logger.warning(\"Failed to decode JSON, response text: {response.text}\")\n                result = response.text\n            except KeyError as key_err:\n                logger.warning(f\"Key error: {key_err}, response content: {result_data}\")\n                raise key_err\n\n            self.status = result\n        except httpx.TimeoutException:\n            return Message(text=\"Request timed out.\")\n        except Exception as exc:\n            logger.error(f\"Error: {exc}\")\n            raise\n\n        return Message(text=result)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "code"
                  }
                },
                "max_tokens_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "max_tokens",
                  "display_name": "Max Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "max_tokens"
                  }
                },
                "model_name_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "options": [
                    "zero-one-ai/Yi-34B-Chat",
                    "allenai/OLMo-7B-Instruct",
                    "allenai/OLMo-7B-Twin-2T",
                    "allenai/OLMo-7B",
                    "Austism/chronos-hermes-13b",
                    "cognitivecomputations/dolphin-2.5-mixtral-8x7b",
                    "deepseek-ai/deepseek-coder-33b-instruct",
                    "deepseek-ai/deepseek-llm-67b-chat",
                    "garage-bAInd/Platypus2-70B-instruct",
                    "google/gemma-2b-it",
                    "google/gemma-7b-it",
                    "Gryphe/MythoMax-L2-13b",
                    "lmsys/vicuna-13b-v1.5",
                    "lmsys/vicuna-7b-v1.5",
                    "codellama/CodeLlama-13b-Instruct-hf",
                    "codellama/CodeLlama-34b-Instruct-hf",
                    "codellama/CodeLlama-70b-Instruct-hf",
                    "codellama/CodeLlama-7b-Instruct-hf",
                    "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
                    "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
                    "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
                    "meta-llama/Llama-2-70b-chat-hf",
                    "meta-llama/Llama-2-13b-chat-hf",
                    "meta-llama/Llama-2-7b-chat-hf",
                    "mistralai/Mistral-7B-Instruct-v0.1",
                    "mistralai/Mistral-7B-Instruct-v0.2",
                    "mistralai/Mixtral-8x7B-Instruct-v0.1",
                    "NousResearch/Nous-Capybara-7B-V1p9",
                    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                    "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
                    "NousResearch/Nous-Hermes-llama-2-7b",
                    "NousResearch/Nous-Hermes-Llama2-13b",
                    "NousResearch/Nous-Hermes-2-Yi-34B",
                    "openchat/openchat-3.5-1210",
                    "Open-Orca/Mistral-7B-OpenOrca",
                    "togethercomputer/Qwen-7B-Chat",
                    "Qwen/Qwen1.5-0.5B-Chat",
                    "Qwen/Qwen1.5-1.8B-Chat",
                    "Qwen/Qwen1.5-4B-Chat",
                    "Qwen/Qwen1.5-7B-Chat",
                    "Qwen/Qwen1.5-14B-Chat",
                    "Qwen/Qwen1.5-72B-Chat",
                    "snorkelai/Snorkel-Mistral-PairRM-DPO",
                    "togethercomputer/alpaca-7b",
                    "teknium/OpenHermes-2-Mistral-7B",
                    "teknium/OpenHermes-2p5-Mistral-7B",
                    "togethercomputer/falcon-40b-instruct",
                    "togethercomputer/falcon-7b-instruct",
                    "togethercomputer/Llama-2-7B-32K-Instruct",
                    "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                    "togethercomputer/RedPajama-INCITE-7B-Chat",
                    "togethercomputer/StripedHyena-Nous-7B",
                    "Undi95/ReMM-SLERP-L2-13B",
                    "Undi95/Toppy-M-7B",
                    "WizardLM/WizardLM-13B-V1.2",
                    "upstage/SOLAR-10.7B-Instruct-v1.0",
                    "gpt-4",
                    "gpt-4-turbo",
                    "gpt-4-0613",
                    "gpt-4-32k",
                    "gpt-4-32k-0613",
                    "gpt-3.5-turbo-0125",
                    "gpt-3.5-turbo",
                    "gpt-3.5-turbo-1106",
                    "gpt-3.5-turbo-instruct",
                    "gpt-3.5-turbo-16k",
                    "gpt-3.5-turbo-0613",
                    "gpt-3.5-turbo-16k-0613",
                    "gpt-4o",
                    "claude-3-opus-20240229",
                    "claude-3-sonnet-20240229",
                    "claude-3-haiku-20240307"
                  ],
                  "combobox": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "value": "gpt-4o",
                  "name": "model_name",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "model_name"
                  }
                },
                "repeat_penalty_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "repeat_penalty",
                  "display_name": "Repeat Penalty",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "repeat_penalty"
                  }
                },
                "stop_tokens_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "stop_tokens",
                  "display_name": "Stop Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "stop_tokens"
                  }
                },
                "system_message_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "system_message",
                  "display_name": "System Message",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "system_message"
                  }
                },
                "temperature_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 0.2,
                  "name": "temperature",
                  "display_name": "Temperature",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Controls the creativity of model responses.",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "temperature"
                  }
                },
                "top_k_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "top_k",
                  "display_name": "Top K",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Limits token selection to top K. (Default: 40)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "top_k"
                  }
                },
                "top_p_AIMLModelComponent-dYv1J": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "top_p",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Works together with top-k. (Default: 0.9)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput",
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "field": "top_p"
                  }
                },
                "code_Prompt-cZqxZ": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "Prompt-cZqxZ",
                    "field": "code"
                  }
                },
                "template_Prompt-cZqxZ": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "You are tasked with reformatting a user's question to optimize it for a RAG (Retrieval-Augmented Generation) system that handles books. The goal is to convert the original question into a format that will be more effective for similarity searches within the book database.\n\nWhen reformatting the question, follow these guidelines:\n1. Remove any unnecessary words or phrases that don't contribute to the core meaning.\n2. Focus on key concepts, characters, themes, or plot elements mentioned in the question.\n3. Use clear, concise language that captures the essence of the query.\n4. Avoid personal pronouns or conversational language.\n5. If the original question is vague, try to make it more specific based on the context provided.\n6. Ensure that the reformatted question maintains the original intent of the user's query.\n7. Use the same language/idiom as the user.\n\nHere is the user's original question:\n<user_question>\n{user_question}\n</user_question>\n\nPlease reformat this question according to the guidelines above. Output your reformatted question in plain text.\n\nHere are some examples of good reformatting:\n\nOriginal: \"Can you tell me about the main character in To Kill a Mockingbird? I forgot her name.\"\nReformatted: Main character name and description in To Kill a Mockingbird\n\nOriginal: \"I'm looking for books that are similar to 1984 by George Orwell. Any suggestions?\"\nReformatted: Books similar to 1984 by George Orwell dystopian themes\n\nOriginal: \"What's that famous Shakespeare play where the guy talks to a skull?\"\nReformatted: Shakespeare play character speaks to skull scene\n\nOriginal: \"What's the author says about an orange and an apple similarity?\"\nReformatted: Orange and apple similarity\n\nNow, please provide your reformatted version of the user's question.",
                  "name": "template",
                  "display_name": "Template",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "prompt",
                  "_input_type": "PromptInput",
                  "proxy": {
                    "id": "Prompt-cZqxZ",
                    "field": "template"
                  }
                },
                "user_question_Prompt-cZqxZ": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "user_question",
                  "display_name": "user_question",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str",
                  "proxy": {
                    "id": "Prompt-cZqxZ",
                    "field": "user_question"
                  }
                }
              },
              "flow": {
                "data": {
                  "nodes": [
                    {
                      "id": "AIMLModelComponent-dYv1J",
                      "type": "genericNode",
                      "position": {
                        "x": -173.7796863862614,
                        "y": -546.2285077366605
                      },
                      "data": {
                        "type": "AIMLModelComponent",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "aiml_api_key": {
                              "load_from_db": true,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "LF_AIML_API_KEY",
                              "name": "aiml_api_key",
                              "display_name": "AI/ML API Key",
                              "advanced": false,
                              "input_types": [],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "password": true,
                              "type": "str",
                              "_input_type": "SecretStrInput"
                            },
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "import json\nimport httpx\nfrom axiestudio.base.models.aiml_constants import AIML_CHAT_MODELS\nfrom axiestudio.custom.custom_component.component import Component\n\nfrom axiestudio.inputs.inputs import FloatInput, IntInput, MessageInput, SecretStrInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.field.base import Output\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.inputs import (\n    DropdownInput,\n    StrInput,\n)\n\n\nclass AIMLModelComponent(Component):\n    display_name = \"AI/ML API\"\n    description = \"Generates text using the AI/ML API\"\n    icon = \"AI/ML\"\n    chat_completion_url = \"https://api.aimlapi.com/v1/chat/completions\"\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text_output\", method=\"make_request\"),\n    ]\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=AIML_CHAT_MODELS,\n            required=True,\n        ),\n        SecretStrInput(\n            name=\"aiml_api_key\",\n            display_name=\"AI/ML API Key\",\n            value=\"AIML_API_KEY\",\n        ),\n        MessageInput(name=\"input_value\", display_name=\"Input\", required=True),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def make_request(self) -> Message:\n        api_key = SecretStr(self.aiml_api_key) if self.aiml_api_key else None\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key.get_secret_value()}\" if api_key else \"\",\n        }\n\n        messages = []\n        if self.system_message:\n            messages.append({\"role\": \"system\", \"content\": self.system_message})\n\n        if self.input_value:\n            if isinstance(self.input_value, Message):\n                # Though we aren't using langchain here, the helper method is useful\n                message = self.input_value.to_lc_message()\n                if message.type == \"human\":\n                    messages.append({\"role\": \"user\", \"content\": message.content})\n                else:\n                    raise ValueError(f\"Expected user message, saw: {message.type}\")\n            else:\n                raise TypeError(f\"Expected Message type, saw: {type(self.input_value)}\")\n        else:\n            raise ValueError(\"Please provide an input value\")\n\n        payload = {\n            \"model\": self.model_name,\n            \"messages\": messages,\n            \"max_tokens\": self.max_tokens or None,\n            \"temperature\": self.temperature or 0.2,\n            \"top_k\": self.top_k or 40,\n            \"top_p\": self.top_p or 0.9,\n            \"repeat_penalty\": self.repeat_penalty or 1.1,\n            \"stop_tokens\": self.stop_tokens or None,\n        }\n\n        try:\n            response = httpx.post(self.chat_completion_url, headers=headers, json=payload)\n            try:\n                response.raise_for_status()\n                result_data = response.json()\n                choice = result_data[\"choices\"][0]\n                result = choice[\"message\"][\"content\"]\n            except httpx.HTTPStatusError as http_err:\n                logger.error(f\"HTTP error occurred: {http_err}\")\n                raise http_err\n            except httpx.RequestError as req_err:\n                logger.error(f\"Request error occurred: {req_err}\")\n                raise req_err\n            except json.JSONDecodeError:\n                logger.warning(\"Failed to decode JSON, response text: {response.text}\")\n                result = response.text\n            except KeyError as key_err:\n                logger.warning(f\"Key error: {key_err}, response content: {result_data}\")\n                raise key_err\n\n            self.status = result\n        except httpx.TimeoutException:\n            return Message(text=\"Request timed out.\")\n        except Exception as exc:\n            logger.error(f\"Error: {exc}\")\n            raise\n\n        return Message(text=result)\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "input_value": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": true,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "input_value",
                              "display_name": "Input",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "MessageInput"
                            },
                            "max_tokens": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "max_tokens",
                              "display_name": "Max Tokens",
                              "advanced": true,
                              "dynamic": false,
                              "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                              "title_case": false,
                              "type": "int",
                              "_input_type": "IntInput"
                            },
                            "model_name": {
                              "trace_as_metadata": true,
                              "options": [
                                "zero-one-ai/Yi-34B-Chat",
                                "allenai/OLMo-7B-Instruct",
                                "allenai/OLMo-7B-Twin-2T",
                                "allenai/OLMo-7B",
                                "Austism/chronos-hermes-13b",
                                "cognitivecomputations/dolphin-2.5-mixtral-8x7b",
                                "deepseek-ai/deepseek-coder-33b-instruct",
                                "deepseek-ai/deepseek-llm-67b-chat",
                                "garage-bAInd/Platypus2-70B-instruct",
                                "google/gemma-2b-it",
                                "google/gemma-7b-it",
                                "Gryphe/MythoMax-L2-13b",
                                "lmsys/vicuna-13b-v1.5",
                                "lmsys/vicuna-7b-v1.5",
                                "codellama/CodeLlama-13b-Instruct-hf",
                                "codellama/CodeLlama-34b-Instruct-hf",
                                "codellama/CodeLlama-70b-Instruct-hf",
                                "codellama/CodeLlama-7b-Instruct-hf",
                                "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
                                "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
                                "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
                                "meta-llama/Llama-2-70b-chat-hf",
                                "meta-llama/Llama-2-13b-chat-hf",
                                "meta-llama/Llama-2-7b-chat-hf",
                                "mistralai/Mistral-7B-Instruct-v0.1",
                                "mistralai/Mistral-7B-Instruct-v0.2",
                                "mistralai/Mixtral-8x7B-Instruct-v0.1",
                                "NousResearch/Nous-Capybara-7B-V1p9",
                                "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                                "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
                                "NousResearch/Nous-Hermes-llama-2-7b",
                                "NousResearch/Nous-Hermes-Llama2-13b",
                                "NousResearch/Nous-Hermes-2-Yi-34B",
                                "openchat/openchat-3.5-1210",
                                "Open-Orca/Mistral-7B-OpenOrca",
                                "togethercomputer/Qwen-7B-Chat",
                                "Qwen/Qwen1.5-0.5B-Chat",
                                "Qwen/Qwen1.5-1.8B-Chat",
                                "Qwen/Qwen1.5-4B-Chat",
                                "Qwen/Qwen1.5-7B-Chat",
                                "Qwen/Qwen1.5-14B-Chat",
                                "Qwen/Qwen1.5-72B-Chat",
                                "snorkelai/Snorkel-Mistral-PairRM-DPO",
                                "togethercomputer/alpaca-7b",
                                "teknium/OpenHermes-2-Mistral-7B",
                                "teknium/OpenHermes-2p5-Mistral-7B",
                                "togethercomputer/falcon-40b-instruct",
                                "togethercomputer/falcon-7b-instruct",
                                "togethercomputer/Llama-2-7B-32K-Instruct",
                                "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
                                "togethercomputer/RedPajama-INCITE-7B-Chat",
                                "togethercomputer/StripedHyena-Nous-7B",
                                "Undi95/ReMM-SLERP-L2-13B",
                                "Undi95/Toppy-M-7B",
                                "WizardLM/WizardLM-13B-V1.2",
                                "upstage/SOLAR-10.7B-Instruct-v1.0",
                                "gpt-4",
                                "gpt-4-turbo",
                                "gpt-4-0613",
                                "gpt-4-32k",
                                "gpt-4-32k-0613",
                                "gpt-3.5-turbo-0125",
                                "gpt-3.5-turbo",
                                "gpt-3.5-turbo-1106",
                                "gpt-3.5-turbo-instruct",
                                "gpt-3.5-turbo-16k",
                                "gpt-3.5-turbo-0613",
                                "gpt-3.5-turbo-16k-0613",
                                "gpt-4o",
                                "claude-3-opus-20240229",
                                "claude-3-sonnet-20240229",
                                "claude-3-haiku-20240307"
                              ],
                              "combobox": false,
                              "required": true,
                              "placeholder": "",
                              "show": true,
                              "value": "gpt-4o",
                              "name": "model_name",
                              "display_name": "Model Name",
                              "advanced": false,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "DropdownInput"
                            },
                            "repeat_penalty": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "repeat_penalty",
                              "display_name": "Repeat Penalty",
                              "advanced": true,
                              "dynamic": false,
                              "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                              "title_case": false,
                              "type": "float",
                              "_input_type": "FloatInput"
                            },
                            "stop_tokens": {
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "stop_tokens",
                              "display_name": "Stop Tokens",
                              "advanced": true,
                              "dynamic": false,
                              "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "StrInput"
                            },
                            "system_message": {
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "system_message",
                              "display_name": "System Message",
                              "advanced": true,
                              "dynamic": false,
                              "info": "System message to pass to the model.",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "StrInput"
                            },
                            "temperature": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": 0.2,
                              "name": "temperature",
                              "display_name": "Temperature",
                              "advanced": false,
                              "dynamic": false,
                              "info": "Controls the creativity of model responses.",
                              "title_case": false,
                              "type": "float",
                              "_input_type": "FloatInput"
                            },
                            "top_k": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "top_k",
                              "display_name": "Top K",
                              "advanced": true,
                              "dynamic": false,
                              "info": "Limits token selection to top K. (Default: 40)",
                              "title_case": false,
                              "type": "int",
                              "_input_type": "IntInput"
                            },
                            "top_p": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "",
                              "name": "top_p",
                              "display_name": "Top P",
                              "advanced": true,
                              "dynamic": false,
                              "info": "Works together with top-k. (Default: 0.9)",
                              "title_case": false,
                              "type": "float",
                              "_input_type": "FloatInput"
                            }
                          },
                          "description": "Generates text using the AI/ML API",
                          "icon": "AI/ML",
                          "base_classes": [
                            "Message"
                          ],
                          "display_name": "AI/ML API",
                          "documentation": "",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Message"
                              ],
                              "selected": "Message",
                              "name": "text_output",
                              "display_name": "Text",
                              "method": "make_request",
                              "value": "__UNDEFINED__",
                              "cache": true
                            }
                          ],
                          "field_order": [
                            "model_name",
                            "aiml_api_key",
                            "input_value",
                            "max_tokens",
                            "stop_tokens",
                            "top_k",
                            "top_p",
                            "repeat_penalty",
                            "temperature",
                            "system_message"
                          ],
                          "beta": false,
                          "edited": false
                        },
                        "id": "AIMLModelComponent-dYv1J"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 583,
                      "positionAbsolute": {
                        "x": -173.7796863862614,
                        "y": -546.2285077366605
                      },
                      "dragging": false
                    },
                    {
                      "id": "Prompt-cZqxZ",
                      "type": "genericNode",
                      "position": {
                        "x": -705.0094599682845,
                        "y": -674.438457674672
                      },
                      "data": {
                        "type": "Prompt",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "template": {
                              "trace_as_input": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "value": "You are tasked with reformatting a user's question to optimize it for a RAG (Retrieval-Augmented Generation) system that handles books. The goal is to convert the original question into a format that will be more effective for similarity searches within the book database.\n\nWhen reformatting the question, follow these guidelines:\n1. Remove any unnecessary words or phrases that don't contribute to the core meaning.\n2. Focus on key concepts, characters, themes, or plot elements mentioned in the question.\n3. Use clear, concise language that captures the essence of the query.\n4. Avoid personal pronouns or conversational language.\n5. If the original question is vague, try to make it more specific based on the context provided.\n6. Ensure that the reformatted question maintains the original intent of the user's query.\n7. Use the same language/idiom as the user.\n\nHere is the user's original question:\n<user_question>\n{user_question}\n</user_question>\n\nPlease reformat this question according to the guidelines above. Output your reformatted question in plain text.\n\nHere are some examples of good reformatting:\n\nOriginal: \"Can you tell me about the main character in To Kill a Mockingbird? I forgot her name.\"\nReformatted: Main character name and description in To Kill a Mockingbird\n\nOriginal: \"I'm looking for books that are similar to 1984 by George Orwell. Any suggestions?\"\nReformatted: Books similar to 1984 by George Orwell dystopian themes\n\nOriginal: \"What's that famous Shakespeare play where the guy talks to a skull?\"\nReformatted: Shakespeare play character speaks to skull scene\n\nOriginal: \"What's the author says about an orange and an apple similarity?\"\nReformatted: Orange and apple similarity\n\nNow, please provide your reformatted version of the user's question.",
                              "name": "template",
                              "display_name": "Template",
                              "advanced": false,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "prompt",
                              "_input_type": "PromptInput"
                            },
                            "user_question": {
                              "field_type": "str",
                              "required": false,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "user_question",
                              "display_name": "user_question",
                              "advanced": false,
                              "input_types": [
                                "Message",
                                "Text"
                              ],
                              "dynamic": false,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "type": "str"
                            }
                          },
                          "description": "Create a prompt template with dynamic variables.",
                          "icon": "prompts",
                          "is_input": null,
                          "is_output": null,
                          "is_composition": null,
                          "base_classes": [
                            "Message"
                          ],
                          "name": "",
                          "display_name": "Prompt",
                          "documentation": "",
                          "custom_fields": {
                            "template": [
                              "user_question"
                            ]
                          },
                          "output_types": [],
                          "full_path": null,
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Message"
                              ],
                              "selected": "Message",
                              "name": "prompt",
                              "hidden": null,
                              "display_name": "Prompt Message",
                              "method": "build_prompt",
                              "value": "__UNDEFINED__",
                              "cache": true
                            }
                          ],
                          "field_order": [
                            "template"
                          ],
                          "beta": false,
                          "error": null,
                          "edited": false
                        },
                        "id": "Prompt-cZqxZ"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 431,
                      "positionAbsolute": {
                        "x": -705.0094599682845,
                        "y": -674.438457674672
                      },
                      "dragging": false
                    }
                  ],
                  "edges": [
                    {
                      "source": "Prompt-cZqxZ",
                      "sourceHandle": "{dataType:Prompt,id:Prompt-cZqxZ,name:prompt,output_types:[Message]}",
                      "target": "AIMLModelComponent-dYv1J",
                      "targetHandle": "{fieldName:input_value,id:AIMLModelComponent-dYv1J,inputTypes:[Message],type:str}",
                      "data": {
                        "targetHandle": {
                          "fieldName": "input_value",
                          "id": "AIMLModelComponent-dYv1J",
                          "inputTypes": [
                            "Message"
                          ],
                          "type": "str"
                        },
                        "sourceHandle": {
                          "dataType": "Prompt",
                          "id": "Prompt-cZqxZ",
                          "name": "prompt",
                          "output_types": [
                            "Message"
                          ]
                        }
                      },
                      "id": "reactflow__edge-Prompt-cZqxZ{dataType:Prompt,id:Prompt-cZqxZ,name:prompt,output_types:[Message]}-AIMLModelComponent-dYv1J{fieldName:input_value,id:AIMLModelComponent-dYv1J,inputTypes:[Message],type:str}"
                    }
                  ],
                  "viewport": {
                    "zoom": 1,
                    "x": 0,
                    "y": 0
                  }
                },
                "is_component": false,
                "name": "Lively Tesla",
                "description": "",
                "id": "kVqfq"
              },
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "AIMLModelComponent-dYv1J_text_output",
                  "display_name": "Text",
                  "method": "make_request",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "proxy": {
                    "id": "AIMLModelComponent-dYv1J",
                    "name": "text_output",
                    "nodeDisplayName": "AI/ML API"
                  }
                }
              ],
              "official": false
            }
          },
          "id": "groupComponent-cEuEn",
          "position": {
            "x": 0,
            "y": 0
          },
          "type": "genericNode"
        }
      ],
      "viewport": {
        "x": 1,
        "y": 1,
        "zoom": 1
      }
    },
    "date_created": "2024-08-04T15:47:04.421Z",
    "date_updated": "2024-08-04T15:47:04.603Z",
    "status": "Public",
    "sort": null,
    "user_updated": "91fd1e9c-c965-4845-a8ce-b81d962e690b",
    "user_created": {
      "username": "appoks",
      "first_name": "Johnathan Rad端nz",
      "last_name": "Rad端nz",
      "id": "91fd1e9c-c965-4845-a8ce-b81d962e690b"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:10.919Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 44,
    "converter_version": "1.0.0"
  }
}