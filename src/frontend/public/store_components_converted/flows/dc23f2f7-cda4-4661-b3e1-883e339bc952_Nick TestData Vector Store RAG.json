{
  "id": "dc23f2f7-cda4-4661-b3e1-883e339bc952",
  "name": "Nick TestData Vector Store RAG",
  "description": "Visit https://docs.langflow.org/tutorials/rag-with-astradb for a detailed guide of this project.\nThis project give you both Ingestion and RAG in a single file. You'll need to visit https://astra.datastax.com/ to create an Astra DB instance, your Token and grab an API Endpoint.\nRunning this project requires you to add a file in the Files component, then define a Collection Name and click on the Play icon on the Astra DB component. \n\nAfter the ingestion ends you are ready to click on the Run button at the lower left corner and start asking questions about your data. (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "MetaNets",
    "first_name": "Nick",
    "last_name": "",
    "id": "2edb86fd-5024-4e6f-9013-f6f3aa272d9d",
    "full_name": "Nick"
  },
  "store_url": "https://www.langflow.store/store/component/dc23f2f7-cda4-4661-b3e1-883e339bc952",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-09-03T04:21:25.390Z",
    "updated": "2024-09-03T04:21:25.574Z",
    "downloaded": "2025-08-19T17:50:06.636Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.15",
    "private": true,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "data": {
          "description": "Get chat inputs from the Playground.",
          "display_name": "Chat Input",
          "id": "ChatInput-sgEOq",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "frozen": false,
            "icon": "ChatInput",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Message",
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "advanced": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "if the first symbol in a reel i s consider to be in position 0 in a 0-based index, then list the index positions of each SCATTER symbol"
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "lf_version": "1.0.15"
          },
          "type": "ChatInput"
        },
        "dragging": false,
        "height": 294,
        "id": "ChatInput-sgEOq",
        "position": {
          "x": 596.5218129363991,
          "y": 248.5402160000408
        },
        "positionAbsolute": {
          "x": 596.5218129363991,
          "y": 248.5402160000408
        },
        "selected": true,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Convert Data into plain text following a specified template.",
          "display_name": "Parse Data",
          "id": "ParseData-4Lvig",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Convert Data into plain text following a specified template.",
            "display_name": "Parse Data",
            "documentation": "",
            "edited": false,
            "field_order": [
              "data",
              "template",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Text",
                "method": "parse_data",
                "name": "text",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"
              },
              "data": {
                "advanced": false,
                "display_name": "Data",
                "dynamic": false,
                "info": "The data to convert to text.",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "name": "data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sep": {
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "sep",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              }
            },
            "lf_version": "1.0.15"
          },
          "type": "ParseData"
        },
        "dragging": false,
        "height": 370,
        "id": "ParseData-4Lvig",
        "position": {
          "x": 1738.7829329736521,
          "y": 1361.5027685828854
        },
        "positionAbsolute": {
          "x": 1738.7829329736521,
          "y": 1361.5027685828854
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-z8nbL",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "context",
                "question"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "icon": "prompts",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "name": "prompt",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "context": {
                "advanced": false,
                "display_name": "context",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "context",
                "password": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message",
                  "Text"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "password": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nQuestion: {question}\n\nAnswer: "
              }
            },
            "lf_version": "1.0.15"
          },
          "type": "Prompt"
        },
        "dragging": false,
        "height": 494,
        "id": "Prompt-z8nbL",
        "position": {
          "x": 2195.228201521283,
          "y": 694.83316377936
        },
        "positionAbsolute": {
          "x": 2195.228201521283,
          "y": 694.83316377936
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-dMHYm",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "ChatOutput",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Message",
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
              },
              "data_template": {
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "advanced": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "lf_version": "1.0.15"
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "height": 294,
        "id": "ChatOutput-dMHYm",
        "position": {
          "x": 3183.728287147309,
          "y": 902.6677310123966
        },
        "positionAbsolute": {
          "x": 3183.728287147309,
          "y": 902.6677310123966
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "id": "pgvector-8NuFT",
        "type": "genericNode",
        "position": {
          "x": 1249.6570639640581,
          "y": 1075.6437991896562
        },
        "data": {
          "type": "pgvector",
          "node": {
            "template": {
              "_type": "Component",
              "embedding": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "embedding",
                "value": "",
                "display_name": "Embedding",
                "advanced": false,
                "input_types": [
                  "Embeddings"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "ingest_data": {
                "trace_as_metadata": true,
                "list": true,
                "trace_as_input": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "ingest_data",
                "value": "",
                "display_name": "Ingestion Data",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import List\n\n#from langchain_community.vectorstores import PGVector\nfrom langchain_postgres import PGVector\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.connection_string_parser import transform_connection_string\n\n\nclass PGVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"PGVector Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    name = \"pgvector\"\n    icon = \"PGVector\"\n\n    inputs = [\n        SecretStrInput(name=\"pg_server_url\", display_name=\"PostgreSQL Server Connection String\", required=True),\n        StrInput(name=\"collection_name\", display_name=\"Table\", required=True),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n    ]\n    \n    def validate_connection_string(self, connection_string: str) -> bool:\n        parts = connection_string.split('@')\n        if len(parts) != 2:\n            return False\n        credentials, host_port_db = parts\n        if ':' not in credentials or ':' not in host_port_db:\n            return False\n        host_port, db = host_port_db.split('/')\n        if ':' not in host_port:\n            return False\n        return True\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n    \n        if not self.validate_connection_string(self.pg_server_url):\n            raise ValueError(\"Invalid connection string format. Expected format: username:password@host:port/database\")\n    \n        # Construct the connection string directly\n        connection_string = f\"postgresql+psycopg://{self.pg_server_url}\"\n    \n        if documents:\n            pgvector = PGVector.from_documents(\n                embedding=self.embedding,\n                documents=documents,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n    \n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_pgvector()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "collection_name": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "collection_name",
                "value": "items",
                "display_name": "Table",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "number_of_results": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "number_of_results",
                "value": 4,
                "display_name": "Number of Results",
                "advanced": true,
                "dynamic": false,
                "info": "Number of results to return.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "pg_server_url": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "pg_server_url",
                "value": "",
                "display_name": "PostgreSQL Server Connection String",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "search_query": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "search_query",
                "value": "",
                "display_name": "Search Query",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "PGVector Vector Store with search capabilities",
            "icon": "PGVector",
            "base_classes": [
              "Data",
              "Retriever",
              "VectorStore"
            ],
            "display_name": "Nicks PGVector Custom Auth",
            "documentation": "https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Retriever"
                ],
                "selected": "Retriever",
                "name": "base_retriever",
                "display_name": "Retriever",
                "method": "build_base_retriever",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "search_results",
                "display_name": "Search Results",
                "method": "search_documents",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "VectorStore"
                ],
                "selected": "VectorStore",
                "name": "vector_store",
                "display_name": "Vector Store",
                "method": "cast_vector_store",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "pg_server_url",
              "collection_name",
              "search_query",
              "ingest_data",
              "embedding",
              "number_of_results",
              "embedding"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.15"
          },
          "id": "pgvector-8NuFT"
        },
        "selected": false,
        "width": 384,
        "height": 644,
        "positionAbsolute": {
          "x": 1249.6570639640581,
          "y": 1075.6437991896562
        },
        "dragging": false
      },
      {
        "id": "OllamaEmbeddings-zW4Ul",
        "type": "genericNode",
        "position": {
          "x": 643.578907492697,
          "y": 783.5843122606201
        },
        "data": {
          "type": "OllamaEmbeddings",
          "node": {
            "template": {
              "_type": "Component",
              "base_url": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Ollama Base URL",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "model": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model",
                "value": "mxbai-embed-large:335m-v1-fp16",
                "display_name": "Ollama Model",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.1,
                "display_name": "Model Temperature",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generate embeddings using Ollama models.",
            "icon": "Ollama",
            "base_classes": [
              "Embeddings"
            ],
            "display_name": "Ollama Embeddings",
            "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Embeddings"
                ],
                "selected": "Embeddings",
                "name": "embeddings",
                "display_name": "Embeddings",
                "method": "build_embeddings",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "model",
              "base_url",
              "temperature"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.15"
          },
          "id": "OllamaEmbeddings-zW4Ul"
        },
        "selected": false,
        "width": 384,
        "height": 388,
        "positionAbsolute": {
          "x": 643.578907492697,
          "y": 783.5843122606201
        },
        "dragging": false
      },
      {
        "id": "OllamaModel-jpiyc",
        "type": "genericNode",
        "position": {
          "x": 2713.7320148852878,
          "y": 517.4838664314314
        },
        "data": {
          "type": "OllamaModel",
          "node": {
            "template": {
              "_type": "Component",
              "base_url": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Base URL",
                "advanced": false,
                "dynamic": false,
                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "format": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "format",
                "value": "",
                "display_name": "Format",
                "advanced": true,
                "dynamic": false,
                "info": "Specify the format of the output (e.g., json).",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "metadata": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "metadata",
                "value": {},
                "display_name": "Metadata",
                "advanced": true,
                "dynamic": false,
                "info": "Metadata to add to the run trace.",
                "title_case": false,
                "type": "dict",
                "_input_type": "DictInput"
              },
              "mirostat": {
                "trace_as_metadata": true,
                "options": [
                  "Disabled",
                  "Mirostat",
                  "Mirostat 2.0"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat",
                "value": "Disabled",
                "display_name": "Mirostat",
                "advanced": true,
                "dynamic": false,
                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                "real_time_refresh": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "mirostat_eta": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat_eta",
                "value": "",
                "display_name": "Mirostat Eta",
                "advanced": true,
                "dynamic": false,
                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "mirostat_tau": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat_tau",
                "value": "",
                "display_name": "Mirostat Tau",
                "advanced": true,
                "dynamic": false,
                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "mxbai-embed-large:335m-v1-fp16",
                  "nomic-embed-text:latest",
                  "deepseek-coder-v2:16b-lite-instruct-fp16",
                  "llama3-groq-tool-use:latest",
                  "gemma-2b-it-q8_0.gguf:latest",
                  "dolphin-mistral:7b-v2.8-fp16",
                  "mistral-nemo:12b-instruct-2407-fp16",
                  "llama3.1:8b-instruct-fp16",
                  "codestral:22b-v0.1-q8_0",
                  "llama3:8b"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "llama3.1:8b-instruct-fp16",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "Refer to https://ollama.ai/library for more models.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "num_ctx": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_ctx",
                "value": "",
                "display_name": "Context Window Size",
                "advanced": true,
                "dynamic": false,
                "info": "Size of the context window for generating tokens. (Default: 2048)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "num_gpu": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_gpu",
                "value": "",
                "display_name": "Number of GPUs",
                "advanced": true,
                "dynamic": false,
                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "num_thread": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_thread",
                "value": "",
                "display_name": "Number of Threads",
                "advanced": true,
                "dynamic": false,
                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "repeat_last_n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "repeat_last_n",
                "value": "",
                "display_name": "Repeat Last N",
                "advanced": true,
                "dynamic": false,
                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "repeat_penalty": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "repeat_penalty",
                "value": "",
                "display_name": "Repeat Penalty",
                "advanced": true,
                "dynamic": false,
                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "stop_tokens": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stop_tokens",
                "value": "",
                "display_name": "Stop Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system",
                "value": "",
                "display_name": "System",
                "advanced": true,
                "dynamic": false,
                "info": "System to use for generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "system_message": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "tags": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tags",
                "value": "",
                "display_name": "Tags",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tags to add to the run trace.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls the creativity of model responses.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "template": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "",
                "display_name": "Template",
                "advanced": true,
                "dynamic": false,
                "info": "Template to use for generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "tfs_z": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tfs_z",
                "value": "",
                "display_name": "TFS Z",
                "advanced": true,
                "dynamic": false,
                "info": "Tail free sampling value. (Default: 1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "timeout": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "timeout",
                "value": "",
                "display_name": "Timeout",
                "advanced": true,
                "dynamic": false,
                "info": "Timeout for the request stream.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Limits token selection to top K. (Default: 40)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "Works together with top-k. (Default: 0.9)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "verbose": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "verbose",
                "value": false,
                "display_name": "Verbose",
                "advanced": false,
                "dynamic": false,
                "info": "Whether to print out response text.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Generate text using Ollama Local LLMs.",
            "icon": "Ollama",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Ollama",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "base_url",
              "model_name",
              "temperature",
              "format",
              "metadata",
              "mirostat",
              "mirostat_eta",
              "mirostat_tau",
              "num_ctx",
              "num_gpu",
              "num_thread",
              "repeat_last_n",
              "repeat_penalty",
              "tfs_z",
              "timeout",
              "top_k",
              "top_p",
              "verbose",
              "tags",
              "stop_tokens",
              "system",
              "template"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.15"
          },
          "id": "OllamaModel-jpiyc"
        },
        "selected": false,
        "width": 384,
        "height": 673,
        "positionAbsolute": {
          "x": 2713.7320148852878,
          "y": 517.4838664314314
        },
        "dragging": false
      },
      {
        "id": "XMLIngestComponent-v0Vke",
        "type": "genericNode",
        "position": {
          "x": -438.47480229619,
          "y": 1540.4230200322936
        },
        "data": {
          "type": "XMLIngestComponent",
          "node": {
            "template": {
              "_type": "Component",
              "xml_file": {
                "trace_as_metadata": true,
                "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                "fileTypes": [
                  "xml"
                ],
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "xml_file",
                "value": "",
                "display_name": "XML File",
                "advanced": false,
                "dynamic": false,
                "info": "Upload an XML file containing game data.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput",
                "load_from_db": false
              },
              "base_url": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Base URL",
                "advanced": false,
                "dynamic": false,
                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import xml.etree.ElementTree as ET\nimport psycopg2\nimport httpx\nfrom axiestudio.io import MessageTextInput\nfrom axiestudio.custom import Component\nfrom axiestudio.io import FileInput, StrInput, FloatInput, Output, DropdownInput\nfrom axiestudio.schema import Data\nfrom langchain_community.embeddings import OllamaEmbeddings\nimport logging\n\nclass XMLIngestComponent(Component):\n    display_name = \"XML Ingest Component\"\n    description = \"Ingests XML game data and inserts it into the PostgreSQL database with embeddings.\"\n    icon = \"file-code\"\n    name = \"XMLIngestComponent\"\n\n    inputs = [\n        FileInput(\n            name=\"xml_file\",\n            display_name=\"XML File\",\n            file_types=[\"xml\"],\n            info=\"Upload an XML file containing game data.\",\n        ),\n        MessageTextInput(\n            name=\"db_connection_string\",\n            display_name=\"DB Connection String\",\n            info=\"Enter the PostgreSQL connection string or connect from another component.\",\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"mxbai-embed-large:335m-v1-fp16\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n            info=\"Temperature for the Ollama model (if applicable).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Ingest Result\", name=\"ingest_result\", method=\"ingest_xml\"),\n    ]\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n            return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    def update_build_config(self, build_config: dict, field_value: any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n        return build_config\n\n    def ingest_xml(self) -> Data:\n        if not self.xml_file:\n            return Data(value=\"Error: No XML file uploaded.\")\n\n        try:\n            with open(self.xml_file, 'r') as file:\n                xml_content = file.read()\n\n            embeddings_model = OllamaEmbeddings(\n                model=self.model_name,\n                base_url=self.base_url,\n                temperature=self.temperature\n            )\n\n            with psycopg2.connect(self.db_connection_string) as conn:\n                agent = self.XMLIngestAgent(conn, embeddings_model)\n                agent.parse_xml_content(xml_content)\n            result = \"XML data ingested successfully\"\n        except Exception as e:\n            result = f\"Error: {str(e)}\"\n\n        return Data(value=result)\n\n    class XMLIngestAgent:\n        def __init__(self, db_connection, embeddings_model):\n            self.db_connection = db_connection\n            self.embeddings_model = embeddings_model\n            logging.basicConfig(level=logging.INFO)\n            self.logger = logging.getLogger(__name__)\n\n        def parse_xml_content(self, xml_content):\n            root = ET.fromstring(xml_content)\n            \n            module_id = root.get('moduleId')\n            rtp = float(root.get('rtp', 0))\n            hit_rate = float(root.get('hitRate', 0))\n            std_deviation = float(root.get('stdDeviation', 0))\n            \n            bet_model = root.find('BetModel')\n            paytable_type = bet_model.get('paytableType')\n            bet_multiplier = int(bet_model.get('betMultiplier', 1))\n        \n            game_text = f\"Game {module_id} with paytable type {paytable_type}, bet multiplier {bet_multiplier}, RTP {rtp}, hit rate {hit_rate}, and standard deviation {std_deviation}\"\n            game_embedding = self.embeddings_model.embed_query(game_text)\n            game_id = self.insert_or_get_game(module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, game_embedding)\n        \n            # Parse and insert symbols\n            for symbol in root.findall('.//Symbol'):\n                name = symbol.get('name')\n                symbol_id = symbol.get('id')\n                if name and symbol_id:\n                    is_wild = symbol.get('isWild') == 'true'\n                    can_wild_substitute = symbol.get('canWildSubstitute') == 'true'\n                    \n                    global_symbol_id = self.insert_or_get_global_symbol(name)\n                    self.insert_game_symbol(game_id, global_symbol_id, int(symbol_id), is_wild, can_wild_substitute)\n        \n            for reel_set in root.findall('.//ReelSet'):\n                reel_set_name = reel_set.get('name')\n                default_positions = [int(reel.get('defaultPosition', 0)) for reel in reel_set.findall('Reel')]\n                reel_set_text = f\"Reel set {reel_set_name} for game {module_id}\"\n                reel_set_embedding = self.embeddings_model.embed_query(reel_set_text)\n                reel_set_id = self.insert_or_get_reel_set(game_id, reel_set_name, default_positions, reel_set_embedding)\n        \n                for reel_index, reel in enumerate(reel_set.findall('Reel')):\n                    symbols = reel.get('symbols', '').split(',')\n                    for symbol_position, symbol_id in enumerate(symbols):\n                        if symbol_id and symbol_id.strip():\n                            symbol_name = self.get_symbol_name(game_id, int(symbol_id))\n                            if symbol_name:\n                                self.insert_reel_symbol(game_id, reel_set_id, reel_index, symbol_position, int(symbol_id), symbol_name)\n                            else:\n                                self.logger.warning(f\"Symbol with ID {symbol_id} not found for game {game_id}\")\n\n        def insert_or_get_game(self, module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Games (module_id, bet_model_paytable_type, bet_model_multiplier, rtp, hit_rate, std_deviation, embedding)\n                VALUES (%s, %s, %s, %s, %s, %s, %s)\n                ON CONFLICT (module_id) DO UPDATE \n                SET bet_model_paytable_type = EXCLUDED.bet_model_paytable_type,\n                    bet_model_multiplier = EXCLUDED.bet_model_multiplier,\n                    rtp = EXCLUDED.rtp,\n                    hit_rate = EXCLUDED.hit_rate,\n                    std_deviation = EXCLUDED.std_deviation,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding))\n            game_id = cursor.fetchone()[0]\n            self.db_connection.commit()\n            return game_id\n\n        def insert_or_get_global_symbol(self, name):\n            cursor = self.db_connection.cursor()\n            symbol_text = f\"Symbol {name}\"\n            symbol_embedding = self.embeddings_model.embed_query(symbol_text)\n            cursor.execute(\"\"\"\n                INSERT INTO GlobalSymbols (name, embedding)\n                VALUES (%s, %s)\n                ON CONFLICT (name) DO UPDATE \n                SET embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (name, symbol_embedding))\n            global_symbol_id = cursor.fetchone()[0]\n            self.db_connection.commit()\n            return global_symbol_id\n\n        def insert_game_symbol(self, game_id, global_symbol_id, game_specific_symbol_id, is_wild, can_wild_substitute):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO GameSymbols (game_id, global_symbol_id, game_specific_symbol_id, is_wild, can_wild_substitute)\n                VALUES (%s, %s, %s, %s, %s)\n                ON CONFLICT (game_id, game_specific_symbol_id) \n                DO UPDATE SET \n                    global_symbol_id = EXCLUDED.global_symbol_id,\n                    is_wild = EXCLUDED.is_wild,\n                    can_wild_substitute = EXCLUDED.can_wild_substitute\n            \"\"\", (game_id, global_symbol_id, game_specific_symbol_id, is_wild, can_wild_substitute))\n            self.db_connection.commit()\n\n        def insert_or_get_reel_set(self, game_id, reel_set_name, default_positions, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO ReelSets (game_id, name, default_positions, embedding)\n                VALUES (%s, %s, %s, %s)\n                ON CONFLICT (game_id, name) DO UPDATE \n                SET default_positions = EXCLUDED.default_positions,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (game_id, reel_set_name, default_positions, embedding))\n            reel_set_id = cursor.fetchone()[0]\n            self.db_connection.commit()\n            return reel_set_id\n\n        def get_symbol_name(self, game_id, game_specific_symbol_id):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                SELECT gs.name \n                FROM GameSymbols gs\n                JOIN GlobalSymbols gls ON gs.global_symbol_id = gls.id\n                WHERE gs.game_id = %s AND gs.game_specific_symbol_id = %s\n            \"\"\", (game_id, game_specific_symbol_id))\n            result = cursor.fetchone()\n            return result[0] if result else None\n\n        def insert_reel_symbol(self, game_id, reel_set_id, reel_index, symbol_position, game_specific_symbol_id, symbol_name):\n            cursor = self.db_connection.cursor()\n            try:\n                cursor.execute(\"\"\"\n                    INSERT INTO Reels (game_id, reel_set_id, reel_index, symbol_reel_position, game_symbol_id, symbol_name)\n                    VALUES (%s, %s, %s, %s, %s, %s)\n                    ON CONFLICT (game_id, reel_set_id, reel_index, symbol_reel_position) \n                    DO UPDATE SET game_symbol_id = EXCLUDED.game_symbol_id, symbol_name = EXCLUDED.symbol_name\n                \"\"\", (game_id, reel_set_id, reel_index, symbol_position, game_specific_symbol_id, symbol_name))\n                self.db_connection.commit()\n            except Exception as e:\n                self.logger.error(f\"Error inserting reel symbol: {e}\")\n                self.logger.error(f\"game_id: {game_id}, reel_set_id: {reel_set_id}, reel_index: {reel_index}, symbol_position: {symbol_position}, game_specific_symbol_id: {game_specific_symbol_id}, symbol_name: {symbol_name}\")\n                raise\n\n# Register the custom component\nXMLIngestComponent()",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "db_connection_string": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "db_connection_string",
                "value": "NormalDBConnectionString",
                "display_name": "DB Connection String",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Enter the PostgreSQL connection string or connect from another component.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "mxbai-embed-large:335m-v1-fp16",
                  "nomic-embed-text:latest",
                  "deepseek-coder-v2:16b-lite-instruct-fp16",
                  "llama3-groq-tool-use:latest",
                  "gemma-2b-it-q8_0.gguf:latest",
                  "dolphin-mistral:7b-v2.8-fp16",
                  "mistral-nemo:12b-instruct-2407-fp16",
                  "llama3.1:8b-instruct-fp16",
                  "codestral:22b-v0.1-q8_0",
                  "llama3:8b"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "mxbai-embed-large:335m-v1-fp16",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "Refer to https://ollama.ai/library for more models.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.1,
                "display_name": "Model Temperature",
                "advanced": true,
                "dynamic": false,
                "info": "Temperature for the Ollama model (if applicable).",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Ingests XML game data and inserts it into the PostgreSQL database with embeddings.",
            "icon": "file-code",
            "base_classes": [
              "Data"
            ],
            "display_name": "Fucked v4 DB XMLIngestNodeEmbeddings",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "ingest_result",
                "display_name": "Ingest Result",
                "method": "ingest_xml",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "xml_file",
              "db_connection_string",
              "base_url",
              "model_name",
              "temperature"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.15"
          },
          "id": "XMLIngestComponent-v0Vke"
        },
        "selected": false,
        "width": 384,
        "height": 580,
        "positionAbsolute": {
          "x": -438.47480229619,
          "y": 1540.4230200322936
        },
        "dragging": false
      },
      {
        "id": "CustomComponent-B0UyY",
        "type": "genericNode",
        "position": {
          "x": 68.23695411058566,
          "y": -64.06857640368476
        },
        "data": {
          "type": "slot_game_chatbot",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.field_typing import NestedDict\nfrom axiestudio.io import StrInput, MessageTextInput, Output\nfrom axiestudio.schema import Data\nfrom langchain_community.vectorstores import PGVector\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.agents import create_sql_agent\nfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.llms import OpenAI\nclass SlotGameChatbotComponent(Component):\n    display_name = \"Slot Game Chatbot\"\n    description = \"A chatbot for querying slot game data using hybrid search (vector + SQL).\"\n    icon = \"chatbot\"\n    name = \"slot_game_chatbot\"\n    inputs = [\n        StrInput(name=\"db_connection_string\", display_name=\"Database Connection String\", value=\"\"),\n        MessageTextInput(name=\"user_query\", display_name=\"User Query\", value=\"\"),\n    ]\n    outputs = [\n        Output(display_name=\"Chatbot Response\", name=\"response\", method=\"process_query\"),\n    ]\n    def hybrid_search(self, query_text, top_k=5):\n        # Vector search\n        query_embedding = self.embeddings_model.embed_query(query_text)\n        vector_store = PGVector(connection_string=self.db_connection_string, embedding_function=self.embeddings_model)\n        vector_results = vector_store.similarity_search_with_score(query_embedding, k=top_k)\n        # SQL search\n        db = SQLDatabase.from_uri(self.db_connection_string)\n        toolkit = SQLDatabaseToolkit(db=db, llm=self.llm)\n        agent = create_sql_agent(llm=self.llm, toolkit=toolkit, verbose=True)\n        sql_result = agent.run(query_text)\n        return vector_results, sql_result\n    def generate_response(self, query_text, vector_results, sql_result):\n        context = f\"\"\"\n        Query: {query_text}\n        Vector Search Results: {vector_results}\n        SQL Query Results: {sql_result}\n        Based on the above information, provide a detailed response to the user's query.\n        If applicable, include the reel state position sequence and the corresponding symbols.\n        Also, if possible, generate a TestData JSON configuration based on the results.\n        \"\"\"\n        response = self.llm.generate(context)\n        return response\n    def process_query(self) -> Data:\n        try:\n            # In a real scenario, you'd initialize these properly\n            self.embeddings_model = OpenAI()  # placeholder\n            self.llm = OpenAI()  # placeholder\n            vector_results, sql_result = self.hybrid_search(self.user_query)\n            response = self.generate_response(self.user_query, vector_results, sql_result)\n\n            return Data(value=response)\n        except Exception as e:\n            return Data(value=f\"Error processing query: {str(e)}\")\n    def generate_testdata_json(self, module_id, reel_set, positions):\n        testdata = {\n            \"testdata\": {\n                \"moduleId\": module_id,\n                \"clientId\": \"[ClientID]\",\n                \"productId\": \"[ProductID]\",\n                \"username\": \"[UserName]\",\n                \"behaviourName\": \"[BehaviourName]\",\n                \"guid\": \"[GUID]\",\n                \"options\": {\n                    f\"{reel_set}\": reel_set,\n                }\n            }\n        }\n        for i, pos in enumerate(positions):\n            testdata[\"testdata\"][\"options\"][f\"{reel_set}${i}\"] = str(pos)\n        return testdata\n# Register the custom component\nSlotGameChatbotComponent()",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "db_connection_string": {
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "db_connection_string",
                "value": "NormalDBConnectionString",
                "display_name": "Database Connection String",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "user_query": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "user_query",
                "value": "How many BONUS Symbols are in the 1st reel of the BaseGame.Type2 Reelset?",
                "display_name": "User Query",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "A chatbot for querying slot game data using hybrid search (vector + SQL).",
            "icon": "chatbot",
            "base_classes": [
              "Data"
            ],
            "display_name": "SpecialSlotChatBot",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "response",
                "display_name": "Chatbot Response",
                "method": "process_query",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "db_connection_string",
              "user_query"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.15"
          },
          "id": "CustomComponent-B0UyY"
        },
        "selected": false,
        "width": 384,
        "height": 408,
        "positionAbsolute": {
          "x": 68.23695411058566,
          "y": -64.06857640368476
        },
        "dragging": false
      },
      {
        "id": "XMLIngestComponent-pNr7W",
        "type": "genericNode",
        "position": {
          "x": 100.07916266073232,
          "y": 1543.6572138966408
        },
        "data": {
          "type": "XMLIngestComponent",
          "node": {
            "template": {
              "_type": "Component",
              "xml_file": {
                "trace_as_metadata": true,
                "file_path": "",
                "fileTypes": [
                  "xml"
                ],
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "xml_file",
                "value": "",
                "display_name": "XML File",
                "advanced": false,
                "dynamic": false,
                "info": "Upload an XML file containing game data.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput",
                "load_from_db": false
              },
              "base_url": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Base URL",
                "advanced": false,
                "dynamic": false,
                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import xml.etree.ElementTree as ET\nimport psycopg2\nimport httpx\nfrom axiestudio.io import MessageTextInput\nfrom axiestudio.custom import Component\nfrom axiestudio.io import FileInput, StrInput, FloatInput, Output, DropdownInput\nfrom axiestudio.schema import Data\nfrom langchain_community.embeddings import OllamaEmbeddings\n\nclass XMLIngestComponent(Component):\n    display_name = \"XML Ingest Component\"\n    description = \"Ingests XML game data and inserts it into the PostgreSQL database with embeddings.\"\n    icon = \"file-code\"\n    name = \"XMLIngestComponent\"\n\n    inputs = [\n        FileInput(\n            name=\"xml_file\",\n            display_name=\"XML File\",\n            file_types=[\"xml\"],\n            info=\"Upload an XML file containing game data.\",\n        ),\n        MessageTextInput(\n            name=\"db_connection_string\",\n            display_name=\"DB Connection String\",\n            info=\"Enter the PostgreSQL connection string or connect from another component.\",\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"mxbai-embed-large:335m-v1-fp16\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n            info=\"Temperature for the Ollama model (if applicable).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Ingest Result\", name=\"ingest_result\", method=\"ingest_xml\"),\n    ]\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    def update_build_config(self, build_config: dict, field_value: any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n        return build_config\n\n    def ingest_xml(self) -> Data:\n        if not self.xml_file:\n            return Data(value=\"Error: No XML file uploaded.\")\n\n        try:\n            with open(self.xml_file, 'r') as file:\n                xml_content = file.read()\n\n            embeddings_model = OllamaEmbeddings(\n                model=self.model_name,\n                base_url=self.base_url,\n                temperature=self.temperature\n            )\n\n            with psycopg2.connect(self.db_connection_string) as conn:\n                agent = self.XMLIngestAgent(conn, embeddings_model)\n                agent.parse_xml_content(xml_content)\n            result = \"XML data ingested successfully\"\n        except Exception as e:\n            result = f\"Error: {str(e)}\"\n\n        return Data(value=result)\n\n    class XMLIngestAgent:\n        def __init__(self, db_connection, embeddings_model):\n            self.db_connection = db_connection\n            self.embeddings_model = embeddings_model\n    \n        def parse_xml_content(self, xml_content):\n            root = ET.fromstring(xml_content)\n            \n            module_id = root.get('moduleId')\n            rtp = float(root.get('rtp', 0))\n            hit_rate = float(root.get('hitRate', 0))\n            std_deviation = float(root.get('stdDeviation', 0))\n            \n            bet_model = root.find('BetModel')\n            paytable_type = bet_model.get('paytableType')\n            bet_multiplier = int(bet_model.get('betMultiplier', 1))\n        \n            game_text = f\"Game {module_id} with paytable type {paytable_type}, bet multiplier {bet_multiplier}, RTP {rtp}, hit rate {hit_rate}, and standard deviation {std_deviation}\"\n            game_embedding = self.embeddings_model.embed_query(game_text)\n            game_id = self.insert_or_get_game(module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, game_embedding)\n        \n            symbol_texts = []\n            symbol_data = []\n    \n            for reel_set in root.findall('.//ReelSet'):\n                reel_set_name = reel_set.get('name')\n                default_positions = [int(reel.get('defaultPosition', 0)) for reel in reel_set.findall('Reel')]\n                reel_set_text = f\"Reel set {reel_set_name} for game {module_id}\"\n                reel_set_embedding = self.embeddings_model.embed_query(reel_set_text)\n                reel_set_id = self.insert_or_get_reel_set(game_id, reel_set_name, default_positions, reel_set_embedding)\n        \n                for reel_index, reel in enumerate(reel_set.findall('Reel')):\n                    symbols = reel.get('symbols').split(',')\n                    for symbol_position, symbol in enumerate(symbols):\n                        symbol_text = f\"Symbol {symbol} at position {symbol_position} on reel {reel_index} in reel set {reel_set_name} of game {module_id}\"\n                        symbol_texts.append(symbol_text)\n                        symbol_data.append((game_id, reel_set_id, reel_index, symbol_position, symbol))\n    \n            # Batch embed all symbols\n            symbol_embeddings = self.embeddings_model.embed_documents(symbol_texts)\n    \n            # Batch insert all symbols\n            self.batch_insert_reel_symbols(symbol_data, symbol_embeddings)\n    \n        def batch_insert_reel_symbols(self, symbol_data, symbol_embeddings):\n            cursor = self.db_connection.cursor()\n            for (game_id, reel_set_id, reel_index, symbol_position, symbol), embedding in zip(symbol_data, symbol_embeddings):\n                cursor.execute(\"\"\"\n                    INSERT INTO Reels (game_id, reel_set_id, reel_index, symbol_reel_position, symbol, embedding)\n                    VALUES (%s, %s, %s, %s, %s, %s)\n                    ON CONFLICT (game_id, reel_set_id, reel_index, symbol_reel_position) \n                    DO UPDATE SET symbol = EXCLUDED.symbol, embedding = EXCLUDED.embedding\n                \"\"\", (game_id, reel_set_id, reel_index, symbol_position, symbol, embedding))\n            self.db_connection.commit()\n\n        def insert_or_get_game(self, module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Games (module_id, bet_model_paytable_type, bet_model_multiplier, rtp, hit_rate, std_deviation, embedding)\n                VALUES (%s, %s, %s, %s, %s, %s, %s)\n                ON CONFLICT (module_id) DO UPDATE \n                SET bet_model_paytable_type = EXCLUDED.bet_model_paytable_type,\n                    bet_model_multiplier = EXCLUDED.bet_model_multiplier,\n                    rtp = EXCLUDED.rtp,\n                    hit_rate = EXCLUDED.hit_rate,\n                    std_deviation = EXCLUDED.std_deviation,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_or_get_reel_set(self, game_id, reel_set_name, default_positions, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO ReelSets (game_id, name, default_positions, embedding)\n                VALUES (%s, %s, %s, %s)\n                ON CONFLICT (game_id, name) DO UPDATE \n                SET default_positions = EXCLUDED.default_positions,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (game_id, reel_set_name, default_positions, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_reel_symbols(self, reel_set_id, reel_index, symbol_position, symbol, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Reels (reel_set_id, reel_index, symbol_reel_position, symbol, embedding)\n                VALUES (%s, %s, %s, %s, %s)\n                ON CONFLICT (reel_set_id, reel_index, symbol_reel_position) \n                DO UPDATE SET symbol = EXCLUDED.symbol, embedding = EXCLUDED.embedding\n            \"\"\", (reel_set_id, reel_index, symbol_position, symbol, embedding))\n            self.db_connection.commit()\n\n# Register the custom component\nXMLIngestComponent()",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "db_connection_string": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "db_connection_string",
                "value": "NormalDBConnectionString",
                "display_name": "DB Connection String",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Enter the PostgreSQL connection string or connect from another component.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "mxbai-embed-large:335m-v1-fp16",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "Refer to https://ollama.ai/library for more models.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.1,
                "display_name": "Model Temperature",
                "advanced": true,
                "dynamic": false,
                "info": "Temperature for the Ollama model (if applicable).",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Ingests XML game data and inserts it into the PostgreSQL database with embeddings.",
            "icon": "file-code",
            "base_classes": [
              "Data"
            ],
            "display_name": "BACKUP XMLIngestNodeEmbeddings",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "ingest_result",
                "display_name": "Ingest Result",
                "method": "ingest_xml",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "xml_file",
              "db_connection_string",
              "base_url",
              "model_name",
              "temperature"
            ],
            "beta": false,
            "edited": true
          },
          "id": "XMLIngestComponent-pNr7W"
        },
        "selected": false,
        "width": 384,
        "height": 580,
        "dragging": false,
        "positionAbsolute": {
          "x": 100.07916266073232,
          "y": 1543.6572138966408
        }
      },
      {
        "id": "XMLIngestComponent-EOvct",
        "type": "genericNode",
        "position": {
          "x": 63.385636526592066,
          "y": 605.1335863778307
        },
        "data": {
          "type": "XMLIngestComponent",
          "node": {
            "template": {
              "_type": "Component",
              "xml_file": {
                "trace_as_metadata": true,
                "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                "fileTypes": [
                  "xml"
                ],
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "xml_file",
                "value": "",
                "display_name": "XML File",
                "advanced": false,
                "dynamic": false,
                "info": "Upload an XML file containing game data.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "base_url": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Base URL",
                "advanced": false,
                "dynamic": false,
                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import xml.etree.ElementTree as ET\nimport psycopg2\nimport httpx\nfrom axiestudio.io import MessageTextInput\nfrom axiestudio.custom import Component\nfrom axiestudio.io import FileInput, StrInput, FloatInput, Output, DropdownInput\nfrom axiestudio.schema import Data\nfrom langchain_community.embeddings import OllamaEmbeddings\n\nclass XMLIngestComponent(Component):\n    display_name = \"XML Ingest Component\"\n    description = \"Ingests XML game data and inserts it into the PostgreSQL database with embeddings.\"\n    icon = \"file-code\"\n    name = \"XMLIngestComponent\"\n\n    inputs = [\n        FileInput(\n            name=\"xml_file\",\n            display_name=\"XML File\",\n            file_types=[\"xml\"],\n            info=\"Upload an XML file containing game data.\",\n        ),\n        MessageTextInput(\n            name=\"db_connection_string\",\n            display_name=\"DB Connection String\",\n            info=\"Enter the PostgreSQL connection string or connect from another component.\",\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"mxbai-embed-large:335m-v1-fp16\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n            info=\"Temperature for the Ollama model (if applicable).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Ingest Result\", name=\"ingest_result\", method=\"ingest_xml\"),\n    ]\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n            return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    def update_build_config(self, build_config: dict, field_value: any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n        return build_config\n\n    def ingest_xml(self) -> Data:\n        if not self.xml_file:\n            return Data(value=\"Error: No XML file uploaded.\")\n\n        try:\n            with open(self.xml_file, 'r') as file:\n                xml_content = file.read()\n\n            embeddings_model = OllamaEmbeddings(\n                model=self.model_name,\n                base_url=self.base_url,\n                temperature=self.temperature\n            )\n\n            with psycopg2.connect(self.db_connection_string) as conn:\n                agent = self.XMLIngestAgent(conn, embeddings_model)\n                agent.parse_xml_content(xml_content)\n            result = \"XML data ingested successfully\"\n        except Exception as e:\n            result = f\"Error: {str(e)}\"\n\n        return Data(value=result)\n\n    class XMLIngestAgent:\n        def __init__(self, db_connection, embeddings_model):\n            self.db_connection = db_connection\n            self.embeddings_model = embeddings_model\n    \n        def parse_xml_content(self, xml_content):\n            root = ET.fromstring(xml_content)\n            \n            module_id = root.get('moduleId')\n            rtp = float(root.get('rtp', 0))\n            hit_rate = float(root.get('hitRate', 0))\n            std_deviation = float(root.get('stdDeviation', 0))\n            \n            bet_model = root.find('BetModel')\n            paytable_type = bet_model.get('paytableType')\n            bet_multiplier = int(bet_model.get('betMultiplier', 1))\n        \n            game_text = f\"Game {module_id} with paytable type {paytable_type}, bet multiplier {bet_multiplier}, RTP {rtp}, hit rate {hit_rate}, and standard deviation {std_deviation}\"\n            game_embedding = self.embeddings_model.embed_query(game_text)\n            game_id = self.insert_or_get_game(module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, game_embedding)\n        \n            symbol_data = []\n    \n            for reel_set in root.findall('.//ReelSet'):\n                reel_set_name = reel_set.get('name')\n                default_positions = [int(reel.get('defaultPosition', 0)) for reel in reel_set.findall('Reel')]\n                reel_set_text = f\"Reel set {reel_set_name} for game {module_id}\"\n                reel_set_embedding = self.embeddings_model.embed_query(reel_set_text)\n                reel_set_id = self.insert_or_get_reel_set(game_id, reel_set_name, default_positions, reel_set_embedding)\n        \n                for reel_index, reel in enumerate(reel_set.findall('Reel')):\n                    symbols = reel.get('symbols').split(',')\n                    for symbol_position, symbol_name in enumerate(symbols):\n                        symbol_id = self.insert_or_get_symbol(symbol_name)\n                        symbol_text = f\"Symbol {symbol_name} at position {symbol_position} on reel {reel_index} in reel set {reel_set_name} of game {module_id}\"\n                        symbol_embedding = self.embeddings_model.embed_query(symbol_text)\n                        symbol_data.append((game_id, reel_set_id, reel_index, symbol_position, symbol_id, symbol_embedding))\n    \n            # Batch insert all symbols\n            self.batch_insert_reel_symbols(symbol_data)\n    \n        def batch_insert_reel_symbols(self, symbol_data):\n            cursor = self.db_connection.cursor()\n            for game_id, reel_set_id, reel_index, symbol_position, symbol_id, embedding in symbol_data:\n                cursor.execute(\"\"\"\n                    INSERT INTO Reels (game_id, reel_set_id, reel_index, symbol_reel_position, symbol_id, embedding)\n                    VALUES (%s, %s, %s, %s, %s, %s)\n                    ON CONFLICT (game_id, reel_set_id, reel_index, symbol_reel_position) \n                    DO UPDATE SET symbol_id = EXCLUDED.symbol_id, embedding = EXCLUDED.embedding\n                \"\"\", (game_id, reel_set_id, reel_index, symbol_position, symbol_id, embedding))\n            self.db_connection.commit()\n\n        def insert_or_get_game(self, module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Games (module_id, bet_model_paytable_type, bet_model_multiplier, rtp, hit_rate, std_deviation, embedding)\n                VALUES (%s, %s, %s, %s, %s, %s, %s)\n                ON CONFLICT (module_id) DO UPDATE \n                SET bet_model_paytable_type = EXCLUDED.bet_model_paytable_type,\n                    bet_model_multiplier = EXCLUDED.bet_model_multiplier,\n                    rtp = EXCLUDED.rtp,\n                    hit_rate = EXCLUDED.hit_rate,\n                    std_deviation = EXCLUDED.std_deviation,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_or_get_reel_set(self, game_id, reel_set_name, default_positions, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO ReelSets (game_id, name, default_positions, embedding)\n                VALUES (%s, %s, %s, %s)\n                ON CONFLICT (game_id, name) DO UPDATE \n                SET default_positions = EXCLUDED.default_positions,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (game_id, reel_set_name, default_positions, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_or_get_symbol(self, symbol_name):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Symbols (name)\n                VALUES (%s)\n                ON CONFLICT (name) DO NOTHING\n                RETURNING id\n            \"\"\", (symbol_name,))\n            result = cursor.fetchone()\n            if result:\n                return result[0]\n            else:\n                cursor.execute(\"SELECT id FROM Symbols WHERE name = %s\", (symbol_name,))\n                return cursor.fetchone()[0]\n\n# Register the custom component\nXMLIngestComponent()",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "db_connection_string": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "db_connection_string",
                "value": "NormalDBConnectionString",
                "display_name": "DB Connection String",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Enter the PostgreSQL connection string or connect from another component.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "mxbai-embed-large:335m-v1-fp16",
                  "nomic-embed-text:latest",
                  "deepseek-coder-v2:16b-lite-instruct-fp16",
                  "llama3-groq-tool-use:latest",
                  "gemma-2b-it-q8_0.gguf:latest",
                  "dolphin-mistral:7b-v2.8-fp16",
                  "mistral-nemo:12b-instruct-2407-fp16",
                  "llama3.1:8b-instruct-fp16",
                  "codestral:22b-v0.1-q8_0",
                  "llama3:8b"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "mxbai-embed-large:335m-v1-fp16",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "Refer to https://ollama.ai/library for more models.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.1,
                "display_name": "Model Temperature",
                "advanced": true,
                "dynamic": false,
                "info": "Temperature for the Ollama model (if applicable).",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Ingests XML game data and inserts it into the PostgreSQL database with embeddings.",
            "icon": "file-code",
            "base_classes": [
              "Data"
            ],
            "display_name": "v5 DB XMLIngestNodeEmbeddings",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "ingest_result",
                "display_name": "Ingest Result",
                "method": "ingest_xml",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "xml_file",
              "db_connection_string",
              "base_url",
              "model_name",
              "temperature"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.15"
          },
          "id": "XMLIngestComponent-EOvct"
        },
        "selected": false,
        "width": 384,
        "height": 580,
        "positionAbsolute": {
          "x": 63.385636526592066,
          "y": 605.1335863778307
        },
        "dragging": false
      },
      {
        "data": {
          "id": "groupComponent-NSkEw",
          "type": "GroupNode",
          "node": {
            "display_name": "Group",
            "documentation": "",
            "description": "",
            "template": {
              "chunk_overlap_SplitText-xQWvM": {
                "advanced": true,
                "display_name": "Chunk Overlap",
                "dynamic": false,
                "info": "Number of characters to overlap between chunks.",
                "list": false,
                "name": "chunk_overlap",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": "200",
                "proxy": {
                  "id": "SplitText-xQWvM",
                  "field": "chunk_overlap"
                }
              },
              "chunk_size_SplitText-xQWvM": {
                "advanced": true,
                "display_name": "Chunk Size",
                "dynamic": false,
                "info": "The maximum number of characters in each chunk.",
                "list": false,
                "name": "chunk_size",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1000,
                "proxy": {
                  "id": "SplitText-xQWvM",
                  "field": "chunk_size"
                }
              },
              "code_SplitText-xQWvM": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom axiestudio.custom import Component\nfrom axiestudio.io import HandleInput, IntInput, MessageTextInput, Output\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n",
                "display_name": "code",
                "proxy": {
                  "id": "SplitText-xQWvM",
                  "field": "code"
                }
              },
              "separator_SplitText-xQWvM": {
                "advanced": false,
                "display_name": "Separator",
                "dynamic": false,
                "info": "The character to split on. Defaults to newline.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "separator",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n",
                "proxy": {
                  "id": "SplitText-xQWvM",
                  "field": "separator"
                }
              },
              "code_File-HmFrT": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from pathlib import Path\n\nfrom axiestudio.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, FileInput, Output\nfrom axiestudio.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data if data else \"No data\"\n        return data or Data()\n",
                "display_name": "code",
                "proxy": {
                  "id": "File-HmFrT",
                  "field": "code"
                }
              },
              "path_File-HmFrT": {
                "advanced": true,
                "display_name": "Path",
                "dynamic": false,
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx"
                ],
                "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx",
                "list": false,
                "name": "path",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": "",
                "proxy": {
                  "id": "File-HmFrT",
                  "field": "path"
                }
              },
              "silent_errors_File-HmFrT": {
                "advanced": true,
                "display_name": "Silent Errors",
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "list": false,
                "name": "silent_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false,
                "proxy": {
                  "id": "File-HmFrT",
                  "field": "silent_errors"
                }
              },
              "base_url_OllamaEmbeddings-tSXRh": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Ollama Base URL",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput",
                "proxy": {
                  "id": "OllamaEmbeddings-tSXRh",
                  "field": "base_url"
                }
              },
              "code_OllamaEmbeddings-tSXRh": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "OllamaEmbeddings-tSXRh",
                  "field": "code"
                }
              },
              "model_OllamaEmbeddings-tSXRh": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model",
                "value": "mxbai-embed-large:335m-v1-fp16",
                "display_name": "Ollama Model",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput",
                "proxy": {
                  "id": "OllamaEmbeddings-tSXRh",
                  "field": "model"
                }
              },
              "temperature_OllamaEmbeddings-tSXRh": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.1,
                "display_name": "Model Temperature",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput",
                "proxy": {
                  "id": "OllamaEmbeddings-tSXRh",
                  "field": "temperature"
                }
              },
              "code_pgvector-TRQkk": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import List\n\n#from langchain_community.vectorstores import PGVector\nfrom langchain_postgres import PGVector\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.connection_string_parser import transform_connection_string\n\n\nclass PGVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"PGVector Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    name = \"pgvector\"\n    icon = \"PGVector\"\n\n    inputs = [\n        SecretStrInput(name=\"pg_server_url\", display_name=\"PostgreSQL Server Connection String\", required=True),\n        StrInput(name=\"collection_name\", display_name=\"Table\", required=True),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n    ]\n    \n    def validate_connection_string(self, connection_string: str) -> bool:\n        parts = connection_string.split('@')\n        if len(parts) != 2:\n            return False\n        credentials, host_port_db = parts\n        if ':' not in credentials or ':' not in host_port_db:\n            return False\n        host_port, db = host_port_db.split('/')\n        if ':' not in host_port:\n            return False\n        return True\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n    \n        if not self.validate_connection_string(self.pg_server_url):\n            raise ValueError(\"Invalid connection string format. Expected format: username:password@host:port/database\")\n    \n        # Construct the connection string directly\n        connection_string = f\"postgresql+psycopg://{self.pg_server_url}\"\n    \n        if documents:\n            pgvector = PGVector.from_documents(\n                embedding=self.embedding,\n                documents=documents,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n    \n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_pgvector()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code",
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "field": "code"
                }
              },
              "collection_name_pgvector-TRQkk": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "collection_name",
                "value": "items",
                "display_name": "Table",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput",
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "field": "collection_name"
                }
              },
              "number_of_results_pgvector-TRQkk": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "number_of_results",
                "value": 4,
                "display_name": "Number of Results",
                "advanced": true,
                "dynamic": false,
                "info": "Number of results to return.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "field": "number_of_results"
                }
              },
              "pg_server_url_pgvector-TRQkk": {
                "load_from_db": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "pg_server_url",
                "value": "",
                "display_name": "PostgreSQL Server Connection String",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput",
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "field": "pg_server_url"
                }
              },
              "search_query_pgvector-TRQkk": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "search_query",
                "value": "",
                "display_name": "Search Query",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput",
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "field": "search_query"
                }
              }
            },
            "flow": {
              "data": {
                "nodes": [
                  {
                    "data": {
                      "description": "Split text into chunks based on specified criteria.",
                      "display_name": "Split Text",
                      "id": "SplitText-xQWvM",
                      "node": {
                        "base_classes": [
                          "Data"
                        ],
                        "beta": false,
                        "conditional_paths": [],
                        "custom_fields": {},
                        "description": "Split text into chunks based on specified criteria.",
                        "display_name": "Split Text",
                        "documentation": "",
                        "edited": false,
                        "field_order": [
                          "data_inputs",
                          "chunk_overlap",
                          "chunk_size",
                          "separator"
                        ],
                        "frozen": false,
                        "icon": "scissors-line-dashed",
                        "output_types": [],
                        "outputs": [
                          {
                            "cache": true,
                            "display_name": "Chunks",
                            "method": "split_text",
                            "name": "chunks",
                            "selected": "Data",
                            "types": [
                              "Data"
                            ],
                            "value": "__UNDEFINED__"
                          }
                        ],
                        "pinned": false,
                        "template": {
                          "_type": "Component",
                          "chunk_overlap": {
                            "advanced": false,
                            "display_name": "Chunk Overlap",
                            "dynamic": false,
                            "info": "Number of characters to overlap between chunks.",
                            "list": false,
                            "name": "chunk_overlap",
                            "placeholder": "",
                            "required": false,
                            "show": true,
                            "title_case": false,
                            "trace_as_metadata": true,
                            "type": "int",
                            "value": "200"
                          },
                          "chunk_size": {
                            "advanced": false,
                            "display_name": "Chunk Size",
                            "dynamic": false,
                            "info": "The maximum number of characters in each chunk.",
                            "list": false,
                            "name": "chunk_size",
                            "placeholder": "",
                            "required": false,
                            "show": true,
                            "title_case": false,
                            "trace_as_metadata": true,
                            "type": "int",
                            "value": 1000
                          },
                          "code": {
                            "advanced": true,
                            "dynamic": true,
                            "fileTypes": [],
                            "file_path": "",
                            "info": "",
                            "list": false,
                            "load_from_db": false,
                            "multiline": true,
                            "name": "code",
                            "password": false,
                            "placeholder": "",
                            "required": true,
                            "show": true,
                            "title_case": false,
                            "type": "code",
                            "value": "from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom axiestudio.custom import Component\nfrom axiestudio.io import HandleInput, IntInput, MessageTextInput, Output\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n",
                            "display_name": "code"
                          },
                          "data_inputs": {
                            "advanced": false,
                            "display_name": "Data Inputs",
                            "dynamic": false,
                            "info": "The data to split.",
                            "input_types": [
                              "Data"
                            ],
                            "list": true,
                            "name": "data_inputs",
                            "placeholder": "",
                            "required": false,
                            "show": true,
                            "title_case": false,
                            "trace_as_metadata": true,
                            "type": "other",
                            "value": ""
                          },
                          "separator": {
                            "advanced": false,
                            "display_name": "Separator",
                            "dynamic": false,
                            "info": "The character to split on. Defaults to newline.",
                            "input_types": [
                              "Message"
                            ],
                            "list": false,
                            "load_from_db": false,
                            "name": "separator",
                            "placeholder": "",
                            "required": false,
                            "show": true,
                            "title_case": false,
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "type": "str",
                            "value": "\n"
                          }
                        },
                        "lf_version": "1.0.15"
                      },
                      "type": "SplitText"
                    },
                    "dragging": false,
                    "height": 542,
                    "id": "SplitText-xQWvM",
                    "position": {
                      "x": -1615.5378919517811,
                      "y": 150.45059818858732
                    },
                    "positionAbsolute": {
                      "x": -1615.5378919517811,
                      "y": 150.45059818858732
                    },
                    "selected": true,
                    "type": "genericNode",
                    "width": 384
                  },
                  {
                    "data": {
                      "description": "A generic file loader.",
                      "display_name": "File",
                      "id": "File-HmFrT",
                      "node": {
                        "base_classes": [
                          "Data"
                        ],
                        "beta": false,
                        "conditional_paths": [],
                        "custom_fields": {},
                        "description": "A generic file loader.",
                        "display_name": "File",
                        "documentation": "",
                        "edited": false,
                        "field_order": [
                          "path",
                          "silent_errors"
                        ],
                        "frozen": false,
                        "icon": "file-text",
                        "output_types": [],
                        "outputs": [
                          {
                            "cache": true,
                            "display_name": "Data",
                            "method": "load_file",
                            "name": "data",
                            "selected": "Data",
                            "types": [
                              "Data"
                            ],
                            "value": "__UNDEFINED__"
                          }
                        ],
                        "pinned": false,
                        "template": {
                          "_type": "Component",
                          "code": {
                            "advanced": true,
                            "dynamic": true,
                            "fileTypes": [],
                            "file_path": "",
                            "info": "",
                            "list": false,
                            "load_from_db": false,
                            "multiline": true,
                            "name": "code",
                            "password": false,
                            "placeholder": "",
                            "required": true,
                            "show": true,
                            "title_case": false,
                            "type": "code",
                            "value": "from pathlib import Path\n\nfrom axiestudio.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, FileInput, Output\nfrom axiestudio.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data if data else \"No data\"\n        return data or Data()\n",
                            "display_name": "code"
                          },
                          "path": {
                            "advanced": false,
                            "display_name": "Path",
                            "dynamic": false,
                            "fileTypes": [
                              "txt",
                              "md",
                              "mdx",
                              "csv",
                              "json",
                              "yaml",
                              "yml",
                              "xml",
                              "html",
                              "htm",
                              "pdf",
                              "docx",
                              "py",
                              "sh",
                              "sql",
                              "js",
                              "ts",
                              "tsx"
                            ],
                            "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                            "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx",
                            "list": false,
                            "name": "path",
                            "placeholder": "",
                            "required": false,
                            "show": true,
                            "title_case": false,
                            "trace_as_metadata": true,
                            "type": "file",
                            "value": ""
                          },
                          "silent_errors": {
                            "advanced": true,
                            "display_name": "Silent Errors",
                            "dynamic": false,
                            "info": "If true, errors will not raise an exception.",
                            "list": false,
                            "name": "silent_errors",
                            "placeholder": "",
                            "required": false,
                            "show": true,
                            "title_case": false,
                            "trace_as_metadata": true,
                            "type": "bool",
                            "value": false
                          }
                        },
                        "lf_version": "1.0.15"
                      },
                      "type": "File"
                    },
                    "dragging": false,
                    "height": 294,
                    "id": "File-HmFrT",
                    "position": {
                      "x": -2136.139058842607,
                      "y": 394.34362259205744
                    },
                    "positionAbsolute": {
                      "x": -2136.139058842607,
                      "y": 394.34362259205744
                    },
                    "selected": true,
                    "type": "genericNode",
                    "width": 384
                  },
                  {
                    "id": "OllamaEmbeddings-tSXRh",
                    "type": "genericNode",
                    "position": {
                      "x": -1099.274159598312,
                      "y": 298.13099021293846
                    },
                    "data": {
                      "type": "OllamaEmbeddings",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "base_url": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "name": "base_url",
                            "value": "http://localhost:11434",
                            "display_name": "Ollama Base URL",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "MessageTextInput"
                          },
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "model": {
                            "trace_as_input": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "name": "model",
                            "value": "mxbai-embed-large:335m-v1-fp16",
                            "display_name": "Ollama Model",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "MessageTextInput"
                          },
                          "temperature": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "name": "temperature",
                            "value": 0.1,
                            "display_name": "Model Temperature",
                            "advanced": true,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "float",
                            "_input_type": "FloatInput"
                          }
                        },
                        "description": "Generate embeddings using Ollama models.",
                        "icon": "Ollama",
                        "base_classes": [
                          "Embeddings"
                        ],
                        "display_name": "Ollama Embeddings",
                        "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Embeddings"
                            ],
                            "selected": "Embeddings",
                            "name": "embeddings",
                            "display_name": "Embeddings",
                            "method": "build_embeddings",
                            "value": "__UNDEFINED__",
                            "cache": true
                          }
                        ],
                        "field_order": [
                          "model",
                          "base_url",
                          "temperature"
                        ],
                        "beta": false,
                        "edited": false,
                        "lf_version": "1.0.15"
                      },
                      "id": "OllamaEmbeddings-tSXRh"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 388,
                    "positionAbsolute": {
                      "x": -1099.274159598312,
                      "y": 298.13099021293846
                    },
                    "dragging": false
                  },
                  {
                    "id": "pgvector-TRQkk",
                    "type": "genericNode",
                    "position": {
                      "x": -576.9483086802642,
                      "y": 39.28261180227025
                    },
                    "data": {
                      "type": "pgvector",
                      "node": {
                        "template": {
                          "_type": "Component",
                          "embedding": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "name": "embedding",
                            "value": "",
                            "display_name": "Embedding",
                            "advanced": false,
                            "input_types": [
                              "Embeddings"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "other",
                            "_input_type": "HandleInput"
                          },
                          "ingest_data": {
                            "trace_as_metadata": true,
                            "list": true,
                            "trace_as_input": true,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "name": "ingest_data",
                            "value": "",
                            "display_name": "Ingestion Data",
                            "advanced": false,
                            "input_types": [
                              "Data"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "other",
                            "_input_type": "DataInput"
                          },
                          "code": {
                            "type": "code",
                            "required": true,
                            "placeholder": "",
                            "list": false,
                            "show": true,
                            "multiline": true,
                            "value": "from typing import List\n\n#from langchain_community.vectorstores import PGVector\nfrom langchain_postgres import PGVector\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.connection_string_parser import transform_connection_string\n\n\nclass PGVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"PGVector Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    name = \"pgvector\"\n    icon = \"PGVector\"\n\n    inputs = [\n        SecretStrInput(name=\"pg_server_url\", display_name=\"PostgreSQL Server Connection String\", required=True),\n        StrInput(name=\"collection_name\", display_name=\"Table\", required=True),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n    ]\n    \n    def validate_connection_string(self, connection_string: str) -> bool:\n        parts = connection_string.split('@')\n        if len(parts) != 2:\n            return False\n        credentials, host_port_db = parts\n        if ':' not in credentials or ':' not in host_port_db:\n            return False\n        host_port, db = host_port_db.split('/')\n        if ':' not in host_port:\n            return False\n        return True\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n    \n        if not self.validate_connection_string(self.pg_server_url):\n            raise ValueError(\"Invalid connection string format. Expected format: username:password@host:port/database\")\n    \n        # Construct the connection string directly\n        connection_string = f\"postgresql+psycopg://{self.pg_server_url}\"\n    \n        if documents:\n            pgvector = PGVector.from_documents(\n                embedding=self.embedding,\n                documents=documents,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n    \n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_pgvector()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n",
                            "fileTypes": [],
                            "file_path": "",
                            "password": false,
                            "name": "code",
                            "advanced": true,
                            "dynamic": true,
                            "info": "",
                            "load_from_db": false,
                            "title_case": false,
                            "display_name": "code"
                          },
                          "collection_name": {
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": true,
                            "placeholder": "",
                            "show": true,
                            "name": "collection_name",
                            "value": "items",
                            "display_name": "Table",
                            "advanced": false,
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "StrInput"
                          },
                          "number_of_results": {
                            "trace_as_metadata": true,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "name": "number_of_results",
                            "value": 4,
                            "display_name": "Number of Results",
                            "advanced": true,
                            "dynamic": false,
                            "info": "Number of results to return.",
                            "title_case": false,
                            "type": "int",
                            "_input_type": "IntInput"
                          },
                          "pg_server_url": {
                            "load_from_db": false,
                            "required": true,
                            "placeholder": "",
                            "show": true,
                            "name": "pg_server_url",
                            "value": "aiuser:aipassword@172.20.0.2:5432/vectordb",
                            "display_name": "PostgreSQL Server Connection String",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "password": true,
                            "type": "str",
                            "_input_type": "SecretStrInput"
                          },
                          "search_query": {
                            "trace_as_input": true,
                            "multiline": true,
                            "trace_as_metadata": true,
                            "load_from_db": false,
                            "list": false,
                            "required": false,
                            "placeholder": "",
                            "show": true,
                            "name": "search_query",
                            "value": "",
                            "display_name": "Search Query",
                            "advanced": false,
                            "input_types": [
                              "Message"
                            ],
                            "dynamic": false,
                            "info": "",
                            "title_case": false,
                            "type": "str",
                            "_input_type": "MultilineInput"
                          }
                        },
                        "description": "PGVector Vector Store with search capabilities",
                        "icon": "PGVector",
                        "base_classes": [
                          "Data",
                          "Retriever",
                          "VectorStore"
                        ],
                        "display_name": "Nicks PGVector Custom Auth",
                        "documentation": "https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                          {
                            "types": [
                              "Retriever"
                            ],
                            "selected": "Retriever",
                            "name": "base_retriever",
                            "display_name": "Retriever",
                            "method": "build_base_retriever",
                            "value": "__UNDEFINED__",
                            "cache": true
                          },
                          {
                            "types": [
                              "Data"
                            ],
                            "selected": "Data",
                            "name": "search_results",
                            "display_name": "Search Results",
                            "method": "search_documents",
                            "value": "__UNDEFINED__",
                            "cache": true
                          },
                          {
                            "types": [
                              "VectorStore"
                            ],
                            "selected": "VectorStore",
                            "name": "vector_store",
                            "display_name": "Vector Store",
                            "method": "cast_vector_store",
                            "value": "__UNDEFINED__",
                            "cache": true
                          }
                        ],
                        "field_order": [
                          "pg_server_url",
                          "collection_name",
                          "search_query",
                          "ingest_data",
                          "embedding",
                          "number_of_results",
                          "embedding"
                        ],
                        "beta": false,
                        "edited": true,
                        "lf_version": "1.0.15"
                      },
                      "id": "pgvector-TRQkk"
                    },
                    "selected": true,
                    "width": 384,
                    "height": 644,
                    "positionAbsolute": {
                      "x": -576.9483086802642,
                      "y": 39.28261180227025
                    },
                    "dragging": false
                  }
                ],
                "edges": [
                  {
                    "className": "",
                    "data": {
                      "sourceHandle": {
                        "dataType": "File",
                        "id": "File-HmFrT",
                        "name": "data",
                        "output_types": [
                          "Data"
                        ]
                      },
                      "targetHandle": {
                        "fieldName": "data_inputs",
                        "id": "SplitText-xQWvM",
                        "inputTypes": [
                          "Data"
                        ],
                        "type": "other"
                      }
                    },
                    "id": "reactflow__edge-File-HmFrT{dataType:File,id:File-HmFrT,name:data,output_types:[Data]}-SplitText-xQWvM{fieldName:data_inputs,id:SplitText-xQWvM,inputTypes:[Data],type:other}",
                    "source": "File-HmFrT",
                    "sourceHandle": "{dataType:File,id:File-HmFrT,name:data,output_types:[Data]}",
                    "target": "SplitText-xQWvM",
                    "targetHandle": "{fieldName:data_inputs,id:SplitText-xQWvM,inputTypes:[Data],type:other}"
                  },
                  {
                    "className": "",
                    "data": {
                      "sourceHandle": {
                        "dataType": "SplitText",
                        "id": "SplitText-xQWvM",
                        "name": "chunks",
                        "output_types": [
                          "Data"
                        ]
                      },
                      "targetHandle": {
                        "fieldName": "ingest_data",
                        "id": "pgvector-TRQkk",
                        "inputTypes": [
                          "Data"
                        ],
                        "type": "other"
                      }
                    },
                    "source": "SplitText-xQWvM",
                    "sourceHandle": "{dataType:SplitText,id:SplitText-xQWvM,name:chunks,output_types:[Data]}",
                    "target": "pgvector-TRQkk",
                    "targetHandle": "{fieldName:ingest_data,id:pgvector-TRQkk,inputTypes:[Data],type:other}",
                    "id": "reactflow__edge-SplitText-xQWvM{dataType:SplitText,id:SplitText-xQWvM,name:chunks,output_types:[Data]}-pgvector-TRQkk{fieldName:ingest_data,id:pgvector-TRQkk,inputTypes:[Data],type:other}"
                  },
                  {
                    "source": "OllamaEmbeddings-tSXRh",
                    "sourceHandle": "{dataType:OllamaEmbeddings,id:OllamaEmbeddings-tSXRh,name:embeddings,output_types:[Embeddings]}",
                    "target": "pgvector-TRQkk",
                    "targetHandle": "{fieldName:embedding,id:pgvector-TRQkk,inputTypes:[Embeddings],type:other}",
                    "data": {
                      "targetHandle": {
                        "fieldName": "embedding",
                        "id": "pgvector-TRQkk",
                        "inputTypes": [
                          "Embeddings"
                        ],
                        "type": "other"
                      },
                      "sourceHandle": {
                        "dataType": "OllamaEmbeddings",
                        "id": "OllamaEmbeddings-tSXRh",
                        "name": "embeddings",
                        "output_types": [
                          "Embeddings"
                        ]
                      }
                    },
                    "id": "reactflow__edge-OllamaEmbeddings-tSXRh{dataType:OllamaEmbeddings,id:OllamaEmbeddings-tSXRh,name:embeddings,output_types:[Embeddings]}-pgvector-TRQkk{fieldName:embedding,id:pgvector-TRQkk,inputTypes:[Embeddings],type:other}"
                  }
                ],
                "viewport": {
                  "zoom": 1,
                  "x": 0,
                  "y": 0
                }
              },
              "is_component": false,
              "name": "Optimistic Pasteur",
              "description": "",
              "id": "nisQj"
            },
            "outputs": [
              {
                "types": [
                  "Retriever"
                ],
                "selected": "Retriever",
                "name": "pgvector-TRQkk_base_retriever",
                "display_name": "Retriever",
                "method": "build_base_retriever",
                "value": "__UNDEFINED__",
                "cache": true,
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "name": "base_retriever",
                  "nodeDisplayName": "Nicks PGVector Custom Auth"
                }
              },
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "pgvector-TRQkk_search_results",
                "display_name": "Search Results",
                "method": "search_documents",
                "value": "__UNDEFINED__",
                "cache": true,
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "name": "search_results",
                  "nodeDisplayName": "Nicks PGVector Custom Auth"
                }
              },
              {
                "types": [
                  "VectorStore"
                ],
                "selected": "VectorStore",
                "name": "pgvector-TRQkk_vector_store",
                "display_name": "Vector Store",
                "method": "cast_vector_store",
                "value": "__UNDEFINED__",
                "cache": true,
                "proxy": {
                  "id": "pgvector-TRQkk",
                  "name": "vector_store",
                  "nodeDisplayName": "Nicks PGVector Custom Auth"
                }
              }
            ]
          }
        },
        "id": "groupComponent-NSkEw",
        "position": {
          "x": -441.4072095841216,
          "y": -71.12875305302705
        },
        "type": "genericNode",
        "width": 384,
        "height": 798,
        "selected": false,
        "positionAbsolute": {
          "x": -441.4072095841216,
          "y": -71.12875305302705
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-4Lvig",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "context",
            "id": "Prompt-z8nbL",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-ParseData-4Lvig{dataType:ParseData,id:ParseData-4Lvig,name:text,output_types:[Message]}-Prompt-z8nbL{fieldName:context,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}",
        "source": "ParseData-4Lvig",
        "sourceHandle": "{dataType:ParseData,id:ParseData-4Lvig,name:text,output_types:[Message]}",
        "target": "Prompt-z8nbL",
        "targetHandle": "{fieldName:context,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}"
      },
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-sgEOq",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-z8nbL",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-ChatInput-sgEOq{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}-Prompt-z8nbL{fieldName:question,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}",
        "source": "ChatInput-sgEOq",
        "sourceHandle": "{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}",
        "target": "Prompt-z8nbL",
        "targetHandle": "{fieldName:question,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}"
      },
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-sgEOq",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "pgvector-8NuFT",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "source": "ChatInput-sgEOq",
        "sourceHandle": "{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}",
        "target": "pgvector-8NuFT",
        "targetHandle": "{fieldName:search_query,id:pgvector-8NuFT,inputTypes:[Message],type:str}",
        "id": "reactflow__edge-ChatInput-sgEOq{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}-pgvector-8NuFT{fieldName:search_query,id:pgvector-8NuFT,inputTypes:[Message],type:str}"
      },
      {
        "source": "pgvector-8NuFT",
        "sourceHandle": "{dataType:pgvector,id:pgvector-8NuFT,name:search_results,output_types:[Data]}",
        "target": "ParseData-4Lvig",
        "targetHandle": "{fieldName:data,id:ParseData-4Lvig,inputTypes:[Data],type:other}",
        "data": {
          "targetHandle": {
            "fieldName": "data",
            "id": "ParseData-4Lvig",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "pgvector",
            "id": "pgvector-8NuFT",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-pgvector-8NuFT{dataType:pgvector,id:pgvector-8NuFT,name:search_results,output_types:[Data]}-ParseData-4Lvig{fieldName:data,id:ParseData-4Lvig,inputTypes:[Data],type:other}",
        "className": ""
      },
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "OllamaEmbeddings",
            "id": "OllamaEmbeddings-zW4Ul",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "pgvector-8NuFT",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "source": "OllamaEmbeddings-zW4Ul",
        "sourceHandle": "{dataType:OllamaEmbeddings,id:OllamaEmbeddings-zW4Ul,name:embeddings,output_types:[Embeddings]}",
        "target": "pgvector-8NuFT",
        "targetHandle": "{fieldName:embedding,id:pgvector-8NuFT,inputTypes:[Embeddings],type:other}",
        "id": "reactflow__edge-OllamaEmbeddings-zW4Ul{dataType:OllamaEmbeddings,id:OllamaEmbeddings-zW4Ul,name:embeddings,output_types:[Embeddings]}-pgvector-8NuFT{fieldName:embedding,id:pgvector-8NuFT,inputTypes:[Embeddings],type:other}"
      },
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-z8nbL",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OllamaModel-jpiyc",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "source": "Prompt-z8nbL",
        "sourceHandle": "{dataType:Prompt,id:Prompt-z8nbL,name:prompt,output_types:[Message]}",
        "target": "OllamaModel-jpiyc",
        "targetHandle": "{fieldName:input_value,id:OllamaModel-jpiyc,inputTypes:[Message],type:str}",
        "id": "reactflow__edge-Prompt-z8nbL{dataType:Prompt,id:Prompt-z8nbL,name:prompt,output_types:[Message]}-OllamaModel-jpiyc{fieldName:input_value,id:OllamaModel-jpiyc,inputTypes:[Message],type:str}"
      },
      {
        "source": "OllamaModel-jpiyc",
        "sourceHandle": "{dataType:OllamaModel,id:OllamaModel-jpiyc,name:text_output,output_types:[Message]}",
        "target": "ChatOutput-dMHYm",
        "targetHandle": "{fieldName:input_value,id:ChatOutput-dMHYm,inputTypes:[Message],type:str}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-dMHYm",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "OllamaModel",
            "id": "OllamaModel-jpiyc",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-OllamaModel-jpiyc{dataType:OllamaModel,id:OllamaModel-jpiyc,name:text_output,output_types:[Message]}-ChatOutput-dMHYm{fieldName:input_value,id:ChatOutput-dMHYm,inputTypes:[Message],type:str}",
        "className": ""
      }
    ],
    "viewport": {
      "x": 553.3519296464808,
      "y": 157.29515997628755,
      "zoom": 0.3841072402165912
    }
  },
  "metadata": {
    "ChatInput": {
      "count": 1
    },
    "ParseData": {
      "count": 1
    },
    "Prompt": {
      "count": 1
    },
    "ChatOutput": {
      "count": 1
    },
    "pgvector": {
      "count": 1
    },
    "OllamaEmbeddings": {
      "count": 1
    },
    "OllamaModel": {
      "count": 1
    },
    "XMLIngestComponent": {
      "count": 3
    },
    "CustomComponent": {
      "count": 1
    },
    "groupComponent": {
      "count": 1
    },
    "total": 12
  },
  "original": {
    "id": "dc23f2f7-cda4-4661-b3e1-883e339bc952",
    "name": "Nick TestData Vector Store RAG",
    "description": "Visit https://docs.langflow.org/tutorials/rag-with-astradb for a detailed guide of this project.\nThis project give you both Ingestion and RAG in a single file. You'll need to visit https://astra.datastax.com/ to create an Astra DB instance, your Token and grab an API Endpoint.\nRunning this project requires you to add a file in the Files component, then define a Collection Name and click on the Play icon on the Astra DB component. \n\nAfter the ingestion ends you are ready to click on the Run button at the lower left corner and start asking questions about your data.",
    "is_component": false,
    "liked_by_count": "0",
    "downloads_count": "0",
    "metadata": {
      "ChatInput": {
        "count": 1
      },
      "ParseData": {
        "count": 1
      },
      "Prompt": {
        "count": 1
      },
      "ChatOutput": {
        "count": 1
      },
      "pgvector": {
        "count": 1
      },
      "OllamaEmbeddings": {
        "count": 1
      },
      "OllamaModel": {
        "count": 1
      },
      "XMLIngestComponent": {
        "count": 3
      },
      "CustomComponent": {
        "count": 1
      },
      "groupComponent": {
        "count": 1
      },
      "total": 12
    },
    "last_tested_version": "1.0.15",
    "private": true,
    "data": {
      "nodes": [
        {
          "data": {
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "id": "ChatInput-sgEOq",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Get chat inputs from the Playground.",
              "display_name": "Chat Input",
              "documentation": "",
              "edited": false,
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "files"
              ],
              "frozen": false,
              "icon": "ChatInput",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Message",
                  "method": "message_response",
                  "name": "message",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                },
                "files": {
                  "advanced": true,
                  "display_name": "Files",
                  "dynamic": false,
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx",
                    "jpg",
                    "jpeg",
                    "png",
                    "bmp",
                    "image"
                  ],
                  "file_path": "",
                  "info": "Files to be sent with the message.",
                  "list": true,
                  "name": "files",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "file",
                  "value": ""
                },
                "input_value": {
                  "advanced": false,
                  "display_name": "Text",
                  "dynamic": false,
                  "info": "Message to be passed as input.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "input_value",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "if the first symbol in a reel i s consider to be in position 0 in a 0-based index, then list the index positions of each SCATTER symbol"
                },
                "sender": {
                  "advanced": true,
                  "display_name": "Sender Type",
                  "dynamic": false,
                  "info": "Type of sender.",
                  "name": "sender",
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "User"
                },
                "sender_name": {
                  "advanced": true,
                  "display_name": "Sender Name",
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "sender_name",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "User"
                },
                "session_id": {
                  "advanced": true,
                  "display_name": "Session ID",
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "session_id",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "should_store_message": {
                  "advanced": true,
                  "display_name": "Store Messages",
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "list": false,
                  "name": "should_store_message",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              },
              "lf_version": "1.0.15"
            },
            "type": "ChatInput"
          },
          "dragging": false,
          "height": 294,
          "id": "ChatInput-sgEOq",
          "position": {
            "x": 596.5218129363991,
            "y": 248.5402160000408
          },
          "positionAbsolute": {
            "x": 596.5218129363991,
            "y": 248.5402160000408
          },
          "selected": true,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Convert Data into plain text following a specified template.",
            "display_name": "Parse Data",
            "id": "ParseData-4Lvig",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Convert Data into plain text following a specified template.",
              "display_name": "Parse Data",
              "documentation": "",
              "edited": false,
              "field_order": [
                "data",
                "template",
                "sep"
              ],
              "frozen": false,
              "icon": "braces",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Text",
                  "method": "parse_data",
                  "name": "text",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"
                },
                "data": {
                  "advanced": false,
                  "display_name": "Data",
                  "dynamic": false,
                  "info": "The data to convert to text.",
                  "input_types": [
                    "Data"
                  ],
                  "list": false,
                  "name": "data",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "sep": {
                  "advanced": true,
                  "display_name": "Separator",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "name": "sep",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "\n"
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "{text}"
                }
              },
              "lf_version": "1.0.15"
            },
            "type": "ParseData"
          },
          "dragging": false,
          "height": 370,
          "id": "ParseData-4Lvig",
          "position": {
            "x": 1738.7829329736521,
            "y": 1361.5027685828854
          },
          "positionAbsolute": {
            "x": 1738.7829329736521,
            "y": 1361.5027685828854
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "id": "Prompt-z8nbL",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {
                "template": [
                  "context",
                  "question"
                ]
              },
              "description": "Create a prompt template with dynamic variables.",
              "display_name": "Prompt",
              "documentation": "",
              "edited": false,
              "field_order": [
                "template"
              ],
              "frozen": false,
              "icon": "prompts",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "name": "prompt",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
                },
                "context": {
                  "advanced": false,
                  "display_name": "context",
                  "dynamic": false,
                  "field_type": "str",
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "context",
                  "password": false,
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "type": "str",
                  "value": ""
                },
                "question": {
                  "advanced": false,
                  "display_name": "question",
                  "dynamic": false,
                  "field_type": "str",
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "question",
                  "password": false,
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "type": "str",
                  "value": ""
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "prompt",
                  "value": "{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nQuestion: {question}\n\nAnswer: "
                }
              },
              "lf_version": "1.0.15"
            },
            "type": "Prompt"
          },
          "dragging": false,
          "height": 494,
          "id": "Prompt-z8nbL",
          "position": {
            "x": 2195.228201521283,
            "y": 694.83316377936
          },
          "positionAbsolute": {
            "x": 2195.228201521283,
            "y": 694.83316377936
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "id": "ChatOutput-dMHYm",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Display a chat message in the Playground.",
              "display_name": "Chat Output",
              "documentation": "",
              "edited": false,
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "frozen": false,
              "icon": "ChatOutput",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Message",
                  "method": "message_response",
                  "name": "message",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                },
                "data_template": {
                  "advanced": true,
                  "display_name": "Data Template",
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "data_template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "{text}"
                },
                "input_value": {
                  "advanced": false,
                  "display_name": "Text",
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "input_value",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "sender": {
                  "advanced": true,
                  "display_name": "Sender Type",
                  "dynamic": false,
                  "info": "Type of sender.",
                  "name": "sender",
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Machine"
                },
                "sender_name": {
                  "advanced": true,
                  "display_name": "Sender Name",
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "sender_name",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "AI"
                },
                "session_id": {
                  "advanced": true,
                  "display_name": "Session ID",
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "session_id",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "should_store_message": {
                  "advanced": true,
                  "display_name": "Store Messages",
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "list": false,
                  "name": "should_store_message",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              },
              "lf_version": "1.0.15"
            },
            "type": "ChatOutput"
          },
          "dragging": false,
          "height": 294,
          "id": "ChatOutput-dMHYm",
          "position": {
            "x": 3183.728287147309,
            "y": 902.6677310123966
          },
          "positionAbsolute": {
            "x": 3183.728287147309,
            "y": 902.6677310123966
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "id": "pgvector-8NuFT",
          "type": "genericNode",
          "position": {
            "x": 1249.6570639640581,
            "y": 1075.6437991896562
          },
          "data": {
            "type": "pgvector",
            "node": {
              "template": {
                "_type": "Component",
                "embedding": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "embedding",
                  "value": "",
                  "display_name": "Embedding",
                  "advanced": false,
                  "input_types": [
                    "Embeddings"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "ingest_data": {
                  "trace_as_metadata": true,
                  "list": true,
                  "trace_as_input": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "ingest_data",
                  "value": "",
                  "display_name": "Ingestion Data",
                  "advanced": false,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "DataInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import List\n\n#from langchain_community.vectorstores import PGVector\nfrom langchain_postgres import PGVector\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.connection_string_parser import transform_connection_string\n\n\nclass PGVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"PGVector Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    name = \"pgvector\"\n    icon = \"PGVector\"\n\n    inputs = [\n        SecretStrInput(name=\"pg_server_url\", display_name=\"PostgreSQL Server Connection String\", required=True),\n        StrInput(name=\"collection_name\", display_name=\"Table\", required=True),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n    ]\n    \n    def validate_connection_string(self, connection_string: str) -> bool:\n        parts = connection_string.split('@')\n        if len(parts) != 2:\n            return False\n        credentials, host_port_db = parts\n        if ':' not in credentials or ':' not in host_port_db:\n            return False\n        host_port, db = host_port_db.split('/')\n        if ':' not in host_port:\n            return False\n        return True\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n    \n        if not self.validate_connection_string(self.pg_server_url):\n            raise ValueError(\"Invalid connection string format. Expected format: username:password@host:port/database\")\n    \n        # Construct the connection string directly\n        connection_string = f\"postgresql+psycopg://{self.pg_server_url}\"\n    \n        if documents:\n            pgvector = PGVector.from_documents(\n                embedding=self.embedding,\n                documents=documents,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n    \n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_pgvector()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "collection_name": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "collection_name",
                  "value": "items",
                  "display_name": "Table",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "number_of_results": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "number_of_results",
                  "value": 4,
                  "display_name": "Number of Results",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of results to return.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "pg_server_url": {
                  "load_from_db": true,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "pg_server_url",
                  "value": "",
                  "display_name": "PostgreSQL Server Connection String",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "search_query": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "search_query",
                  "value": "",
                  "display_name": "Search Query",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "PGVector Vector Store with search capabilities",
              "icon": "PGVector",
              "base_classes": [
                "Data",
                "Retriever",
                "VectorStore"
              ],
              "display_name": "Nicks PGVector Custom Auth",
              "documentation": "https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Retriever"
                  ],
                  "selected": "Retriever",
                  "name": "base_retriever",
                  "display_name": "Retriever",
                  "method": "build_base_retriever",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "search_results",
                  "display_name": "Search Results",
                  "method": "search_documents",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "VectorStore"
                  ],
                  "selected": "VectorStore",
                  "name": "vector_store",
                  "display_name": "Vector Store",
                  "method": "cast_vector_store",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "pg_server_url",
                "collection_name",
                "search_query",
                "ingest_data",
                "embedding",
                "number_of_results",
                "embedding"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.15"
            },
            "id": "pgvector-8NuFT"
          },
          "selected": false,
          "width": 384,
          "height": 644,
          "positionAbsolute": {
            "x": 1249.6570639640581,
            "y": 1075.6437991896562
          },
          "dragging": false
        },
        {
          "id": "OllamaEmbeddings-zW4Ul",
          "type": "genericNode",
          "position": {
            "x": 643.578907492697,
            "y": 783.5843122606201
          },
          "data": {
            "type": "OllamaEmbeddings",
            "node": {
              "template": {
                "_type": "Component",
                "base_url": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "base_url",
                  "value": "http://localhost:11434",
                  "display_name": "Ollama Base URL",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "model": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model",
                  "value": "mxbai-embed-large:335m-v1-fp16",
                  "display_name": "Ollama Model",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.1,
                  "display_name": "Model Temperature",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                }
              },
              "description": "Generate embeddings using Ollama models.",
              "icon": "Ollama",
              "base_classes": [
                "Embeddings"
              ],
              "display_name": "Ollama Embeddings",
              "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Embeddings"
                  ],
                  "selected": "Embeddings",
                  "name": "embeddings",
                  "display_name": "Embeddings",
                  "method": "build_embeddings",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "model",
                "base_url",
                "temperature"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.15"
            },
            "id": "OllamaEmbeddings-zW4Ul"
          },
          "selected": false,
          "width": 384,
          "height": 388,
          "positionAbsolute": {
            "x": 643.578907492697,
            "y": 783.5843122606201
          },
          "dragging": false
        },
        {
          "id": "OllamaModel-jpiyc",
          "type": "genericNode",
          "position": {
            "x": 2713.7320148852878,
            "y": 517.4838664314314
          },
          "data": {
            "type": "OllamaModel",
            "node": {
              "template": {
                "_type": "Component",
                "base_url": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "base_url",
                  "value": "http://localhost:11434",
                  "display_name": "Base URL",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "format": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "format",
                  "value": "",
                  "display_name": "Format",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Specify the format of the output (e.g., json).",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageInput"
                },
                "metadata": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "metadata",
                  "value": {},
                  "display_name": "Metadata",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Metadata to add to the run trace.",
                  "title_case": false,
                  "type": "dict",
                  "_input_type": "DictInput"
                },
                "mirostat": {
                  "trace_as_metadata": true,
                  "options": [
                    "Disabled",
                    "Mirostat",
                    "Mirostat 2.0"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "mirostat",
                  "value": "Disabled",
                  "display_name": "Mirostat",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                  "real_time_refresh": true,
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "mirostat_eta": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "mirostat_eta",
                  "value": "",
                  "display_name": "Mirostat Eta",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "mirostat_tau": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "mirostat_tau",
                  "value": "",
                  "display_name": "Mirostat Tau",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "mxbai-embed-large:335m-v1-fp16",
                    "nomic-embed-text:latest",
                    "deepseek-coder-v2:16b-lite-instruct-fp16",
                    "llama3-groq-tool-use:latest",
                    "gemma-2b-it-q8_0.gguf:latest",
                    "dolphin-mistral:7b-v2.8-fp16",
                    "mistral-nemo:12b-instruct-2407-fp16",
                    "llama3.1:8b-instruct-fp16",
                    "codestral:22b-v0.1-q8_0",
                    "llama3:8b"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "llama3.1:8b-instruct-fp16",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Refer to https://ollama.ai/library for more models.",
                  "refresh_button": true,
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "num_ctx": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "num_ctx",
                  "value": "",
                  "display_name": "Context Window Size",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Size of the context window for generating tokens. (Default: 2048)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "num_gpu": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "num_gpu",
                  "value": "",
                  "display_name": "Number of GPUs",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "num_thread": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "num_thread",
                  "value": "",
                  "display_name": "Number of Threads",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "repeat_last_n": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "repeat_last_n",
                  "value": "",
                  "display_name": "Repeat Last N",
                  "advanced": true,
                  "dynamic": false,
                  "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "repeat_penalty": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "repeat_penalty",
                  "value": "",
                  "display_name": "Repeat Penalty",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "stop_tokens": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "stop_tokens",
                  "value": "",
                  "display_name": "Stop Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "stream",
                  "value": false,
                  "display_name": "Stream",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                },
                "system": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system",
                  "value": "",
                  "display_name": "System",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System to use for generating text.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "system_message": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system_message",
                  "value": "",
                  "display_name": "System Message",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "tags": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "tags",
                  "value": "",
                  "display_name": "Tags",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Comma-separated list of tags to add to the run trace.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.2,
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Controls the creativity of model responses.",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "template": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "template",
                  "value": "",
                  "display_name": "Template",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Template to use for generating text.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "tfs_z": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "tfs_z",
                  "value": "",
                  "display_name": "TFS Z",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Tail free sampling value. (Default: 1)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "timeout": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "timeout",
                  "value": "",
                  "display_name": "Timeout",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Timeout for the request stream.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "top_k": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_k",
                  "value": "",
                  "display_name": "Top K",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Limits token selection to top K. (Default: 40)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "top_p": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_p",
                  "value": "",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Works together with top-k. (Default: 0.9)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "verbose": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "verbose",
                  "value": false,
                  "display_name": "Verbose",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Whether to print out response text.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Generate text using Ollama Local LLMs.",
              "icon": "Ollama",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "Ollama",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "system_message",
                "stream",
                "base_url",
                "model_name",
                "temperature",
                "format",
                "metadata",
                "mirostat",
                "mirostat_eta",
                "mirostat_tau",
                "num_ctx",
                "num_gpu",
                "num_thread",
                "repeat_last_n",
                "repeat_penalty",
                "tfs_z",
                "timeout",
                "top_k",
                "top_p",
                "verbose",
                "tags",
                "stop_tokens",
                "system",
                "template"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.15"
            },
            "id": "OllamaModel-jpiyc"
          },
          "selected": false,
          "width": 384,
          "height": 673,
          "positionAbsolute": {
            "x": 2713.7320148852878,
            "y": 517.4838664314314
          },
          "dragging": false
        },
        {
          "id": "XMLIngestComponent-v0Vke",
          "type": "genericNode",
          "position": {
            "x": -438.47480229619,
            "y": 1540.4230200322936
          },
          "data": {
            "type": "XMLIngestComponent",
            "node": {
              "template": {
                "_type": "Component",
                "xml_file": {
                  "trace_as_metadata": true,
                  "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                  "fileTypes": [
                    "xml"
                  ],
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "xml_file",
                  "value": "",
                  "display_name": "XML File",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Upload an XML file containing game data.",
                  "title_case": false,
                  "type": "file",
                  "_input_type": "FileInput",
                  "load_from_db": false
                },
                "base_url": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "base_url",
                  "value": "http://localhost:11434",
                  "display_name": "Base URL",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import xml.etree.ElementTree as ET\nimport psycopg2\nimport httpx\nfrom axiestudio.io import MessageTextInput\nfrom axiestudio.custom import Component\nfrom axiestudio.io import FileInput, StrInput, FloatInput, Output, DropdownInput\nfrom axiestudio.schema import Data\nfrom langchain_community.embeddings import OllamaEmbeddings\nimport logging\n\nclass XMLIngestComponent(Component):\n    display_name = \"XML Ingest Component\"\n    description = \"Ingests XML game data and inserts it into the PostgreSQL database with embeddings.\"\n    icon = \"file-code\"\n    name = \"XMLIngestComponent\"\n\n    inputs = [\n        FileInput(\n            name=\"xml_file\",\n            display_name=\"XML File\",\n            file_types=[\"xml\"],\n            info=\"Upload an XML file containing game data.\",\n        ),\n        MessageTextInput(\n            name=\"db_connection_string\",\n            display_name=\"DB Connection String\",\n            info=\"Enter the PostgreSQL connection string or connect from another component.\",\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"mxbai-embed-large:335m-v1-fp16\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n            info=\"Temperature for the Ollama model (if applicable).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Ingest Result\", name=\"ingest_result\", method=\"ingest_xml\"),\n    ]\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n            return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    def update_build_config(self, build_config: dict, field_value: any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n        return build_config\n\n    def ingest_xml(self) -> Data:\n        if not self.xml_file:\n            return Data(value=\"Error: No XML file uploaded.\")\n\n        try:\n            with open(self.xml_file, 'r') as file:\n                xml_content = file.read()\n\n            embeddings_model = OllamaEmbeddings(\n                model=self.model_name,\n                base_url=self.base_url,\n                temperature=self.temperature\n            )\n\n            with psycopg2.connect(self.db_connection_string) as conn:\n                agent = self.XMLIngestAgent(conn, embeddings_model)\n                agent.parse_xml_content(xml_content)\n            result = \"XML data ingested successfully\"\n        except Exception as e:\n            result = f\"Error: {str(e)}\"\n\n        return Data(value=result)\n\n    class XMLIngestAgent:\n        def __init__(self, db_connection, embeddings_model):\n            self.db_connection = db_connection\n            self.embeddings_model = embeddings_model\n            logging.basicConfig(level=logging.INFO)\n            self.logger = logging.getLogger(__name__)\n\n        def parse_xml_content(self, xml_content):\n            root = ET.fromstring(xml_content)\n            \n            module_id = root.get('moduleId')\n            rtp = float(root.get('rtp', 0))\n            hit_rate = float(root.get('hitRate', 0))\n            std_deviation = float(root.get('stdDeviation', 0))\n            \n            bet_model = root.find('BetModel')\n            paytable_type = bet_model.get('paytableType')\n            bet_multiplier = int(bet_model.get('betMultiplier', 1))\n        \n            game_text = f\"Game {module_id} with paytable type {paytable_type}, bet multiplier {bet_multiplier}, RTP {rtp}, hit rate {hit_rate}, and standard deviation {std_deviation}\"\n            game_embedding = self.embeddings_model.embed_query(game_text)\n            game_id = self.insert_or_get_game(module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, game_embedding)\n        \n            # Parse and insert symbols\n            for symbol in root.findall('.//Symbol'):\n                name = symbol.get('name')\n                symbol_id = symbol.get('id')\n                if name and symbol_id:\n                    is_wild = symbol.get('isWild') == 'true'\n                    can_wild_substitute = symbol.get('canWildSubstitute') == 'true'\n                    \n                    global_symbol_id = self.insert_or_get_global_symbol(name)\n                    self.insert_game_symbol(game_id, global_symbol_id, int(symbol_id), is_wild, can_wild_substitute)\n        \n            for reel_set in root.findall('.//ReelSet'):\n                reel_set_name = reel_set.get('name')\n                default_positions = [int(reel.get('defaultPosition', 0)) for reel in reel_set.findall('Reel')]\n                reel_set_text = f\"Reel set {reel_set_name} for game {module_id}\"\n                reel_set_embedding = self.embeddings_model.embed_query(reel_set_text)\n                reel_set_id = self.insert_or_get_reel_set(game_id, reel_set_name, default_positions, reel_set_embedding)\n        \n                for reel_index, reel in enumerate(reel_set.findall('Reel')):\n                    symbols = reel.get('symbols', '').split(',')\n                    for symbol_position, symbol_id in enumerate(symbols):\n                        if symbol_id and symbol_id.strip():\n                            symbol_name = self.get_symbol_name(game_id, int(symbol_id))\n                            if symbol_name:\n                                self.insert_reel_symbol(game_id, reel_set_id, reel_index, symbol_position, int(symbol_id), symbol_name)\n                            else:\n                                self.logger.warning(f\"Symbol with ID {symbol_id} not found for game {game_id}\")\n\n        def insert_or_get_game(self, module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Games (module_id, bet_model_paytable_type, bet_model_multiplier, rtp, hit_rate, std_deviation, embedding)\n                VALUES (%s, %s, %s, %s, %s, %s, %s)\n                ON CONFLICT (module_id) DO UPDATE \n                SET bet_model_paytable_type = EXCLUDED.bet_model_paytable_type,\n                    bet_model_multiplier = EXCLUDED.bet_model_multiplier,\n                    rtp = EXCLUDED.rtp,\n                    hit_rate = EXCLUDED.hit_rate,\n                    std_deviation = EXCLUDED.std_deviation,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding))\n            game_id = cursor.fetchone()[0]\n            self.db_connection.commit()\n            return game_id\n\n        def insert_or_get_global_symbol(self, name):\n            cursor = self.db_connection.cursor()\n            symbol_text = f\"Symbol {name}\"\n            symbol_embedding = self.embeddings_model.embed_query(symbol_text)\n            cursor.execute(\"\"\"\n                INSERT INTO GlobalSymbols (name, embedding)\n                VALUES (%s, %s)\n                ON CONFLICT (name) DO UPDATE \n                SET embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (name, symbol_embedding))\n            global_symbol_id = cursor.fetchone()[0]\n            self.db_connection.commit()\n            return global_symbol_id\n\n        def insert_game_symbol(self, game_id, global_symbol_id, game_specific_symbol_id, is_wild, can_wild_substitute):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO GameSymbols (game_id, global_symbol_id, game_specific_symbol_id, is_wild, can_wild_substitute)\n                VALUES (%s, %s, %s, %s, %s)\n                ON CONFLICT (game_id, game_specific_symbol_id) \n                DO UPDATE SET \n                    global_symbol_id = EXCLUDED.global_symbol_id,\n                    is_wild = EXCLUDED.is_wild,\n                    can_wild_substitute = EXCLUDED.can_wild_substitute\n            \"\"\", (game_id, global_symbol_id, game_specific_symbol_id, is_wild, can_wild_substitute))\n            self.db_connection.commit()\n\n        def insert_or_get_reel_set(self, game_id, reel_set_name, default_positions, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO ReelSets (game_id, name, default_positions, embedding)\n                VALUES (%s, %s, %s, %s)\n                ON CONFLICT (game_id, name) DO UPDATE \n                SET default_positions = EXCLUDED.default_positions,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (game_id, reel_set_name, default_positions, embedding))\n            reel_set_id = cursor.fetchone()[0]\n            self.db_connection.commit()\n            return reel_set_id\n\n        def get_symbol_name(self, game_id, game_specific_symbol_id):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                SELECT gs.name \n                FROM GameSymbols gs\n                JOIN GlobalSymbols gls ON gs.global_symbol_id = gls.id\n                WHERE gs.game_id = %s AND gs.game_specific_symbol_id = %s\n            \"\"\", (game_id, game_specific_symbol_id))\n            result = cursor.fetchone()\n            return result[0] if result else None\n\n        def insert_reel_symbol(self, game_id, reel_set_id, reel_index, symbol_position, game_specific_symbol_id, symbol_name):\n            cursor = self.db_connection.cursor()\n            try:\n                cursor.execute(\"\"\"\n                    INSERT INTO Reels (game_id, reel_set_id, reel_index, symbol_reel_position, game_symbol_id, symbol_name)\n                    VALUES (%s, %s, %s, %s, %s, %s)\n                    ON CONFLICT (game_id, reel_set_id, reel_index, symbol_reel_position) \n                    DO UPDATE SET game_symbol_id = EXCLUDED.game_symbol_id, symbol_name = EXCLUDED.symbol_name\n                \"\"\", (game_id, reel_set_id, reel_index, symbol_position, game_specific_symbol_id, symbol_name))\n                self.db_connection.commit()\n            except Exception as e:\n                self.logger.error(f\"Error inserting reel symbol: {e}\")\n                self.logger.error(f\"game_id: {game_id}, reel_set_id: {reel_set_id}, reel_index: {reel_index}, symbol_position: {symbol_position}, game_specific_symbol_id: {game_specific_symbol_id}, symbol_name: {symbol_name}\")\n                raise\n\n# Register the custom component\nXMLIngestComponent()",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "db_connection_string": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "db_connection_string",
                  "value": "NormalDBConnectionString",
                  "display_name": "DB Connection String",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Enter the PostgreSQL connection string or connect from another component.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "mxbai-embed-large:335m-v1-fp16",
                    "nomic-embed-text:latest",
                    "deepseek-coder-v2:16b-lite-instruct-fp16",
                    "llama3-groq-tool-use:latest",
                    "gemma-2b-it-q8_0.gguf:latest",
                    "dolphin-mistral:7b-v2.8-fp16",
                    "mistral-nemo:12b-instruct-2407-fp16",
                    "llama3.1:8b-instruct-fp16",
                    "codestral:22b-v0.1-q8_0",
                    "llama3:8b"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "mxbai-embed-large:335m-v1-fp16",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Refer to https://ollama.ai/library for more models.",
                  "refresh_button": true,
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.1,
                  "display_name": "Model Temperature",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Temperature for the Ollama model (if applicable).",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                }
              },
              "description": "Ingests XML game data and inserts it into the PostgreSQL database with embeddings.",
              "icon": "file-code",
              "base_classes": [
                "Data"
              ],
              "display_name": "Fucked v4 DB XMLIngestNodeEmbeddings",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "ingest_result",
                  "display_name": "Ingest Result",
                  "method": "ingest_xml",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "xml_file",
                "db_connection_string",
                "base_url",
                "model_name",
                "temperature"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.15"
            },
            "id": "XMLIngestComponent-v0Vke"
          },
          "selected": false,
          "width": 384,
          "height": 580,
          "positionAbsolute": {
            "x": -438.47480229619,
            "y": 1540.4230200322936
          },
          "dragging": false
        },
        {
          "id": "CustomComponent-B0UyY",
          "type": "genericNode",
          "position": {
            "x": 68.23695411058566,
            "y": -64.06857640368476
          },
          "data": {
            "type": "slot_game_chatbot",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.field_typing import NestedDict\nfrom axiestudio.io import StrInput, MessageTextInput, Output\nfrom axiestudio.schema import Data\nfrom langchain_community.vectorstores import PGVector\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.agents import create_sql_agent\nfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.llms import OpenAI\nclass SlotGameChatbotComponent(Component):\n    display_name = \"Slot Game Chatbot\"\n    description = \"A chatbot for querying slot game data using hybrid search (vector + SQL).\"\n    icon = \"chatbot\"\n    name = \"slot_game_chatbot\"\n    inputs = [\n        StrInput(name=\"db_connection_string\", display_name=\"Database Connection String\", value=\"\"),\n        MessageTextInput(name=\"user_query\", display_name=\"User Query\", value=\"\"),\n    ]\n    outputs = [\n        Output(display_name=\"Chatbot Response\", name=\"response\", method=\"process_query\"),\n    ]\n    def hybrid_search(self, query_text, top_k=5):\n        # Vector search\n        query_embedding = self.embeddings_model.embed_query(query_text)\n        vector_store = PGVector(connection_string=self.db_connection_string, embedding_function=self.embeddings_model)\n        vector_results = vector_store.similarity_search_with_score(query_embedding, k=top_k)\n        # SQL search\n        db = SQLDatabase.from_uri(self.db_connection_string)\n        toolkit = SQLDatabaseToolkit(db=db, llm=self.llm)\n        agent = create_sql_agent(llm=self.llm, toolkit=toolkit, verbose=True)\n        sql_result = agent.run(query_text)\n        return vector_results, sql_result\n    def generate_response(self, query_text, vector_results, sql_result):\n        context = f\"\"\"\n        Query: {query_text}\n        Vector Search Results: {vector_results}\n        SQL Query Results: {sql_result}\n        Based on the above information, provide a detailed response to the user's query.\n        If applicable, include the reel state position sequence and the corresponding symbols.\n        Also, if possible, generate a TestData JSON configuration based on the results.\n        \"\"\"\n        response = self.llm.generate(context)\n        return response\n    def process_query(self) -> Data:\n        try:\n            # In a real scenario, you'd initialize these properly\n            self.embeddings_model = OpenAI()  # placeholder\n            self.llm = OpenAI()  # placeholder\n            vector_results, sql_result = self.hybrid_search(self.user_query)\n            response = self.generate_response(self.user_query, vector_results, sql_result)\n\n            return Data(value=response)\n        except Exception as e:\n            return Data(value=f\"Error processing query: {str(e)}\")\n    def generate_testdata_json(self, module_id, reel_set, positions):\n        testdata = {\n            \"testdata\": {\n                \"moduleId\": module_id,\n                \"clientId\": \"[ClientID]\",\n                \"productId\": \"[ProductID]\",\n                \"username\": \"[UserName]\",\n                \"behaviourName\": \"[BehaviourName]\",\n                \"guid\": \"[GUID]\",\n                \"options\": {\n                    f\"{reel_set}\": reel_set,\n                }\n            }\n        }\n        for i, pos in enumerate(positions):\n            testdata[\"testdata\"][\"options\"][f\"{reel_set}${i}\"] = str(pos)\n        return testdata\n# Register the custom component\nSlotGameChatbotComponent()",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "db_connection_string": {
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "db_connection_string",
                  "value": "NormalDBConnectionString",
                  "display_name": "Database Connection String",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "user_query": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "user_query",
                  "value": "How many BONUS Symbols are in the 1st reel of the BaseGame.Type2 Reelset?",
                  "display_name": "User Query",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                }
              },
              "description": "A chatbot for querying slot game data using hybrid search (vector + SQL).",
              "icon": "chatbot",
              "base_classes": [
                "Data"
              ],
              "display_name": "SpecialSlotChatBot",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "response",
                  "display_name": "Chatbot Response",
                  "method": "process_query",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "db_connection_string",
                "user_query"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.15"
            },
            "id": "CustomComponent-B0UyY"
          },
          "selected": false,
          "width": 384,
          "height": 408,
          "positionAbsolute": {
            "x": 68.23695411058566,
            "y": -64.06857640368476
          },
          "dragging": false
        },
        {
          "id": "XMLIngestComponent-pNr7W",
          "type": "genericNode",
          "position": {
            "x": 100.07916266073232,
            "y": 1543.6572138966408
          },
          "data": {
            "type": "XMLIngestComponent",
            "node": {
              "template": {
                "_type": "Component",
                "xml_file": {
                  "trace_as_metadata": true,
                  "file_path": "",
                  "fileTypes": [
                    "xml"
                  ],
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "xml_file",
                  "value": "",
                  "display_name": "XML File",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Upload an XML file containing game data.",
                  "title_case": false,
                  "type": "file",
                  "_input_type": "FileInput",
                  "load_from_db": false
                },
                "base_url": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "base_url",
                  "value": "http://localhost:11434",
                  "display_name": "Base URL",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import xml.etree.ElementTree as ET\nimport psycopg2\nimport httpx\nfrom axiestudio.io import MessageTextInput\nfrom axiestudio.custom import Component\nfrom axiestudio.io import FileInput, StrInput, FloatInput, Output, DropdownInput\nfrom axiestudio.schema import Data\nfrom langchain_community.embeddings import OllamaEmbeddings\n\nclass XMLIngestComponent(Component):\n    display_name = \"XML Ingest Component\"\n    description = \"Ingests XML game data and inserts it into the PostgreSQL database with embeddings.\"\n    icon = \"file-code\"\n    name = \"XMLIngestComponent\"\n\n    inputs = [\n        FileInput(\n            name=\"xml_file\",\n            display_name=\"XML File\",\n            file_types=[\"xml\"],\n            info=\"Upload an XML file containing game data.\",\n        ),\n        MessageTextInput(\n            name=\"db_connection_string\",\n            display_name=\"DB Connection String\",\n            info=\"Enter the PostgreSQL connection string or connect from another component.\",\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"mxbai-embed-large:335m-v1-fp16\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n            info=\"Temperature for the Ollama model (if applicable).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Ingest Result\", name=\"ingest_result\", method=\"ingest_xml\"),\n    ]\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    def update_build_config(self, build_config: dict, field_value: any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n        return build_config\n\n    def ingest_xml(self) -> Data:\n        if not self.xml_file:\n            return Data(value=\"Error: No XML file uploaded.\")\n\n        try:\n            with open(self.xml_file, 'r') as file:\n                xml_content = file.read()\n\n            embeddings_model = OllamaEmbeddings(\n                model=self.model_name,\n                base_url=self.base_url,\n                temperature=self.temperature\n            )\n\n            with psycopg2.connect(self.db_connection_string) as conn:\n                agent = self.XMLIngestAgent(conn, embeddings_model)\n                agent.parse_xml_content(xml_content)\n            result = \"XML data ingested successfully\"\n        except Exception as e:\n            result = f\"Error: {str(e)}\"\n\n        return Data(value=result)\n\n    class XMLIngestAgent:\n        def __init__(self, db_connection, embeddings_model):\n            self.db_connection = db_connection\n            self.embeddings_model = embeddings_model\n    \n        def parse_xml_content(self, xml_content):\n            root = ET.fromstring(xml_content)\n            \n            module_id = root.get('moduleId')\n            rtp = float(root.get('rtp', 0))\n            hit_rate = float(root.get('hitRate', 0))\n            std_deviation = float(root.get('stdDeviation', 0))\n            \n            bet_model = root.find('BetModel')\n            paytable_type = bet_model.get('paytableType')\n            bet_multiplier = int(bet_model.get('betMultiplier', 1))\n        \n            game_text = f\"Game {module_id} with paytable type {paytable_type}, bet multiplier {bet_multiplier}, RTP {rtp}, hit rate {hit_rate}, and standard deviation {std_deviation}\"\n            game_embedding = self.embeddings_model.embed_query(game_text)\n            game_id = self.insert_or_get_game(module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, game_embedding)\n        \n            symbol_texts = []\n            symbol_data = []\n    \n            for reel_set in root.findall('.//ReelSet'):\n                reel_set_name = reel_set.get('name')\n                default_positions = [int(reel.get('defaultPosition', 0)) for reel in reel_set.findall('Reel')]\n                reel_set_text = f\"Reel set {reel_set_name} for game {module_id}\"\n                reel_set_embedding = self.embeddings_model.embed_query(reel_set_text)\n                reel_set_id = self.insert_or_get_reel_set(game_id, reel_set_name, default_positions, reel_set_embedding)\n        \n                for reel_index, reel in enumerate(reel_set.findall('Reel')):\n                    symbols = reel.get('symbols').split(',')\n                    for symbol_position, symbol in enumerate(symbols):\n                        symbol_text = f\"Symbol {symbol} at position {symbol_position} on reel {reel_index} in reel set {reel_set_name} of game {module_id}\"\n                        symbol_texts.append(symbol_text)\n                        symbol_data.append((game_id, reel_set_id, reel_index, symbol_position, symbol))\n    \n            # Batch embed all symbols\n            symbol_embeddings = self.embeddings_model.embed_documents(symbol_texts)\n    \n            # Batch insert all symbols\n            self.batch_insert_reel_symbols(symbol_data, symbol_embeddings)\n    \n        def batch_insert_reel_symbols(self, symbol_data, symbol_embeddings):\n            cursor = self.db_connection.cursor()\n            for (game_id, reel_set_id, reel_index, symbol_position, symbol), embedding in zip(symbol_data, symbol_embeddings):\n                cursor.execute(\"\"\"\n                    INSERT INTO Reels (game_id, reel_set_id, reel_index, symbol_reel_position, symbol, embedding)\n                    VALUES (%s, %s, %s, %s, %s, %s)\n                    ON CONFLICT (game_id, reel_set_id, reel_index, symbol_reel_position) \n                    DO UPDATE SET symbol = EXCLUDED.symbol, embedding = EXCLUDED.embedding\n                \"\"\", (game_id, reel_set_id, reel_index, symbol_position, symbol, embedding))\n            self.db_connection.commit()\n\n        def insert_or_get_game(self, module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Games (module_id, bet_model_paytable_type, bet_model_multiplier, rtp, hit_rate, std_deviation, embedding)\n                VALUES (%s, %s, %s, %s, %s, %s, %s)\n                ON CONFLICT (module_id) DO UPDATE \n                SET bet_model_paytable_type = EXCLUDED.bet_model_paytable_type,\n                    bet_model_multiplier = EXCLUDED.bet_model_multiplier,\n                    rtp = EXCLUDED.rtp,\n                    hit_rate = EXCLUDED.hit_rate,\n                    std_deviation = EXCLUDED.std_deviation,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_or_get_reel_set(self, game_id, reel_set_name, default_positions, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO ReelSets (game_id, name, default_positions, embedding)\n                VALUES (%s, %s, %s, %s)\n                ON CONFLICT (game_id, name) DO UPDATE \n                SET default_positions = EXCLUDED.default_positions,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (game_id, reel_set_name, default_positions, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_reel_symbols(self, reel_set_id, reel_index, symbol_position, symbol, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Reels (reel_set_id, reel_index, symbol_reel_position, symbol, embedding)\n                VALUES (%s, %s, %s, %s, %s)\n                ON CONFLICT (reel_set_id, reel_index, symbol_reel_position) \n                DO UPDATE SET symbol = EXCLUDED.symbol, embedding = EXCLUDED.embedding\n            \"\"\", (reel_set_id, reel_index, symbol_position, symbol, embedding))\n            self.db_connection.commit()\n\n# Register the custom component\nXMLIngestComponent()",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "db_connection_string": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "db_connection_string",
                  "value": "NormalDBConnectionString",
                  "display_name": "DB Connection String",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Enter the PostgreSQL connection string or connect from another component.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "mxbai-embed-large:335m-v1-fp16",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Refer to https://ollama.ai/library for more models.",
                  "refresh_button": true,
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.1,
                  "display_name": "Model Temperature",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Temperature for the Ollama model (if applicable).",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                }
              },
              "description": "Ingests XML game data and inserts it into the PostgreSQL database with embeddings.",
              "icon": "file-code",
              "base_classes": [
                "Data"
              ],
              "display_name": "BACKUP XMLIngestNodeEmbeddings",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "ingest_result",
                  "display_name": "Ingest Result",
                  "method": "ingest_xml",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "xml_file",
                "db_connection_string",
                "base_url",
                "model_name",
                "temperature"
              ],
              "beta": false,
              "edited": true
            },
            "id": "XMLIngestComponent-pNr7W"
          },
          "selected": false,
          "width": 384,
          "height": 580,
          "dragging": false,
          "positionAbsolute": {
            "x": 100.07916266073232,
            "y": 1543.6572138966408
          }
        },
        {
          "id": "XMLIngestComponent-EOvct",
          "type": "genericNode",
          "position": {
            "x": 63.385636526592066,
            "y": 605.1335863778307
          },
          "data": {
            "type": "XMLIngestComponent",
            "node": {
              "template": {
                "_type": "Component",
                "xml_file": {
                  "trace_as_metadata": true,
                  "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                  "fileTypes": [
                    "xml"
                  ],
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "xml_file",
                  "value": "",
                  "display_name": "XML File",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Upload an XML file containing game data.",
                  "title_case": false,
                  "type": "file",
                  "_input_type": "FileInput"
                },
                "base_url": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "base_url",
                  "value": "http://localhost:11434",
                  "display_name": "Base URL",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import xml.etree.ElementTree as ET\nimport psycopg2\nimport httpx\nfrom axiestudio.io import MessageTextInput\nfrom axiestudio.custom import Component\nfrom axiestudio.io import FileInput, StrInput, FloatInput, Output, DropdownInput\nfrom axiestudio.schema import Data\nfrom langchain_community.embeddings import OllamaEmbeddings\n\nclass XMLIngestComponent(Component):\n    display_name = \"XML Ingest Component\"\n    description = \"Ingests XML game data and inserts it into the PostgreSQL database with embeddings.\"\n    icon = \"file-code\"\n    name = \"XMLIngestComponent\"\n\n    inputs = [\n        FileInput(\n            name=\"xml_file\",\n            display_name=\"XML File\",\n            file_types=[\"xml\"],\n            info=\"Upload an XML file containing game data.\",\n        ),\n        MessageTextInput(\n            name=\"db_connection_string\",\n            display_name=\"DB Connection String\",\n            info=\"Enter the PostgreSQL connection string or connect from another component.\",\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"mxbai-embed-large:335m-v1-fp16\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n            info=\"Temperature for the Ollama model (if applicable).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Ingest Result\", name=\"ingest_result\", method=\"ingest_xml\"),\n    ]\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n            return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    def update_build_config(self, build_config: dict, field_value: any, field_name: str | None = None):\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n        return build_config\n\n    def ingest_xml(self) -> Data:\n        if not self.xml_file:\n            return Data(value=\"Error: No XML file uploaded.\")\n\n        try:\n            with open(self.xml_file, 'r') as file:\n                xml_content = file.read()\n\n            embeddings_model = OllamaEmbeddings(\n                model=self.model_name,\n                base_url=self.base_url,\n                temperature=self.temperature\n            )\n\n            with psycopg2.connect(self.db_connection_string) as conn:\n                agent = self.XMLIngestAgent(conn, embeddings_model)\n                agent.parse_xml_content(xml_content)\n            result = \"XML data ingested successfully\"\n        except Exception as e:\n            result = f\"Error: {str(e)}\"\n\n        return Data(value=result)\n\n    class XMLIngestAgent:\n        def __init__(self, db_connection, embeddings_model):\n            self.db_connection = db_connection\n            self.embeddings_model = embeddings_model\n    \n        def parse_xml_content(self, xml_content):\n            root = ET.fromstring(xml_content)\n            \n            module_id = root.get('moduleId')\n            rtp = float(root.get('rtp', 0))\n            hit_rate = float(root.get('hitRate', 0))\n            std_deviation = float(root.get('stdDeviation', 0))\n            \n            bet_model = root.find('BetModel')\n            paytable_type = bet_model.get('paytableType')\n            bet_multiplier = int(bet_model.get('betMultiplier', 1))\n        \n            game_text = f\"Game {module_id} with paytable type {paytable_type}, bet multiplier {bet_multiplier}, RTP {rtp}, hit rate {hit_rate}, and standard deviation {std_deviation}\"\n            game_embedding = self.embeddings_model.embed_query(game_text)\n            game_id = self.insert_or_get_game(module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, game_embedding)\n        \n            symbol_data = []\n    \n            for reel_set in root.findall('.//ReelSet'):\n                reel_set_name = reel_set.get('name')\n                default_positions = [int(reel.get('defaultPosition', 0)) for reel in reel_set.findall('Reel')]\n                reel_set_text = f\"Reel set {reel_set_name} for game {module_id}\"\n                reel_set_embedding = self.embeddings_model.embed_query(reel_set_text)\n                reel_set_id = self.insert_or_get_reel_set(game_id, reel_set_name, default_positions, reel_set_embedding)\n        \n                for reel_index, reel in enumerate(reel_set.findall('Reel')):\n                    symbols = reel.get('symbols').split(',')\n                    for symbol_position, symbol_name in enumerate(symbols):\n                        symbol_id = self.insert_or_get_symbol(symbol_name)\n                        symbol_text = f\"Symbol {symbol_name} at position {symbol_position} on reel {reel_index} in reel set {reel_set_name} of game {module_id}\"\n                        symbol_embedding = self.embeddings_model.embed_query(symbol_text)\n                        symbol_data.append((game_id, reel_set_id, reel_index, symbol_position, symbol_id, symbol_embedding))\n    \n            # Batch insert all symbols\n            self.batch_insert_reel_symbols(symbol_data)\n    \n        def batch_insert_reel_symbols(self, symbol_data):\n            cursor = self.db_connection.cursor()\n            for game_id, reel_set_id, reel_index, symbol_position, symbol_id, embedding in symbol_data:\n                cursor.execute(\"\"\"\n                    INSERT INTO Reels (game_id, reel_set_id, reel_index, symbol_reel_position, symbol_id, embedding)\n                    VALUES (%s, %s, %s, %s, %s, %s)\n                    ON CONFLICT (game_id, reel_set_id, reel_index, symbol_reel_position) \n                    DO UPDATE SET symbol_id = EXCLUDED.symbol_id, embedding = EXCLUDED.embedding\n                \"\"\", (game_id, reel_set_id, reel_index, symbol_position, symbol_id, embedding))\n            self.db_connection.commit()\n\n        def insert_or_get_game(self, module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Games (module_id, bet_model_paytable_type, bet_model_multiplier, rtp, hit_rate, std_deviation, embedding)\n                VALUES (%s, %s, %s, %s, %s, %s, %s)\n                ON CONFLICT (module_id) DO UPDATE \n                SET bet_model_paytable_type = EXCLUDED.bet_model_paytable_type,\n                    bet_model_multiplier = EXCLUDED.bet_model_multiplier,\n                    rtp = EXCLUDED.rtp,\n                    hit_rate = EXCLUDED.hit_rate,\n                    std_deviation = EXCLUDED.std_deviation,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (module_id, paytable_type, bet_multiplier, rtp, hit_rate, std_deviation, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_or_get_reel_set(self, game_id, reel_set_name, default_positions, embedding):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO ReelSets (game_id, name, default_positions, embedding)\n                VALUES (%s, %s, %s, %s)\n                ON CONFLICT (game_id, name) DO UPDATE \n                SET default_positions = EXCLUDED.default_positions,\n                    embedding = EXCLUDED.embedding\n                RETURNING id\n            \"\"\", (game_id, reel_set_name, default_positions, embedding))\n            return cursor.fetchone()[0]\n\n        def insert_or_get_symbol(self, symbol_name):\n            cursor = self.db_connection.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO Symbols (name)\n                VALUES (%s)\n                ON CONFLICT (name) DO NOTHING\n                RETURNING id\n            \"\"\", (symbol_name,))\n            result = cursor.fetchone()\n            if result:\n                return result[0]\n            else:\n                cursor.execute(\"SELECT id FROM Symbols WHERE name = %s\", (symbol_name,))\n                return cursor.fetchone()[0]\n\n# Register the custom component\nXMLIngestComponent()",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "db_connection_string": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "db_connection_string",
                  "value": "NormalDBConnectionString",
                  "display_name": "DB Connection String",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Enter the PostgreSQL connection string or connect from another component.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "mxbai-embed-large:335m-v1-fp16",
                    "nomic-embed-text:latest",
                    "deepseek-coder-v2:16b-lite-instruct-fp16",
                    "llama3-groq-tool-use:latest",
                    "gemma-2b-it-q8_0.gguf:latest",
                    "dolphin-mistral:7b-v2.8-fp16",
                    "mistral-nemo:12b-instruct-2407-fp16",
                    "llama3.1:8b-instruct-fp16",
                    "codestral:22b-v0.1-q8_0",
                    "llama3:8b"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "mxbai-embed-large:335m-v1-fp16",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Refer to https://ollama.ai/library for more models.",
                  "refresh_button": true,
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.1,
                  "display_name": "Model Temperature",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Temperature for the Ollama model (if applicable).",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                }
              },
              "description": "Ingests XML game data and inserts it into the PostgreSQL database with embeddings.",
              "icon": "file-code",
              "base_classes": [
                "Data"
              ],
              "display_name": "v5 DB XMLIngestNodeEmbeddings",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "ingest_result",
                  "display_name": "Ingest Result",
                  "method": "ingest_xml",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "xml_file",
                "db_connection_string",
                "base_url",
                "model_name",
                "temperature"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.15"
            },
            "id": "XMLIngestComponent-EOvct"
          },
          "selected": false,
          "width": 384,
          "height": 580,
          "positionAbsolute": {
            "x": 63.385636526592066,
            "y": 605.1335863778307
          },
          "dragging": false
        },
        {
          "data": {
            "id": "groupComponent-NSkEw",
            "type": "GroupNode",
            "node": {
              "display_name": "Group",
              "documentation": "",
              "description": "",
              "template": {
                "chunk_overlap_SplitText-xQWvM": {
                  "advanced": true,
                  "display_name": "Chunk Overlap",
                  "dynamic": false,
                  "info": "Number of characters to overlap between chunks.",
                  "list": false,
                  "name": "chunk_overlap",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "int",
                  "value": "200",
                  "proxy": {
                    "id": "SplitText-xQWvM",
                    "field": "chunk_overlap"
                  }
                },
                "chunk_size_SplitText-xQWvM": {
                  "advanced": true,
                  "display_name": "Chunk Size",
                  "dynamic": false,
                  "info": "The maximum number of characters in each chunk.",
                  "list": false,
                  "name": "chunk_size",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "int",
                  "value": 1000,
                  "proxy": {
                    "id": "SplitText-xQWvM",
                    "field": "chunk_size"
                  }
                },
                "code_SplitText-xQWvM": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom axiestudio.custom import Component\nfrom axiestudio.io import HandleInput, IntInput, MessageTextInput, Output\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n",
                  "display_name": "code",
                  "proxy": {
                    "id": "SplitText-xQWvM",
                    "field": "code"
                  }
                },
                "separator_SplitText-xQWvM": {
                  "advanced": false,
                  "display_name": "Separator",
                  "dynamic": false,
                  "info": "The character to split on. Defaults to newline.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "separator",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "\n",
                  "proxy": {
                    "id": "SplitText-xQWvM",
                    "field": "separator"
                  }
                },
                "code_File-HmFrT": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from pathlib import Path\n\nfrom axiestudio.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, FileInput, Output\nfrom axiestudio.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data if data else \"No data\"\n        return data or Data()\n",
                  "display_name": "code",
                  "proxy": {
                    "id": "File-HmFrT",
                    "field": "code"
                  }
                },
                "path_File-HmFrT": {
                  "advanced": true,
                  "display_name": "Path",
                  "dynamic": false,
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx"
                  ],
                  "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                  "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx",
                  "list": false,
                  "name": "path",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "file",
                  "value": "",
                  "proxy": {
                    "id": "File-HmFrT",
                    "field": "path"
                  }
                },
                "silent_errors_File-HmFrT": {
                  "advanced": true,
                  "display_name": "Silent Errors",
                  "dynamic": false,
                  "info": "If true, errors will not raise an exception.",
                  "list": false,
                  "name": "silent_errors",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false,
                  "proxy": {
                    "id": "File-HmFrT",
                    "field": "silent_errors"
                  }
                },
                "base_url_OllamaEmbeddings-tSXRh": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "base_url",
                  "value": "http://localhost:11434",
                  "display_name": "Ollama Base URL",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput",
                  "proxy": {
                    "id": "OllamaEmbeddings-tSXRh",
                    "field": "base_url"
                  }
                },
                "code_OllamaEmbeddings-tSXRh": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "OllamaEmbeddings-tSXRh",
                    "field": "code"
                  }
                },
                "model_OllamaEmbeddings-tSXRh": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model",
                  "value": "mxbai-embed-large:335m-v1-fp16",
                  "display_name": "Ollama Model",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput",
                  "proxy": {
                    "id": "OllamaEmbeddings-tSXRh",
                    "field": "model"
                  }
                },
                "temperature_OllamaEmbeddings-tSXRh": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.1,
                  "display_name": "Model Temperature",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput",
                  "proxy": {
                    "id": "OllamaEmbeddings-tSXRh",
                    "field": "temperature"
                  }
                },
                "code_pgvector-TRQkk": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import List\n\n#from langchain_community.vectorstores import PGVector\nfrom langchain_postgres import PGVector\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.connection_string_parser import transform_connection_string\n\n\nclass PGVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"PGVector Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    name = \"pgvector\"\n    icon = \"PGVector\"\n\n    inputs = [\n        SecretStrInput(name=\"pg_server_url\", display_name=\"PostgreSQL Server Connection String\", required=True),\n        StrInput(name=\"collection_name\", display_name=\"Table\", required=True),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n    ]\n    \n    def validate_connection_string(self, connection_string: str) -> bool:\n        parts = connection_string.split('@')\n        if len(parts) != 2:\n            return False\n        credentials, host_port_db = parts\n        if ':' not in credentials or ':' not in host_port_db:\n            return False\n        host_port, db = host_port_db.split('/')\n        if ':' not in host_port:\n            return False\n        return True\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n    \n        if not self.validate_connection_string(self.pg_server_url):\n            raise ValueError(\"Invalid connection string format. Expected format: username:password@host:port/database\")\n    \n        # Construct the connection string directly\n        connection_string = f\"postgresql+psycopg://{self.pg_server_url}\"\n    \n        if documents:\n            pgvector = PGVector.from_documents(\n                embedding=self.embedding,\n                documents=documents,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n    \n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_pgvector()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "display_name": "code",
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "field": "code"
                  }
                },
                "collection_name_pgvector-TRQkk": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "collection_name",
                  "value": "items",
                  "display_name": "Table",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput",
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "field": "collection_name"
                  }
                },
                "number_of_results_pgvector-TRQkk": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "number_of_results",
                  "value": 4,
                  "display_name": "Number of Results",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of results to return.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput",
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "field": "number_of_results"
                  }
                },
                "pg_server_url_pgvector-TRQkk": {
                  "load_from_db": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "pg_server_url",
                  "value": "",
                  "display_name": "PostgreSQL Server Connection String",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput",
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "field": "pg_server_url"
                  }
                },
                "search_query_pgvector-TRQkk": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "search_query",
                  "value": "",
                  "display_name": "Search Query",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput",
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "field": "search_query"
                  }
                }
              },
              "flow": {
                "data": {
                  "nodes": [
                    {
                      "data": {
                        "description": "Split text into chunks based on specified criteria.",
                        "display_name": "Split Text",
                        "id": "SplitText-xQWvM",
                        "node": {
                          "base_classes": [
                            "Data"
                          ],
                          "beta": false,
                          "conditional_paths": [],
                          "custom_fields": {},
                          "description": "Split text into chunks based on specified criteria.",
                          "display_name": "Split Text",
                          "documentation": "",
                          "edited": false,
                          "field_order": [
                            "data_inputs",
                            "chunk_overlap",
                            "chunk_size",
                            "separator"
                          ],
                          "frozen": false,
                          "icon": "scissors-line-dashed",
                          "output_types": [],
                          "outputs": [
                            {
                              "cache": true,
                              "display_name": "Chunks",
                              "method": "split_text",
                              "name": "chunks",
                              "selected": "Data",
                              "types": [
                                "Data"
                              ],
                              "value": "__UNDEFINED__"
                            }
                          ],
                          "pinned": false,
                          "template": {
                            "_type": "Component",
                            "chunk_overlap": {
                              "advanced": false,
                              "display_name": "Chunk Overlap",
                              "dynamic": false,
                              "info": "Number of characters to overlap between chunks.",
                              "list": false,
                              "name": "chunk_overlap",
                              "placeholder": "",
                              "required": false,
                              "show": true,
                              "title_case": false,
                              "trace_as_metadata": true,
                              "type": "int",
                              "value": "200"
                            },
                            "chunk_size": {
                              "advanced": false,
                              "display_name": "Chunk Size",
                              "dynamic": false,
                              "info": "The maximum number of characters in each chunk.",
                              "list": false,
                              "name": "chunk_size",
                              "placeholder": "",
                              "required": false,
                              "show": true,
                              "title_case": false,
                              "trace_as_metadata": true,
                              "type": "int",
                              "value": 1000
                            },
                            "code": {
                              "advanced": true,
                              "dynamic": true,
                              "fileTypes": [],
                              "file_path": "",
                              "info": "",
                              "list": false,
                              "load_from_db": false,
                              "multiline": true,
                              "name": "code",
                              "password": false,
                              "placeholder": "",
                              "required": true,
                              "show": true,
                              "title_case": false,
                              "type": "code",
                              "value": "from typing import List\n\nfrom langchain_text_splitters import CharacterTextSplitter\n\nfrom axiestudio.custom import Component\nfrom axiestudio.io import HandleInput, IntInput, MessageTextInput, Output\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data Inputs\",\n            info=\"The data to split.\",\n            input_types=[\"Data\"],\n            is_list=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum number of characters in each chunk.\",\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=\"The character to split on. Defaults to newline.\",\n            value=\"\\n\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n    ]\n\n    def _docs_to_data(self, docs):\n        data = []\n        for doc in docs:\n            data.append(Data(text=doc.page_content, data=doc.metadata))\n        return data\n\n    def split_text(self) -> List[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = []\n        for _input in self.data_inputs:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n",
                              "display_name": "code"
                            },
                            "data_inputs": {
                              "advanced": false,
                              "display_name": "Data Inputs",
                              "dynamic": false,
                              "info": "The data to split.",
                              "input_types": [
                                "Data"
                              ],
                              "list": true,
                              "name": "data_inputs",
                              "placeholder": "",
                              "required": false,
                              "show": true,
                              "title_case": false,
                              "trace_as_metadata": true,
                              "type": "other",
                              "value": ""
                            },
                            "separator": {
                              "advanced": false,
                              "display_name": "Separator",
                              "dynamic": false,
                              "info": "The character to split on. Defaults to newline.",
                              "input_types": [
                                "Message"
                              ],
                              "list": false,
                              "load_from_db": false,
                              "name": "separator",
                              "placeholder": "",
                              "required": false,
                              "show": true,
                              "title_case": false,
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "type": "str",
                              "value": "\n"
                            }
                          },
                          "lf_version": "1.0.15"
                        },
                        "type": "SplitText"
                      },
                      "dragging": false,
                      "height": 542,
                      "id": "SplitText-xQWvM",
                      "position": {
                        "x": -1615.5378919517811,
                        "y": 150.45059818858732
                      },
                      "positionAbsolute": {
                        "x": -1615.5378919517811,
                        "y": 150.45059818858732
                      },
                      "selected": true,
                      "type": "genericNode",
                      "width": 384
                    },
                    {
                      "data": {
                        "description": "A generic file loader.",
                        "display_name": "File",
                        "id": "File-HmFrT",
                        "node": {
                          "base_classes": [
                            "Data"
                          ],
                          "beta": false,
                          "conditional_paths": [],
                          "custom_fields": {},
                          "description": "A generic file loader.",
                          "display_name": "File",
                          "documentation": "",
                          "edited": false,
                          "field_order": [
                            "path",
                            "silent_errors"
                          ],
                          "frozen": false,
                          "icon": "file-text",
                          "output_types": [],
                          "outputs": [
                            {
                              "cache": true,
                              "display_name": "Data",
                              "method": "load_file",
                              "name": "data",
                              "selected": "Data",
                              "types": [
                                "Data"
                              ],
                              "value": "__UNDEFINED__"
                            }
                          ],
                          "pinned": false,
                          "template": {
                            "_type": "Component",
                            "code": {
                              "advanced": true,
                              "dynamic": true,
                              "fileTypes": [],
                              "file_path": "",
                              "info": "",
                              "list": false,
                              "load_from_db": false,
                              "multiline": true,
                              "name": "code",
                              "password": false,
                              "placeholder": "",
                              "required": true,
                              "show": true,
                              "title_case": false,
                              "type": "code",
                              "value": "from pathlib import Path\n\nfrom axiestudio.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, FileInput, Output\nfrom axiestudio.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = \"File\"\n    description = \"A generic file loader.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data if data else \"No data\"\n        return data or Data()\n",
                              "display_name": "code"
                            },
                            "path": {
                              "advanced": false,
                              "display_name": "Path",
                              "dynamic": false,
                              "fileTypes": [
                                "txt",
                                "md",
                                "mdx",
                                "csv",
                                "json",
                                "yaml",
                                "yml",
                                "xml",
                                "html",
                                "htm",
                                "pdf",
                                "docx",
                                "py",
                                "sh",
                                "sql",
                                "js",
                                "ts",
                                "tsx"
                              ],
                              "file_path": "d1f96559-ebce-4dba-9ab2-62f06afdd05f/101440.xml",
                              "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx",
                              "list": false,
                              "name": "path",
                              "placeholder": "",
                              "required": false,
                              "show": true,
                              "title_case": false,
                              "trace_as_metadata": true,
                              "type": "file",
                              "value": ""
                            },
                            "silent_errors": {
                              "advanced": true,
                              "display_name": "Silent Errors",
                              "dynamic": false,
                              "info": "If true, errors will not raise an exception.",
                              "list": false,
                              "name": "silent_errors",
                              "placeholder": "",
                              "required": false,
                              "show": true,
                              "title_case": false,
                              "trace_as_metadata": true,
                              "type": "bool",
                              "value": false
                            }
                          },
                          "lf_version": "1.0.15"
                        },
                        "type": "File"
                      },
                      "dragging": false,
                      "height": 294,
                      "id": "File-HmFrT",
                      "position": {
                        "x": -2136.139058842607,
                        "y": 394.34362259205744
                      },
                      "positionAbsolute": {
                        "x": -2136.139058842607,
                        "y": 394.34362259205744
                      },
                      "selected": true,
                      "type": "genericNode",
                      "width": 384
                    },
                    {
                      "id": "OllamaEmbeddings-tSXRh",
                      "type": "genericNode",
                      "position": {
                        "x": -1099.274159598312,
                        "y": 298.13099021293846
                      },
                      "data": {
                        "type": "OllamaEmbeddings",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "base_url": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "name": "base_url",
                              "value": "http://localhost:11434",
                              "display_name": "Ollama Base URL",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "MessageTextInput"
                            },
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "model": {
                              "trace_as_input": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "name": "model",
                              "value": "mxbai-embed-large:335m-v1-fp16",
                              "display_name": "Ollama Model",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "MessageTextInput"
                            },
                            "temperature": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "name": "temperature",
                              "value": 0.1,
                              "display_name": "Model Temperature",
                              "advanced": true,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "float",
                              "_input_type": "FloatInput"
                            }
                          },
                          "description": "Generate embeddings using Ollama models.",
                          "icon": "Ollama",
                          "base_classes": [
                            "Embeddings"
                          ],
                          "display_name": "Ollama Embeddings",
                          "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Embeddings"
                              ],
                              "selected": "Embeddings",
                              "name": "embeddings",
                              "display_name": "Embeddings",
                              "method": "build_embeddings",
                              "value": "__UNDEFINED__",
                              "cache": true
                            }
                          ],
                          "field_order": [
                            "model",
                            "base_url",
                            "temperature"
                          ],
                          "beta": false,
                          "edited": false,
                          "lf_version": "1.0.15"
                        },
                        "id": "OllamaEmbeddings-tSXRh"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 388,
                      "positionAbsolute": {
                        "x": -1099.274159598312,
                        "y": 298.13099021293846
                      },
                      "dragging": false
                    },
                    {
                      "id": "pgvector-TRQkk",
                      "type": "genericNode",
                      "position": {
                        "x": -576.9483086802642,
                        "y": 39.28261180227025
                      },
                      "data": {
                        "type": "pgvector",
                        "node": {
                          "template": {
                            "_type": "Component",
                            "embedding": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "name": "embedding",
                              "value": "",
                              "display_name": "Embedding",
                              "advanced": false,
                              "input_types": [
                                "Embeddings"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "other",
                              "_input_type": "HandleInput"
                            },
                            "ingest_data": {
                              "trace_as_metadata": true,
                              "list": true,
                              "trace_as_input": true,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "name": "ingest_data",
                              "value": "",
                              "display_name": "Ingestion Data",
                              "advanced": false,
                              "input_types": [
                                "Data"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "other",
                              "_input_type": "DataInput"
                            },
                            "code": {
                              "type": "code",
                              "required": true,
                              "placeholder": "",
                              "list": false,
                              "show": true,
                              "multiline": true,
                              "value": "from typing import List\n\n#from langchain_community.vectorstores import PGVector\nfrom langchain_postgres import PGVector\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import HandleInput, IntInput, StrInput, SecretStrInput, DataInput, MultilineInput\nfrom axiestudio.schema import Data\nfrom axiestudio.utils.connection_string_parser import transform_connection_string\n\n\nclass PGVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"PGVector Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    name = \"pgvector\"\n    icon = \"PGVector\"\n\n    inputs = [\n        SecretStrInput(name=\"pg_server_url\", display_name=\"PostgreSQL Server Connection String\", required=True),\n        StrInput(name=\"collection_name\", display_name=\"Table\", required=True),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n    ]\n    \n    def validate_connection_string(self, connection_string: str) -> bool:\n        parts = connection_string.split('@')\n        if len(parts) != 2:\n            return False\n        credentials, host_port_db = parts\n        if ':' not in credentials or ':' not in host_port_db:\n            return False\n        host_port, db = host_port_db.split('/')\n        if ':' not in host_port:\n            return False\n        return True\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n    \n        if not self.validate_connection_string(self.pg_server_url):\n            raise ValueError(\"Invalid connection string format. Expected format: username:password@host:port/database\")\n    \n        # Construct the connection string directly\n        connection_string = f\"postgresql+psycopg://{self.pg_server_url}\"\n    \n        if documents:\n            pgvector = PGVector.from_documents(\n                embedding=self.embedding,\n                documents=documents,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=self.collection_name,\n                connection=connection_string,\n            )\n    \n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        vector_store = self._build_pgvector()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        else:\n            return []\n",
                              "fileTypes": [],
                              "file_path": "",
                              "password": false,
                              "name": "code",
                              "advanced": true,
                              "dynamic": true,
                              "info": "",
                              "load_from_db": false,
                              "title_case": false,
                              "display_name": "code"
                            },
                            "collection_name": {
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": true,
                              "placeholder": "",
                              "show": true,
                              "name": "collection_name",
                              "value": "items",
                              "display_name": "Table",
                              "advanced": false,
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "StrInput"
                            },
                            "number_of_results": {
                              "trace_as_metadata": true,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "name": "number_of_results",
                              "value": 4,
                              "display_name": "Number of Results",
                              "advanced": true,
                              "dynamic": false,
                              "info": "Number of results to return.",
                              "title_case": false,
                              "type": "int",
                              "_input_type": "IntInput"
                            },
                            "pg_server_url": {
                              "load_from_db": false,
                              "required": true,
                              "placeholder": "",
                              "show": true,
                              "name": "pg_server_url",
                              "value": "aiuser:aipassword@172.20.0.2:5432/vectordb",
                              "display_name": "PostgreSQL Server Connection String",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "password": true,
                              "type": "str",
                              "_input_type": "SecretStrInput"
                            },
                            "search_query": {
                              "trace_as_input": true,
                              "multiline": true,
                              "trace_as_metadata": true,
                              "load_from_db": false,
                              "list": false,
                              "required": false,
                              "placeholder": "",
                              "show": true,
                              "name": "search_query",
                              "value": "",
                              "display_name": "Search Query",
                              "advanced": false,
                              "input_types": [
                                "Message"
                              ],
                              "dynamic": false,
                              "info": "",
                              "title_case": false,
                              "type": "str",
                              "_input_type": "MultilineInput"
                            }
                          },
                          "description": "PGVector Vector Store with search capabilities",
                          "icon": "PGVector",
                          "base_classes": [
                            "Data",
                            "Retriever",
                            "VectorStore"
                          ],
                          "display_name": "Nicks PGVector Custom Auth",
                          "documentation": "https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/",
                          "custom_fields": {},
                          "output_types": [],
                          "pinned": false,
                          "conditional_paths": [],
                          "frozen": false,
                          "outputs": [
                            {
                              "types": [
                                "Retriever"
                              ],
                              "selected": "Retriever",
                              "name": "base_retriever",
                              "display_name": "Retriever",
                              "method": "build_base_retriever",
                              "value": "__UNDEFINED__",
                              "cache": true
                            },
                            {
                              "types": [
                                "Data"
                              ],
                              "selected": "Data",
                              "name": "search_results",
                              "display_name": "Search Results",
                              "method": "search_documents",
                              "value": "__UNDEFINED__",
                              "cache": true
                            },
                            {
                              "types": [
                                "VectorStore"
                              ],
                              "selected": "VectorStore",
                              "name": "vector_store",
                              "display_name": "Vector Store",
                              "method": "cast_vector_store",
                              "value": "__UNDEFINED__",
                              "cache": true
                            }
                          ],
                          "field_order": [
                            "pg_server_url",
                            "collection_name",
                            "search_query",
                            "ingest_data",
                            "embedding",
                            "number_of_results",
                            "embedding"
                          ],
                          "beta": false,
                          "edited": true,
                          "lf_version": "1.0.15"
                        },
                        "id": "pgvector-TRQkk"
                      },
                      "selected": true,
                      "width": 384,
                      "height": 644,
                      "positionAbsolute": {
                        "x": -576.9483086802642,
                        "y": 39.28261180227025
                      },
                      "dragging": false
                    }
                  ],
                  "edges": [
                    {
                      "className": "",
                      "data": {
                        "sourceHandle": {
                          "dataType": "File",
                          "id": "File-HmFrT",
                          "name": "data",
                          "output_types": [
                            "Data"
                          ]
                        },
                        "targetHandle": {
                          "fieldName": "data_inputs",
                          "id": "SplitText-xQWvM",
                          "inputTypes": [
                            "Data"
                          ],
                          "type": "other"
                        }
                      },
                      "id": "reactflow__edge-File-HmFrT{dataType:File,id:File-HmFrT,name:data,output_types:[Data]}-SplitText-xQWvM{fieldName:data_inputs,id:SplitText-xQWvM,inputTypes:[Data],type:other}",
                      "source": "File-HmFrT",
                      "sourceHandle": "{dataType:File,id:File-HmFrT,name:data,output_types:[Data]}",
                      "target": "SplitText-xQWvM",
                      "targetHandle": "{fieldName:data_inputs,id:SplitText-xQWvM,inputTypes:[Data],type:other}"
                    },
                    {
                      "className": "",
                      "data": {
                        "sourceHandle": {
                          "dataType": "SplitText",
                          "id": "SplitText-xQWvM",
                          "name": "chunks",
                          "output_types": [
                            "Data"
                          ]
                        },
                        "targetHandle": {
                          "fieldName": "ingest_data",
                          "id": "pgvector-TRQkk",
                          "inputTypes": [
                            "Data"
                          ],
                          "type": "other"
                        }
                      },
                      "source": "SplitText-xQWvM",
                      "sourceHandle": "{dataType:SplitText,id:SplitText-xQWvM,name:chunks,output_types:[Data]}",
                      "target": "pgvector-TRQkk",
                      "targetHandle": "{fieldName:ingest_data,id:pgvector-TRQkk,inputTypes:[Data],type:other}",
                      "id": "reactflow__edge-SplitText-xQWvM{dataType:SplitText,id:SplitText-xQWvM,name:chunks,output_types:[Data]}-pgvector-TRQkk{fieldName:ingest_data,id:pgvector-TRQkk,inputTypes:[Data],type:other}"
                    },
                    {
                      "source": "OllamaEmbeddings-tSXRh",
                      "sourceHandle": "{dataType:OllamaEmbeddings,id:OllamaEmbeddings-tSXRh,name:embeddings,output_types:[Embeddings]}",
                      "target": "pgvector-TRQkk",
                      "targetHandle": "{fieldName:embedding,id:pgvector-TRQkk,inputTypes:[Embeddings],type:other}",
                      "data": {
                        "targetHandle": {
                          "fieldName": "embedding",
                          "id": "pgvector-TRQkk",
                          "inputTypes": [
                            "Embeddings"
                          ],
                          "type": "other"
                        },
                        "sourceHandle": {
                          "dataType": "OllamaEmbeddings",
                          "id": "OllamaEmbeddings-tSXRh",
                          "name": "embeddings",
                          "output_types": [
                            "Embeddings"
                          ]
                        }
                      },
                      "id": "reactflow__edge-OllamaEmbeddings-tSXRh{dataType:OllamaEmbeddings,id:OllamaEmbeddings-tSXRh,name:embeddings,output_types:[Embeddings]}-pgvector-TRQkk{fieldName:embedding,id:pgvector-TRQkk,inputTypes:[Embeddings],type:other}"
                    }
                  ],
                  "viewport": {
                    "zoom": 1,
                    "x": 0,
                    "y": 0
                  }
                },
                "is_component": false,
                "name": "Optimistic Pasteur",
                "description": "",
                "id": "nisQj"
              },
              "outputs": [
                {
                  "types": [
                    "Retriever"
                  ],
                  "selected": "Retriever",
                  "name": "pgvector-TRQkk_base_retriever",
                  "display_name": "Retriever",
                  "method": "build_base_retriever",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "name": "base_retriever",
                    "nodeDisplayName": "Nicks PGVector Custom Auth"
                  }
                },
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "pgvector-TRQkk_search_results",
                  "display_name": "Search Results",
                  "method": "search_documents",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "name": "search_results",
                    "nodeDisplayName": "Nicks PGVector Custom Auth"
                  }
                },
                {
                  "types": [
                    "VectorStore"
                  ],
                  "selected": "VectorStore",
                  "name": "pgvector-TRQkk_vector_store",
                  "display_name": "Vector Store",
                  "method": "cast_vector_store",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "proxy": {
                    "id": "pgvector-TRQkk",
                    "name": "vector_store",
                    "nodeDisplayName": "Nicks PGVector Custom Auth"
                  }
                }
              ]
            }
          },
          "id": "groupComponent-NSkEw",
          "position": {
            "x": -441.4072095841216,
            "y": -71.12875305302705
          },
          "type": "genericNode",
          "width": 384,
          "height": 798,
          "selected": false,
          "positionAbsolute": {
            "x": -441.4072095841216,
            "y": -71.12875305302705
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "ParseData",
              "id": "ParseData-4Lvig",
              "name": "text",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "context",
              "id": "Prompt-z8nbL",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            }
          },
          "id": "reactflow__edge-ParseData-4Lvig{dataType:ParseData,id:ParseData-4Lvig,name:text,output_types:[Message]}-Prompt-z8nbL{fieldName:context,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}",
          "source": "ParseData-4Lvig",
          "sourceHandle": "{dataType:ParseData,id:ParseData-4Lvig,name:text,output_types:[Message]}",
          "target": "Prompt-z8nbL",
          "targetHandle": "{fieldName:context,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}"
        },
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-sgEOq",
              "name": "message",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "question",
              "id": "Prompt-z8nbL",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            }
          },
          "id": "reactflow__edge-ChatInput-sgEOq{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}-Prompt-z8nbL{fieldName:question,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}",
          "source": "ChatInput-sgEOq",
          "sourceHandle": "{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}",
          "target": "Prompt-z8nbL",
          "targetHandle": "{fieldName:question,id:Prompt-z8nbL,inputTypes:[Message,Text],type:str}"
        },
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-sgEOq",
              "name": "message",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "search_query",
              "id": "pgvector-8NuFT",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            }
          },
          "source": "ChatInput-sgEOq",
          "sourceHandle": "{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}",
          "target": "pgvector-8NuFT",
          "targetHandle": "{fieldName:search_query,id:pgvector-8NuFT,inputTypes:[Message],type:str}",
          "id": "reactflow__edge-ChatInput-sgEOq{dataType:ChatInput,id:ChatInput-sgEOq,name:message,output_types:[Message]}-pgvector-8NuFT{fieldName:search_query,id:pgvector-8NuFT,inputTypes:[Message],type:str}"
        },
        {
          "source": "pgvector-8NuFT",
          "sourceHandle": "{dataType:pgvector,id:pgvector-8NuFT,name:search_results,output_types:[Data]}",
          "target": "ParseData-4Lvig",
          "targetHandle": "{fieldName:data,id:ParseData-4Lvig,inputTypes:[Data],type:other}",
          "data": {
            "targetHandle": {
              "fieldName": "data",
              "id": "ParseData-4Lvig",
              "inputTypes": [
                "Data"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "pgvector",
              "id": "pgvector-8NuFT",
              "name": "search_results",
              "output_types": [
                "Data"
              ]
            }
          },
          "id": "reactflow__edge-pgvector-8NuFT{dataType:pgvector,id:pgvector-8NuFT,name:search_results,output_types:[Data]}-ParseData-4Lvig{fieldName:data,id:ParseData-4Lvig,inputTypes:[Data],type:other}",
          "className": ""
        },
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "OllamaEmbeddings",
              "id": "OllamaEmbeddings-zW4Ul",
              "name": "embeddings",
              "output_types": [
                "Embeddings"
              ]
            },
            "targetHandle": {
              "fieldName": "embedding",
              "id": "pgvector-8NuFT",
              "inputTypes": [
                "Embeddings"
              ],
              "type": "other"
            }
          },
          "source": "OllamaEmbeddings-zW4Ul",
          "sourceHandle": "{dataType:OllamaEmbeddings,id:OllamaEmbeddings-zW4Ul,name:embeddings,output_types:[Embeddings]}",
          "target": "pgvector-8NuFT",
          "targetHandle": "{fieldName:embedding,id:pgvector-8NuFT,inputTypes:[Embeddings],type:other}",
          "id": "reactflow__edge-OllamaEmbeddings-zW4Ul{dataType:OllamaEmbeddings,id:OllamaEmbeddings-zW4Ul,name:embeddings,output_types:[Embeddings]}-pgvector-8NuFT{fieldName:embedding,id:pgvector-8NuFT,inputTypes:[Embeddings],type:other}"
        },
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-z8nbL",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "input_value",
              "id": "OllamaModel-jpiyc",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            }
          },
          "source": "Prompt-z8nbL",
          "sourceHandle": "{dataType:Prompt,id:Prompt-z8nbL,name:prompt,output_types:[Message]}",
          "target": "OllamaModel-jpiyc",
          "targetHandle": "{fieldName:input_value,id:OllamaModel-jpiyc,inputTypes:[Message],type:str}",
          "id": "reactflow__edge-Prompt-z8nbL{dataType:Prompt,id:Prompt-z8nbL,name:prompt,output_types:[Message]}-OllamaModel-jpiyc{fieldName:input_value,id:OllamaModel-jpiyc,inputTypes:[Message],type:str}"
        },
        {
          "source": "OllamaModel-jpiyc",
          "sourceHandle": "{dataType:OllamaModel,id:OllamaModel-jpiyc,name:text_output,output_types:[Message]}",
          "target": "ChatOutput-dMHYm",
          "targetHandle": "{fieldName:input_value,id:ChatOutput-dMHYm,inputTypes:[Message],type:str}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-dMHYm",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "OllamaModel",
              "id": "OllamaModel-jpiyc",
              "name": "text_output",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-OllamaModel-jpiyc{dataType:OllamaModel,id:OllamaModel-jpiyc,name:text_output,output_types:[Message]}-ChatOutput-dMHYm{fieldName:input_value,id:ChatOutput-dMHYm,inputTypes:[Message],type:str}",
          "className": ""
        }
      ],
      "viewport": {
        "x": 553.3519296464808,
        "y": 157.29515997628755,
        "zoom": 0.3841072402165912
      }
    },
    "date_created": "2024-09-03T04:21:25.390Z",
    "date_updated": "2024-09-03T04:21:25.574Z",
    "status": "Public",
    "sort": null,
    "user_updated": "2edb86fd-5024-4e6f-9013-f6f3aa272d9d",
    "user_created": {
      "username": "MetaNets",
      "first_name": "Nick",
      "last_name": null,
      "id": "2edb86fd-5024-4e6f-9013-f6f3aa272d9d"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:06.663Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 164,
    "converter_version": "1.0.0"
  }
}